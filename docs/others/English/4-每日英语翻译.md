# 每日翻译练习

翻译资料来源：https://github.com/extreme-assistant/CVPR2022-Paper-Code-Interpretation

## 4月14日翻译

`OW-DETR: Open-world Detection Transformer`

Open-world object detection (OWOD) is a challenging computer vision problem, where the task is to detect a known set of object categories while simultaneously identifying unknown objects. Additionally, the model must incrementally learn new classes that become known in the next training episodes. Distinct from standard object detection, the OWOD setting `poses significant challenges` for generating quality candidate proposals on potentially unknown objects, separating the unknown objects from the background, and detecting diverse unknown objects. Here, we introduce a novel end-to-end transformer-based framework, OW-DETR, for open-world object detection. The proposed OW-DETR comprises three dedicated components namely, attention-driven pseudo-labeling, novelty classification and objectness scoring to explicitly address the aforementioned OWOD challenges. Our OW-DETR `explicitly` encodes multi-scale contextual information, possesses less `inductive bias`, enables knowledge transfer from known classes to the unknown class and can better discriminate between unknown objects and background. Comprehensive experiments are performed on two benchmarks: MS-COCO and PASCAL VOC. The `extensive ablations` reveal the `merits` of our proposed contributions. Further, our model outperforms the recently introduced OWOD approach, ORE, with absolute gains ranging from 1.8% to 3.3% in terms of unknown recall on MS-COCO. In the case of incremental object detection, OW-DETR outperforms the state-of-the-art for all settings on PASCAL VOC. Our code is available at [this https URL](https://github.com/akshitac8/OW-DETR).

inductive bias: 归纳偏置

explicitly： 明确地

ablations：消融

merits：优点

`AdaMixer: A Fast-Converging Query-Based Object Detector`

Traditional object detectors employ the `dense` `paradigm` of scanning over locations and scales in an image. The recent query-based object detectors break this convention by decoding image features with a set of learnable queries. However, this paradigm still suffers from slow `convergence`, limited performance, and design complexity of extra networks between backbone and decoder. In this paper, we find that the key to these issues is the adaptability of decoders for casting queries to varying objects. Accordingly, we propose a fast-converging query-based detector, named AdaMixer, by improving the adaptability of query-based decoding processes in two aspects. First, each query adaptively samples features over space and scales based on estimated offsets, which allows AdaMixer to efficiently attend to the `coherent` regions of objects. Then, we dynamically decode these `sampled` features with an adaptive MLP-Mixer under the guidance of each query. Thanks to these two `critical` designs, AdaMixer enjoys architectural simplicity without requiring dense attentional encoders or `explicit` pyramid networks. On the challenging MS COCO benchmark, AdaMixer with ResNet-50 as the backbone, with 12 training epochs, reaches up to 45.0 AP on the validation set along with 27.9 APs in detecting small objects. With the longer training scheme, AdaMixer with ResNeXt-101-DCN and Swin-S reaches 49.5 and 51.3 AP. Our work sheds light on a simple, accurate, and fast converging architecture for query-based object detectors. The code is made available at [this https URL](https://github.com/MCG-NJU/AdaMixer)

## 4月15日翻译

`Multi-Granularity Alignment Domain Adaptation for Object Detection`

Domain adaptive object detection is challenging due to distinctive data distribution between the source domain and target domain. In this paper, we propose a unified multi-granularity alignment-based object detection framework towards domain-invariant feature learning. To this end, we encode the dependencies across different granularity perspectives including pixel-, instance-, and category- levels simultaneously to align two domains. Based on pixel-level feature maps from the backbone network, we first develop the Omni-scale gated fusion module to aggregate discriminative representations of instances by scale-aware convolutions, leading to robust multi-scale object detection. Meanwhile, the multi-granularity discriminators are proposed to identify which domain different granularities of samples(i.e., pixels, instances, and categories) come from. Notably, we leverage not only the instance discriminability in different categories but also the category consistency between two domains. Extensive experiments are carried out on multiple domain adaptation scenarios, demonstrating the effectiveness of our framework over state-of-the-art algorithms on top of anchor-free FCOS and anchor-based Faster RCNN detectors with different backbones.

distinctive：独特的

invariant：不变的

To this end：为了这个目的

dependency：依赖关系

Omni-scale：全尺度

gated：门控

discriminators：鉴别器

aggregate：集合

`End-to-End Human-Gaze-Target Detection with Transformers`

In this paper, we propose an effective and efficient method for Human-Gaze-Target (HGT) detection, i.e., gaze following. Current approaches decouple the HGT detection task into separate branches of salient object detection and human gaze prediction, employing a two-stage framework where human head locations must first be detected and then be fed into the next gaze target prediction sub-network. In contrast, we redefine the HGT detection task as detecting human head locations and their gaze targets, simultaneously. By this way, our method, named Human-Gaze-Target detection TRansformer or HGTTR, streamlines the HGT detection pipeline by eliminating all other additional components. HGTTR reasons about the relations of salient objects and human gaze from the global image context. Moreover, unlike existing two-stage methods that require human head locations as input and can predict only one human's gaze target at a time, HGTTR can directly predict the locations of all people and their gaze targets at one time in an end-to-end manner. The effectiveness and robustness of our proposed method are verified with extensive experiments on the two standard benchmark datasets, GazeFollowing and VideoAttentionTarget. Without bells and whistles, HGTTR outperforms existing state-of-the-art methods by large margins (6.4 mAP gain on GazeFollowing and 10.3 mAP gain on VideoAttentionTarget) with a much simpler architecture.

decouple：解耦

salient object detection：显著性目标检测

reason：论证

manner：方式



























