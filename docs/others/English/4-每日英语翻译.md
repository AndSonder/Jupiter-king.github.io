# 每日翻译练习

## 翻译练习1（11月20日）

**深度前馈网络(deep feedforward network)，也叫作前馈神经网络(feedforward neural network)或者 多层感知机(multilayer perceptron, MLP)，是典型的深度学习模型。**

Deep feedforward network, also called feedforward neural network or mutilayer perceptron, is a typical deep learning model.

Depp feedforward network`s`, also often called feedforward neural network`s`, or multilayer perceptron`s`, are `the` `quintessential` deep learning method. 

**前馈网络的目标是近似某个函数 $f^*$。**

The goal of the feedforward network is similarity a function $f^*$.

The goal of `a` feedforward network is `to` `approximate` some function $f^*$.


**例如，对于分类器，$y = f^*(x)$ 将输入$x$映射到一个类别 $y$。**

For a classifier $y = f^*(x)$ map the input $x$ to a class $y$

For example, for a classifier, $y = f^*(x)$ map`s` an input $x$ to a category $y$.

**前馈网络定义了一个映射 $y = f(x; θ)$，并且学习参数 $θ$ 的值，使它能够得到最佳的函数近似。**

Deep feedforward network defines a map $y = f(x;\theta)$ and learn the parameter $\theta$ to get the best approximation to function.

`A` feedforward network defines a mapping $y=f(x;\theta)$ and learns `the value of the parameters` $\theta$ that `result in` the best `function approximation`.

**这种模型被称为 前向(feedforward)的，是因为信息流过 x 的函数，流经用于定义 $f$ 的中间计算过程，最终到达输出 $y$。**

That model called feedforward  

These models are called feedforward because information flow through the function being evaluated from $x$, through the `intermediate` `computations` used to define $f$, and finally to the output $y$.

**在模型的输出和模型本身之间没有反馈 (feedback)连接。**

Model's outputs and model itself don't connected by feedback.

`There are` no feedback connections in which outputs of the model are `fed back` into itself.

**当前馈神经网络被扩展成包含反馈连接时，它们被称为循环神经网络(recurrent neural network)，在第十章介绍。**

Now feedback network is called recurrent netual network when it be extended an includes feedback connection.

When feedforward neural networks `are extended to` include feedback connections, they are called recurrent neural networks.

**前馈网络对于机器学习的从业者是极其重要的。**

The feedback network is very important for marchine learning practitioners

Feedforward networks `are of extreme important` to machine learning practitioners.

**它们是许多重要商业应用的基础。**

They are the basic of many business applications.

They `form the basis of` many important `commercial` applications.

**例如，用于对照片中的对象进行识别的卷积神经网络就是一种专门的前馈网络。 **

For example, CNN is a specialized feed forward network used to recognize objects in the picture.

For example, the convolutional networks used for object `recognition from photos` are a specialized `kind of` feedforward network.

**前馈网络是通往循环网络之路的概念基石，后者在自然语言的许多应用中发挥着巨大作用。**

Feedback neural network is the basic concept of the recurrent neural network which plays an important role in many applications about natural language.

Feedforward networks are `a conceptual stepping stone` on the path to recurrent networks, which `power` many natural language applications.

> 总结：名词单复数掌握的不好，the 和 a 搞不清楚什么时候用，动词词组还是匮乏

## 翻译练习2（11月21日）

**前馈神经网络被称作网络 (network) 是因为它们通常用许多不同函数复合在一起来表示。**

The reason why feedward neural networks are called network  is they always combine many different functions.

Feedforward neural networks are called networks because they are `typically` `represented by` `composing together` many different functions.

**例如, 我们有三个函数 $f^{(1)}, f^{(2)}$ 和 $f^{(3)}$ 连接在一个链上以形成 $f(\boldsymbol{x})=f^{(3)}\left(f^{(2)}\left(f^{(1)}(\boldsymbol{x})\right)\right)$ 。**



For example, we have three functions, $f^{(1)}, f^{(2)}$ and $f^{(3)}$, join in a chain to form $f(\boldsymbol{x})=f^{(3)}\left(f^{(2)}\left(f^{(1)}(\boldsymbol{x})\right)\right)$.

For example, we `might` have three functions $f^{(1)}, f^{(2)}$ and $f^{(3)}$ connected in a chain, `to form` $f(\boldsymbol{x})=f^{(3)}\left(f^{(2)}\left(f^{(1)}(\boldsymbol{x})\right)\right)$.

**这些链式结构是神经网络中最常用的结构。**

These chain structures are the most usually used in neural networks.

These chain structures are the most `commonly` used structures `of` neutral networks.

**在这种情况下, $f^{(1)}$ 被称为网络的 第一层 ( first layer), $f^{(2)}$ 被称为 第二层 ( second layer ), 以此类推。**

In this situation, $f^{(1)}$ is called the first layer, $f^{(2)}$ is called the second layer and so on.

In this case, $f^{(1)}$ is called the first layer, $f^{(2)}$ is called the second layer and so on.

**链的全长称为模型的深度 ( depth )。**

The length of the chain is called depth.

The `overall` length of the chain `gives the depth of` the model.

**正是因为这个术语才出现了 “深度学习" 这个名字。**

Deep learning is from this term.

It is from this terminology that the name "deep learning" arises.

**前馈网络的最后一层被称为 输出层 ( output layer )。**

The last layer of the feedforward network is called the output layer.

The finial layer of `a` feedforward network is called the output layer.

**在神经网络训练的过程中, 我们让 $f(\boldsymbol{x})$ 去匹配 $f^*(\boldsymbol{x})$ 的值。**

During the training processing of neural networks, we let $f(\boldsymbol{x})$ to map the values of $f^*(\boldsymbol{x})$.

During neural network training, we `drive` $f(x)$ to `match` $f^*(\boldsymbol{x})$.

**训练数据为我们提供了在不同训练点上取值的、含有噪声的 $f^*(\boldsymbol{x})$ 的近似实例。**

Train datas provide us examples of approximation 

The training data provides us with noisy, approximate examples of $f^*(\boldsymbol{x})$ evaluated at different training point.

**每个样本 $\boldsymbol{x}$ 都伴随着一个标签 $y \approx f^*(\boldsymbol{x})$ 。 **

Each sample $\boldsymbol{x}$ has a label $y \approx f^*(\boldsymbol{x})$.

Each example $\boldsymbol{x}$ `is accompanied by` a label $y \approx f^*(\boldsymbol{x})$.

**训练样本直接指明了输出层在每一点 $x$ 上必须做什么; 它必须产生一个接近 $y$ 的值。**

Train datas directly indicate that what the output layer should do in each point $x$, it must produce a value closed to $y$.

The training examples specify directly what the output layer must do `at` each point $x$; it must produce a value that is close to $y$.

## 翻译练习3（11月22日）

**学习算法必须决定如何使用这些层来产生想要的输出，但是训练数据并没有说每个单独的层应该做什么。**

Learning algorithms must use those layers to produce the desired outputs,but training datas don't directly indicate what other layers should do.

`The` learning algorithm must `decide how to` use those layer to produce the desired output, but the training data `does not say what each individual layer should do`.

**相反，学习算法必须决定如何使用这些层来最好地实现 $f^∗$ 的近似。**

On the contrary, learning algorithms must decide how to use these layers to get the approximate $f^*$.

`Instead`, the learning algorithm must decide how to use these layer to `best implement` an `approximation` of $f^*$

**因为训练数据并没有给出这些层中的每一层所需的输出，所以这些层被称为隐藏层(hidden layer)。**

Because training datas don't give the outputs for each layer, so they are called the hidden layer.

Because the training data does not show the desired output for each of these layers, these layers are called hidden layer.

**最后，这些网络被称为神经网络是因为它们或多或少地受到神经科学的启发。**

At last, these networks are called neural network because they're more or less inspired by neuroscience.

Finally, these networks are called neural because they are `loosely` `inspired by` neuroscience.

**网络中的每个隐藏层通常都是向量值的。**

Each hidden layer of networks usually is value of the vector.

Each hidden layer of the network is `typically` `vector-value`.

**这些隐藏层的维数决定了模型的宽度 (width)。**

These hidden layers' dimension give the model's width.

The dimensionality of these hidden layers determines the width of the model.

:::tip

描述 A 的 B时，多用 B of A, 而不是用 A's B.

:::

**向量的每个元素都可以被视为起到类似一个神经元的作用。**

Each value in vectors can be seen as playing a similar role like a neure.

Each `element` of the vector may `be interpreted as` playing a role `analogous` to a neuron.

## 翻译练习3（11月24日）

**一种理解前馈网络的方式是从线性模型开始, 并考虑如何克服它的局限性。**

One way to understand the feedward network is starting from the linear model and considing how to overcome its limitation.

One way to understand feedforward networks is `to begin with` linear models and consider how to overcome their limitation`s`

**线性模型, 例如逻辑回归和线性回归, 是非常吸引人的, 因为无论是通过闭解形式还 是使用凸优化, 它们都能高效且可靠地拟合。**

Linear model such as the linear regression and the logistic regression are very attractive because whatever through convex optimization or other methods they can fit efficient and reliable.

Linear model, such as logistic regression and linear regression, are appealing because they may be fit efficiently and reliably, either in closed from or with convex optimisation.

**线性模型也有明显的缺陷, 那就是该模型的能力被局限在线性函数里, 所以它无法理解任何两个输人变量间的相互作用。**

Linear model also have the obvious defeat that the ability of the model are limited in the linear function therefore it can not understand the interaction between two inputs.

Linear models also have the obvious defect the model `capacity`  is limited to linear functions, so the model cannot understand the interaction between any two input variables.

**为了扩展线性模型来表示 $\boldsymbol{x}$ 的非线性函数, 我们可以不把线性模型用于 $\boldsymbol{x}$ 本身, 而是用在一个变换后的输人 $\phi(\boldsymbol{x})$ 上, 这里 $\phi$ 是一个非线性变换。**

In order to using the extended linear model to show the nonlinear function of $\boldsymbol{x}$, we use a transformed input $\phi(\boldsymbol{x})$ instead of using a linear function for $\boldsymbol{x}$ itself, here $\phi$ is a nonlinear transformation.

To extend linear models to represent nonlinear functions of $\boldsymbol{x}$, we can `apply` `the` linear `not to` $\boldsymbol{x}$ itself `but to` a transformed input $\phi(\boldsymbol{x})$, where $\phi$ is a nonlinear transformation.

:::tip

not to .... but to ... 不 ... 而是 ...

:::

**同样, 我们可以使用第 5.7.2 节中描述的核技巧, 来得到一个基于隐含地使用 $\phi$ 映射的非线性学习算法。**

As well, we can use the kernel skill described by section 5.7.2 to get a nonlinear learning algorithm based on using $\phi$ impliedly.

Equtivalenty, we can apply the kernel trick described in Sec. 5.7.2 to obtain a nonlinear learning algorithm based on implicitly applying the $\phi$ mapping.

**我们可以认为 $\phi$ 提供了一组描述 $\boldsymbol{x}$ 的特征, 或者认为它提供了 $x$ 的一个新的表示。**

We can also think $\phi$ gives out a group of description about the feature of $\boldsymbol{x}$, or provides a new representation of $x$.

We can `think of $\phi$ as` providing `a set of` features desirabling $\boldsymbol{x}$, or providing a new representation for $\boldsymbol{x}$.

:::tip

think of sth as ... 将 ... 看作 ...

:::

## 翻译练习4（11月26日）

卷积网络(convolutional network)，也叫做 卷积神经网络(convolutional neural network, CNN)，是一种专门用来处理具有类似网格结构的数据的神经网络

Convolutional networks, also called convolutional neural networks (CNNS), are a type of special neural networks used to process datas with a grid structure.

Convolutional networks, `also known as` conventional neural networks or CNNs, are a `specialized kind of` neural network for processing data that has a known, grid-like `topology`.

例如时间序列数据(可以认为是在时间轴上有规律地采样形成的一维网格)和图像数据(可以看作是二维的像素网格)。

For example, time series datas can be seen as one-dimension grids by regularly sampling on the time axis and image datas can be seen as two-dimension pixel grids.

Examples include time-series data, which `can be thought of as` a 1D grid taking samples at `regular time intervals`.

卷积网络在诸多应用领域都表现优异。

Convolutional networks perform outstanding in many application areas.

Convolutional networks have been `tremendously successful` in practical applications.

"卷积神经网络" 一词表明该网络使用了卷积(convolution)这种数学运算。

The word "Convolutional networks" indicates that networks use the convolution mathematical operation.

The name "convolutional neural network" indicates that the network `employs` a mathematical operation called convolution.

:::tip

使用可以翻译为 employ

:::

卷积是一种特殊的线性运算。

Convolution is a special linear operation. 

Convolution is a specialized kind of linear operation.

:::tip

一种可以翻译为 kind of

:::

卷积网络是指那些至少在网络的一层中使用卷积运算来替代一般的矩阵乘法运算的神经网络。

Conventional networks are those networks using convolution operation replace general matrix multiplication operation.

Convolutional networks are simply neural networks that use convolution `in place of` general matrix multiplication in at least one of their layer.

本章，我们首先说明什么是卷积运算。

In this chapter, we first show what is the convolution operation.

In this chapter, we will first describe `what convolution is`. 

接着，我们会解释在神经网络中使用卷积运算的动机。

Than, we will explain the motivation about using convolution in neural networks.

Next, we will explain the motivation `behind` using convolution in neural networks.

然后我们会介绍池化(pooling)，这是一种几乎所有的卷积网络都会用到的操作。

Next, we will introduce pooling that is a operation almost all all of convolution networks will use it.

We will then describe an operation called pooling, which almost all all convolutional networks `employ`.

通常来说，卷积神经网络中用到的卷积运算和其他领域(例如工程领域以及纯数学领域)中的定义并不完全一致。

Generally speak, convention in convention neural networks is different to other areas, such as engineering and mathematical field.

Usually, the operating used in convolution neural network `does not as` engineering or pure mathematics.

我们会对神经网络实践中广泛应用的几种卷积函数的变体进行说明。

We will give the explain against several variants of the convolution function widely used in neural networks application.

We will describe several variants `on` the convolution function that are widely used in `practice for` neural networks.

我们也会说明如何在多种不同维数的数据上使用卷积运算。

We will also show how to do convolution operation in different dimension's datas.

We will also show how convolution may be applied to many kinds of data, with differen number of dimensions.

之后我们讨论使得卷积运算更加高效的一些方法。

And than, we will discuss some methods that can make convention more efficient.

We than discuss `means` of making convolution more efficient.

:::tip

可以用 mean 去替换 method

:::

卷积网络是神经科学原积运算。

Convention network xxx  不会 :( 

Convolutional networks `stand out as an example of` neuroscientific principles influencing deep learning.

### 翻译练习5


之后我们讨论使得卷积运算更加高效的一些方法。


卷积网络是神经科学原理影响深度学习的典型代表。

我们之后也会讨论这些神经科学的原理，并对卷积网络在深度学习发展史中的作用作出评价。

本章没有涉及如何为你的卷积网络选择合适的结构，因为本章的目标是说明卷积网络提供的各种工具。

第十一章将会对如何在具体环境中选择使用相应的工具给出通用的准则。

对于卷积网络结构的研究进展得如此迅速，以至于针对特定基准 (benchmark)，数月甚至几周就会公开一个新的最优的网络结构，甚至在写这本书时也不好描述究竟哪种结构是最好的。


然而，最好的结构也是由本章所描述的基本部件逐步搭建起来的。










































