# 每日翻译练习

## 翻译练习1（11月20日）

深度前馈网络(deep feedforward network)，也叫作前馈神经网络(feedforward neural network)或者 多层感知机(multilayer perceptron, MLP)，是典型的深度学习模型。

Deep feedforward network, also called feedforward neural network or mutilayer perceptron, is a typical deep learning model.

Depp feedforward network`s`, also often called feedforward neural network`s`, or multilayer perceptron`s`, are `the` `quintessential` deep learning method. 

前馈网络的目标是近似某个函数 $f^*$。

The goal of the feedforward network is similarity a function $f^*$.

The goal of `a` feedforward network is `to` `approximate` some function $f^*$.


例如，对于分类器，$y = f^*(x)$ 将输入$x$映射到一个类别 $y$。

For a classifier $y = f^*(x)$ map the input $x$ to a class $y$

For example, for a classifier, $y = f^*(x)$ map`s` an input $x$ to a category $y$.

前馈网络定义了一个映射 $y = f(x; θ)$，并且学习参数 $θ$ 的值，使它能够得到最佳的函数近似。

Deep feedforward network defines a map $y = f(x;\theta)$ and learn the parameter $\theta$ to get the best approximation to function.

`A` feedforward network defines a mapping $y=f(x;\theta)$ and learns `the value of the parameters` $\theta$ that `result in` the best `function approximation`.

这种模型被称为 前向(feedforward)的，是因为信息流过 x 的函数，流经用于定义 $f$ 的中间计算过程，最终到达输出 $y$。

That model called feedforward  

These models are called feedforward because information flow through the function being evaluated from $x$, through the `intermediate` `computations` used to define $f$, and finally to the output $y$.

在模型的输出和模型本身之间没有反馈 (feedback)连接。

Model's outputs and model itself don't connected by feedback.

`There are` no feedback connections in which outputs of the model are `fed back` into itself.

当前馈神经网络被扩展成包含反馈连接时，它们被称为循环神经网络(recurrent neural network)，在第十章介绍。

Now feedback network is called recurrent netual network when it be extended an includes feedback connection.

When feedforward neural networks `are extended to` include feedback connections, they are called recurrent neural networks.

前馈网络对于机器学习的从业者是极其重要的。

The feedback network is very important for marchine learning practitioners

Feedforward networks `are of extreme important` to machine learning practitioners.

它们是许多重要商业应用的基础。

They are the basic of many business applications.

They `form the basis of` many important `commercial` applications.

例如，用于对照片中的对象进行识别的卷积神经网络就是一种专门的前馈网络。 

For example, CNN is a specialized feed forward network used to recognize objects in the picture.

For example, the convolutional networks used for object `recognition from photos` are a specialized `kind of` feedforward network.

前馈网络是通往循环网络之路的概念基石，后者在自然语言的许多应用中发挥着巨大作用。

Feedback neural network is the basic concept of the recurrent neural network which plays an important role in many applications about natural language.

Feedforward networks are `a conceptual stepping stone` on the path to recurrent networks, which `power` many natural language applications.

> 总结：名词单复数掌握的不好，the 和 a 搞不清楚什么时候用，动词词组还是匮乏

## 翻译练习2（11月21日）

前馈神经网络被称作网络 (network) 是因为它们通常用许多不同函数复合在一起来表示。

The reason why feedward neural networks are called network  is they always combine many different functions.

Feedforward neural networks are called networks because they are `typically` `represented by` `composing together` many different functions.

例如, 我们有三个函数 $f^{(1)}, f^{(2)}$ 和 $f^{(3)}$ 连接在一个链上以形成 $f(\boldsymbol{x})=f^{(3)}\left(f^{(2)}\left(f^{(1)}(\boldsymbol{x})\right)\right)$ 。



For example, we have three functions, $f^{(1)}, f^{(2)}$ and $f^{(3)}$, join in a chain to form $f(\boldsymbol{x})=f^{(3)}\left(f^{(2)}\left(f^{(1)}(\boldsymbol{x})\right)\right)$.

For example, we `might` have three functions $f^{(1)}, f^{(2)}$ and $f^{(3)}$ connected in a chain, `to form` $f(\boldsymbol{x})=f^{(3)}\left(f^{(2)}\left(f^{(1)}(\boldsymbol{x})\right)\right)$.

这些链式结构是神经网络中最常用的结构。

These chain structures are the most usually used in neural networks.

These chain structures are the most `commonly` used structures `of` neutral networks.

在这种情况下, $f^{(1)}$ 被称为网络的 第一层 ( first layer), $f^{(2)}$ 被称为 第二层 ( second layer ), 以此类推。

In this situation, $f^{(1)}$ is called the first layer, $f^{(2)}$ is called the second layer and so on.

In this case, $f^{(1)}$ is called the first layer, $f^{(2)}$ is called the second layer and so on.

链的全长称为模型的深度 ( depth )。

The length of the chain is called depth.

The `overall` length of the chain `gives the depth of` the model.

正是因为这个术语才出现了 “深度学习" 这个名字。

Deep learning is from this term.

It is from this terminology that the name "deep learning" arises.

前馈网络的最后一层被称为 输出层 ( output layer )。

The last layer of the feedforward network is called the output layer.

The finial layer of `a` feedforward network is called the output layer.

在神经网络训练的过程中, 我们让 $f(\boldsymbol{x})$ 去匹配 $f^*(\boldsymbol{x})$ 的值。

During the training processing of neural networks, we let $f(\boldsymbol{x})$ to map the values of $f^*(\boldsymbol{x})$.

During neural network training, we `drive` $f(x)$ to `match` $f^*(\boldsymbol{x})$.

训练数据为我们提供了在不同训练点上取值的、含有噪声的 $f^*(\boldsymbol{x})$ 的近似实例。

Train datas provide us examples of approximation 

The training data provides us with noisy, approximate examples of $f^*(\boldsymbol{x})$ evaluated at different training point.

每个样本 $\boldsymbol{x}$ 都伴随着一个标签 $y \approx f^*(\boldsymbol{x})$ 。 

Each sample $\boldsymbol{x}$ has a label $y \approx f^*(\boldsymbol{x})$.

Each example $\boldsymbol{x}$ `is accompanied by` a label $y \approx f^*(\boldsymbol{x})$.

训练样本直接指明了输出层在每一点 $x$ 上必须做什么; 它必须产生一个接近 $y$ 的值。

Train datas directly indicate that what the output layer should do in each point $x$, it must produce a value closed to $y$.

The training examples specify directly what the output layer must do `at` each point $x$; it must produce a value that is close to $y$.

## 翻译练习3（11月22日）

学习算法必须决定如何使用这些层来产生想要的输出，但是训练数据并没有说每个单独的层应该做什么。

Learning algorithms must use those layers to produce the desired outputs,but training datas don't directly indicate what other layers should do.

相反，学习算法必须决定如何使用这些层来最好地实现 $f^∗$ 的近似。

On the contrary, learning algorithms must decide how to use those layers to get the approximate $f^*$.

因为训练数据并没有给出这些层中的每一层所需的输出，所以这些层被称为隐藏层(hidden layer)。

Because training datas don't give the outputs for each layer, so they are called the hidden layer.

最后，这些网络被称为神经网络是因为它们或多或少地受到神经科学的启发。

At last, these networks are called neural network because they're more or less inspired by neuroscience.

网络中的每个隐藏层通常都是向量值的。

Each hidden layer of networks usually is value of the vector.

这些隐藏层的维数决定了模型的宽度 (width)。

Those hidden layers' dimension give the model's width.

向量的每个元素都可以被视为起到类似一个神经元的作用。

Each value in vectors can be seen as playing a similar role like a neure.































