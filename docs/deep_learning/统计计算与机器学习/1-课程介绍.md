# 课程介绍
:::tip
本博客笔记对应上海交通大学许老师的统计计算与机器学习课程，感谢许老师提供的优质学习资源。

对应视频链接：[https://www.bilibili.com/video/BV1i7411K7YV?p=3](https://www.bilibili.com/video/BV1i7411K7YV?p=3)

:::
## 统计学习与机器学习的差异
:::tip
统计学习：`model-base`

机器学习：`model-free`
:::

接下来我们用线性回归的例子来理解一下为什么统计学习是`model-base`的而机器学习的`model-free`的

### 统计学习的角度

假设我们有一些数据集Data, $S = \{(x_i,y_i)\}_{i=1}^n$; 拿股票举例，如果$x_i$是一种投资组合，$y_i$就是预期的投资收益。我们现在就是想给定$x_i$来预测$y_i$。

如果从最简单的角度进行考虑，我们可以用一个线性的模型来表示这个任务：
$$
\begin{gathered}
y=a x+b=[x, 1]\left[\begin{array}{l}
a \\
b
\end{array}\right]=\bar{x} \beta \\
\bar{x}=[x, 1] \quad \beta=\left[\begin{array}{l}
a \\
b
\end{array}\right]
\end{gathered}
$$
如果我们有一系列的数据，那么我们可以把这一些的数据用一个矩阵的形式写出来：
$$
X=\left[\begin{array}{cc}
x_{1} & 1 \\
x_{2} & 1 \\
\vdots & \vdots \\

x_{n} & 1
\end{array}\right] \quad Y=\left[\begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{n}
\end{array}\right]
$$
稍微整理一下就可以得到：
$$
Y=X \beta
$$
从统计的角度来说，我们的$\beta$ 可以这样选：
$$
\hat{\beta}=\left(X^{\top} X\right)^{-1} X^{\top} Y
$$
**那么为什么$\hat{\beta}$ 可以这样选呢？** 

$\hat{\beta}$ 也叫做最小二乘估计，其可以使得下面的公式成立：
$$
\hat{\beta}=\arg \min _{\beta}\langle x \beta-Y\rangle^{\top}(x \beta-Y)
$$
:::note
高斯马尔科夫定理：

假设(X,Y)是从一项线性的模型 $Y=X \beta+\varepsilon$ 采样出来的；

对于$X \in R^{n \times d}, Y \in R^{n \times 1}$ 假设 $(XX^T)$ 是 可逆的，且$E(\varepsilon)=0, \quad \operatorname{Var}(\varepsilon)=\sigma^{2} I_{n \times n}$   即两个采样之间是相互独立的；

那么，

(1) $\hat{\beta}$ 是无偏的，即 $E \hat{\beta}=\beta$ 

(2)  $\forall \tilde{\beta}=C Y$, $\operatorname{Var}(\hat{\beta}) \leq \operatorname{Var}(\widetilde{\beta})$
:::

结合上面的内容我们就可以知道，如果使用上述的线性模型+最小二乘估计去做预测。想要使得预测的非常准确那么需要满足模型本身的线性的且有部分的噪音

:::tip
统计就是你能够知道X和Y符合什么样的关系，且知道在什么情况下会符合的比较好。需要知道具体为什么模型是这样去设计设计的。
:::

### 机器学习的角度

:::tip
对于机器学习来说，就不需要那么严格的估计。我就是假定你符合某个分布，至于参数是什么样的。我们是通过大量的数据去学习出来的
:::

也是假设我们有数据集Data, $S = \{(x_i,y_i)\}_{i=1}^n$;   有了数据之后我们就进行学习，首先我们需要一个假设空间：$\{y=a x+b\}$ 参数a和b都是通过学习获得的。那么要如何通过学习得到参数呢？

我们首先需要一个目标，首先我们定义一个$L_n$：
$$
L_{n}=\frac{1}{n} \sum_{i=1}^{n}\left(a x_{i}+b-y_{i}\right)^{2}
$$
**$L_n$表示的含义就是预测的值与真实值的一个平方距离**。那么算法的目标也很简单，就是让这个距离尽可能的小：
$$
min \ L_n
$$

下面我们来考虑一下如何更新a和b，为了方便书写我们改写一下$L_n$的公式：
$$
\begin{aligned}
&h_{i} \triangleq a x_{i}+b \\
&L_{n}=\frac{1}{2n} \sum_{i=1}^{n}\left(h_{i}-y_{i}\right)^{2}
\end{aligned}
$$
a和b的更新思路也非常的简单只需要按照梯度的方向进行更新即可, 以更新a举例（更新b是同理的）：
$$
\begin{aligned}
\frac{\partial \operatorname{Ln}}{\partial a} &=\frac{1}{n} \sum_{i=1}^{n}\left(h_{i}-y_{i}\right) \frac{\partial h_{i}}{\partial a} \\
&=\frac{1}{n} \sum_{i=1}^{n}\left(h_{i}-y_{i}\right) x_{i}
\end{aligned}
$$
那么a的更新公式如下：
$$
a^{t}=a^{t}-  \eta \frac{\partial Ln }{\partial a}
$$
其中 $\eta$ 学习率，可以用来控制参数更新的速度，这种更新的方式就是使用梯度下降进行更新参数

:::tip
当然了这里并不一定非要使用线性回归的模型，你也可以使用神经网络去拟合，同理可以继续使用梯度下降的算法去更新神经网络的参数；
:::

按照上述的思路对a和b进行更新我们最后得到的模型就可以在新的数据集上也能够取得很好的预测结果。

假设现在我们有测试数据集：$\left\{\left(x_{i}^{\text {test }}, y_{i}^{\text {test }}\right)\right\}_{i=1}^{n_{t}}$ ，那么希望测速误差`Test error`能够尽可能的小。也就是让 $L_{text}$ 尽可能的小。

$$
L_{\text {test }}=\frac{1}{2 n_{t}} \sum_{i=1}^{n_{t}} \left(h_{i}^{t}-y_{i}\right)^{2}
$$
## 机器学习中的基础概念

### 机器学习的三种主要分类
1. 有监督学习（supervised learning）：`带有标签的学习` ，比如猫狗分类，数据集有标签
2. 无监督学习（unsupervised learning）: `不带有标签的学习`， 数据集没有标签
3. 强化学习（reinforcement learing）：`学习做出最优决策`，没有标签而是给算法 “奖励”

### 符号定义
为了更加方便的了解，这里先对一些符号做数学上的定义（感觉老师的板书好像写论文，数学符号什么的都好规范）

`数据集(Dataset)`:  $S=\left\{z_{i}\right\}_{i=1}^{n} \quad z=(x, y)$, $z$ 是从一个分布当中采样得到的，分布$D$ 的表示方法是 $D = X \times Y$，$X \in R^d$ 是样本集，$Y \in R^{d_o}$ 是标签集,  $n$ 表示样本的输入。

`函数(Function)`:  如果存在一个目标函数，那么就可以记作 $f(x)$ 或者 $f^t(x)$

我们有一个假设空间（Hypothetical space）：$\mathcal{H}$ ；

$\mathcal{H}$中的一个函数就可以定义为：$f_{\theta} \in \mathcal{H} \quad f(x, \theta) \in \mathcal{H}$；

其中 $\theta$ 表示参数的集合；

`损失函数（loss function）`： 用于衡量当前任务表现效果的一个函数

一个损失函数可以表示为：$l: \mathcal{H} \times Z \rightarrow R_+$

e.g. $L_2$ 损失函数也叫做均方误差，他们可以表示为：
$$
l\left(f_{\theta}, z\right)=\frac{1}{2}\left(f_{\theta}(x)-y\right)^{2}
$$
训练集上的误差可以叫做: `Empirical Risk`, `traing loss` 经常表示为：$L_{n}(\theta), L_{s}(\theta), R_{n} \cdot(\theta), R_{s}(\theta)$ 训练的过程就是让训练loss尽可能的小，也就是Empirical Risk minimization (ERM)

在测试集上的loss叫做：`population risk` 可以表示为：$L_{D}(\theta), R_{D}(\theta)$

`两层神经网络`： Two-layer neural Network, 我们表示为：$\sum_{j=1}^{m} a_{j} \sigma\left(\omega_{j} x+b_{j}\right)$

其中 m: Neural number, x: input, $w_j$: input weight, $b_j$: bias term, $\sigma(x)$: Activation function，$a_j$: output weight

:::note
在计算神经网络层数的时候，输入层不算作一层
:::

`激活函数`： Activation function

可以康康我在另一篇文章中写的对激活函数的理解：[点我进入另一篇文章](https://space.keter.top/docs/deep_learning/基础知识/神经网络与反向传播)

常见的激活函数：

![](https://gitee.com/coronapolvo/images/raw/master/20210817141807image-20210817141805246.png)

`一般的神经网络`： General Deep Neural Network

就无限的套娃一层的神经网络：

![](https://gitee.com/coronapolvo/images/raw/master/20220209223905.png)

`训练`： Training, 通过梯度下降（GD）的算法进行更新，针对梯度下降也有很多的算法比如 Stochastic gradient descent(SGD)

$$
L_{\delta}(\theta)=\frac{1}{n} \sum_{j=1}^{n} l\left(f_{\theta}\left(x_{i}-y_{i}\right)\right.
$$
但是考虑到显存/内存的因素，我们一般会一个Batch，一个Batch的进行训练：
$$
L_{B}(\theta)=\frac{1}{|B|} \sum_{x \in B} l\left(f_{\theta}(x)-y\right)
$$
:::tip
1. 在数据科学中我们关心的是对未知样本的预测能力
2. 线性回归问题中可以把用神经网络去拟合，但是线性回归问题是一个`凸问题`，在这个问题当中你可能会感觉统计学习和机器学习没有什么太大的区别。但是如果在一个复杂的问题机器学习就会发挥出更加强大的优势；
3. 随着数据量和硬件计算能力的增强，深度学习已经开始大放光彩，这也吸引着我们去理解深度学习以及去发现深度学习是否会再次出现危机；
:::

## 梯度下降

在这一小节当中我们将会使用梯度下降算法来更新神经网络的参数，首先我们定义一个两层的神经网络：

$$
f_{\theta}(x)=\sum_{j=1}^{m} a_{j} \sigma\left(w_{j} x+b_{j}\right)
$$
我们将$f_{\theta}(x_i)$记作$h_i$, 然后使用MSE损失函数：

$$
L_{s(\theta)}=\frac{1}{2 n} \sum_{i=1}^{n}\left(h_{i}-y_{i}\right)^{2}
$$
然后我们对损失函数求偏导可以得到：

$$
\begin{aligned}
\frac{\partial L_{s}(\theta)}{\partial a_{j}} &=\frac{1}{n} \sum_{i=1}^{n}\left(h_{i}-y_{i}\right) \frac{\partial h_{i}}{\partial a_{j}} \\
&=\frac{1}{n} \sum_{i=1}^{n}\left(h_{i}-y_{i}\right) \sigma\left(w_{j} x_{i}+b_{j}\right)
\end{aligned}
$$
$$
\begin{aligned}
\frac{\partial L_{s}}{\partial w_{j}} &=\frac{1}{n} \sum_{i=1}^{n}\left(h_{i}-y_{i}\right) \frac{\partial h_{i}}{\partial w_{j}} \\
& =\frac{1}{n} \sum_{i=1}^{n}\left(h_{i}-y_{i}\right) a_{j} \ \frac{\partial \sigma\left(w_{i} x_{i}+b_{j}\right)}{\partial w_{j}}\\
&=\left.\frac{1}{n} \sum_{i=1}^{n}\left(h_{i}-y_{i}\right) a_{j} \frac{\partial \sigma(x)}{\partial x}\right|_{x=w_{j} x_{i}+b_{j}} x_{i}
\end{aligned}
$$
在进行梯度更新的时候，信息流是这样的：$h_i \rightarrow a_{j} \rightarrow \sigma \rightarrow x_{i}$

由于和输入一个数据x的时候的顺序是相反的，所以人们也把他叫做反向传播

## 深度学习中的问题
:::tip
这一个小节中主要关注了当下深度学习存在的主要问题，了解这些问题有助有我们之后对模型进行改良和优化；
:::

### 误差分析
我们在设计一个模型的时候很有可能产生各种各样的误差，这些误差可能来自于以下几个方面：
1. 模型没有拟合真实的数据分布
2. 数据在采样的过程中本身就是有误差的

在训练神经网络的过程中可能会产生各种各样的误差，那么我们要如何对误差进行分析呢？
1. 对噪声进行分类
	- **逼近误差（approximation error）**
	- **泛华误差（generalization error）**：在预测`没有见过的样本`时的泛华能力
	- **优化误差（optimization error）**：the distance between the best can be learned by the dataset and the best function can be learned by the dataset and the algorithm.

:::note
经验告诉我们，通过梯度下降的方法我们基本上是可以找到全局最优的。

泛化误差是我们关注的关键问题
:::

![](https://gitee.com/coronapolvo/images/raw/master/20220210203747.png)

### Mutli-layer
:::note
目前我们还不知道多层神经网络为什么比少层的神经网络有好，好多少
:::

### 训练
:::note
为什么GD/SGD可以训练那么大规模的网络
:::

Loss函数是一个非凸的函数，为什么GD往往能够找到最优的结果，什么时候找不到呢？

### 维数灾难
:::note
为什么DNN看起来可以解决维数灾难？
:::

许老师的课件中也指出了这一部分可以参考 Prof. Weinan E 的笔迹中的第一讲。这块确实挺难理解的，下面的内容主要是参考许老师的讲义

1) 线性插值 (Linear interpolation). 先看一个简单的例子，令 d = 1，$f_{\theta}$为目标函数$f^*$在 [0,1] 上以 $h = \frac{1}{n}$ 为间隔的分段线性插值函数，即$f_{\theta}\left(x_{j}\right)=f^{*}\left(x_{j}\right), j=0,1, \cdots, n$. 如果 $x \in\left[x_{j-1}, x_{j}\right]$，由广义泰勒剩余定理(参考任意数值分析教材)可知，
$$
f_{\theta}(x)-f^{*}(x)=\frac{1}{2}\left(x-x_{j-1}\right)\left(x_{j}-x\right) f^{\prime \prime}(\tilde{x}), \quad \tilde{x} \in\left[x_{j-1}, x_{j}\right]
	$$
因为$\left|\left(x-x_{j-1}\right)\left(x_{j}-x\right)\right| \leq \frac{h^{2}}{4}$,  于是有

$$
\left|f_{\theta}(x)-f^{*}(x)\right| \leq \frac{h^{2}}{8}\left\|f^{\prime \prime}\right\|_{\infty}=\frac{1}{8 n^{2}}\left\|f^{\prime \prime}\right\|_{\infty}
$$

其中 $\left\|f^{\prime \prime}\right\|_{\infty}=\max _{\tilde{x} \in[0,1]}\left|f^{\prime \prime}(\tilde{x})\right|$ .

对于 d 维，有类似的结论:

$$
\left|f_{\theta}(x)-f^{*}(x)\right| \leq C_{0} n^{-\frac{2}{d}}\left\|D^{2} f\right\|_{\infty}
$$

对于这种线性逼近方法，采样的数量随着维数的增加呈指数增长，这也就是所谓的“维 数灾难”问题:如果要使得误差达到一个精度 ε，那么所需的样本随着维数指数增长。

$$
n \approx \epsilon^{-\frac{d}{2}}
$$

如果 ε = 0.1,d = 200，那么 n ≈ 10^100。注意当 d = 200 时，并不算是一个很高的维数， 因为图像如果是 1M 大小，那它的维数 d ≈ 10^5。所以这个采样数是不可能达到的，造成 了维数灾难。
	
:::note
 以 MNIST 数据集中的样本为例，它的每一张图都是由 28 × 28 = 784 个像 素点组成(每个像素点都是 0 或 1)，那么如果要填满这个空间，至少需要 2^784 个样本， 显然不可能提供那么多的样本用于训练。
:::

 2) 积分问题。最简单直观的是数值积分问题。令
$$
I=\int_{0}^{1} f(x) \mathrm{d} x
$$
 如果我们用梯形公式计算每一个长度为 h 的均匀格子面积，则:
 $$
I_{n}=\frac{h}{2}(f(0)+f(1))+h \sum_{i=1}^{n-1} f\left(x_{i}\right)
$$
 其中 $h = \frac{1}{n}$ , $x_i = ih$。误差的上界则为:
$$
\left|I-I_{n}\right| \leq \frac{h^{2}}{8}\left\|f^{\prime \prime}\right\|_{\infty}=\frac{n^{-2}}{8}\left\|f^{\prime \prime}\right\|_{\infty}
$$
 对于 d 维，也有类似结论:
 $$
\left|I-I_{n}\right| \leq \frac{h^{\frac{2}{d}}}{8}\left\|f^{\prime \prime}\right\|_{\infty}=\frac{n^{-\frac{2}{d}}}{8}\left\|f^{\prime \prime}\right\|_{\infty}
$$
 
d 维时的结论中的 $\frac{1}{d}$可以通过 d 维空间的等距网格划分理解:假设一维时填充 [0, 1] 区间的等距划分需要 n 个点(假设划分数为 n)，那么对于 d 维空间，同样距离为$\frac{1}{n}$的等距划分需要 $n^d = n_0$个点，所以此时的划分点距离为$\frac{1}{n} = n^{-\frac{1}{d}}_0$ ，也就是上式中的$n^{-\frac{1}{d}}$。

3) 蒙特卡洛插值。这个方法很好的解决了维数灾难的问题。令$\left\{x_{i}\right\}_{i=1}^{n}$为均匀分布在 [0, 1]$^d$上的随机独立采样点，得到$\left\{x_{i}, f\left(x_{i}\right)\right\}_{i=1}^{n}$。接着我们用这些采样点估计积分:
$$
I=\int_{[0,1]^{d}} f(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \approx \frac{1}{n} \sum_{i=1}^{n} f\left(x_{i}\right)
$$
 用期望的定义知$\mathbb{E} I_{n}=I$. 下面计算方差:
$$
\begin{aligned}
\mathbb{E}\left(I-I_{n}\right)^{2} &=\mathbb{E}\left(1-\frac{1}{n} \sum_{i=1}^{n} f\left(x_{i}\right)\right)\left(1-\frac{1}{n} \sum_{i=1}^{n} f\left(x_{i}\right)\right) \\
&=\mathbb{E} \frac{1}{n} \sum_{i=1}^{n}\left(I-f\left(x_{i}\right)\right) \cdot \frac{1}{n} \sum_{j=1}^{n}\left(I-f\left(x_{j}\right)\right) \\
&=\frac{1}{n^{2}} \sum_{i, j=1}^{n} \mathbb{E}\left(I-f\left(x_{i}\right)\right)\left(I-f\left(x_{j}\right)\right) \\
&=\frac{1}{n^{2}} \sum_{i=j} \mathbb{E}\left(I-f\left(x_{i}\right)\right)\left(I-f\left(x_{j}\right)\right)+\frac{1}{n^{2}} \sum_{i \neq j} \mathbb{E}\left(I-f\left(x_{i}\right)\right)\left(I-f\left(x_{j}\right)\right) \\
&=\frac{1}{n^{2}} \sum_{i=1}^{n} \mathbb{E}\left(I-f\left(x_{i}\right)\right)^{2}+0 \\
&=\frac{\operatorname{Var}(f)}{n},
\end{aligned}
$$
 
其中 Var(f ) 是 f 的方差。所以它的误差大约为$n^{-\frac{1}{2}}$ (方差的精度为$\frac{1}{n}$，所以误差即标 准差的精度为$n^{-\frac{1}{2}}$)，这与维数 d 无关。蒙特卡洛插值法现在已经得到了很好的发展，在 统计物理领域，人们经常用这个方法处理高维数值积分问题。深度神经网络的形式与蒙 特卡洛采样类似，所以我们认为它的精度可能与神经元数量 m 有关，所以不会遇到维数灾难的问题。

:::tip
 前面给出的线性插值和积分问题的例子之所以有维数灾难，与他们的划分是均匀的(有规律的)有关，而蒙特卡洛采样则用了采样的随机性很好的避免了维数灾难。
:::

而神经网络也是利用了类似的原理来避免维度灾难的问题，我们先来看一个二维的神经网络：

$$
f_{\boldsymbol{\theta}}(\boldsymbol{x})=\frac{1}{m} \sum_{j=1}^{m} a_{j} \sigma\left(\boldsymbol{w}_{j} \cdot \boldsymbol{x}+b_{j}\right)
$$
你可以看到它的形式和MC是有几分的相似了，神经网络也是用了一个数据采样的方式避免了维度灾难。至少可以用这样的方式证明使用神经网络去结果问题存在着这样一个最优解；


