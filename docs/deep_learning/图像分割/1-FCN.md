# FCN

## 传统CNN分割网络的缺点

传统的基于CNN的分割方法: 为了对一个像素分类,使用该像素周围的一个图像块作为CNN的输
入,用于训练与预测,这种方法主要有几个缺点：
1. **存储开销大**, 例如,对每个像素使用15\*15的图像块,然后不断滑动窗口,将图像块输入到CNN中进行类别判断,因此,需要的存储空间随滑动窗口的次数和大小急剧上升
2. **效率低下**, 相邻像素块基本上是重复的,针对每个像素块逐个计算卷积,这种计算有很大程度上的重复;
3. 像素块的大小**限制了感受区域的大小**, 通常像素块的大小比整幅图像的大小小很多,只能提取一些局部特征,从而导致分类性能受到限制。

而全巻积网络(FCN)则是从抽象的特征中**恢复出每个像素所属的类别**即从图像级别的分类进一步延伸到像素级别的分类

## FCN与传统CNN的区别

对于一般的分类CNN网络，如VGG和Resnet，都会在网络的最后加入一些**全连接层**，经过softmax后就可以获取类别概率信息。FCN则是把后面几个全连接全部换成卷积，这样就可以获取一张二维的feature map，后解softmax获取每个像素的分类信息，从而解决了分类问题。

:::tip
**为什么全连接不适合做图像分割？**

经过全连接网络获取的类别概率信息是1维的，只能够识别整个图片的类别。而图像分割需要对每个像素点分类也就是一个二维问题。
:::


## FCN的网络结构

FCN可以接受任意尺度的输入图像，采用反卷积层度最后一个卷积层的feature map进行上采样，使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生一个预测，同时保留了原始输入空间中的空间信息，最后在上采样的特征图上进行了逐像素分类。



:::tip
**为什么传统CNN网络的输入大小需要相同？**

因为有全连接层的存在，全连接层的输入大小是固定的
:::

## 反卷积层

上采样（Upsampling）的操作可以看做是反卷积，卷积运算的参数和CNN的参数一样是训练FCN的时候通过梯度下降算法学习得到的。反卷积也是卷积层，不关心input的大小，滑窗后输出output。

:::tip
**反卷积层的缺点**

可以看到在进行反卷积的时候我们是对feature map进行padding之后在进行一个卷积的运算，但是这样一个padding恢复的数据可能会有一个信息的丢失。FCN在编码方面有着无以轮比的优势但是在解码方式上就非常的low，这也是一个transformer能够击败传统FCN的一个原因。
:::

## 跳级结构
对CNN的结果做处理, 得到了 dense prediction, 在实验中发现,得到的分割结果比较粗糙,所以考虑加入更多前层的细节信息,也就是把倒数第几层的输出和最后的输出做一个 fusion,实际上也就是**加和**，对没错大家天天说的特征融合的最简单的操作就是特征的相加。



## FCN的缺点

1. 得到的结果还是不够精细。进行8倍上采样虽然比32倍的效果好了很多，但是**上采样的结果还是比较模糊和平滑，对图像的细节不敏感。**
2. 对各像素进行分类，没有充分考虑像素与像素之间的关系。忽略了在通常的基于像素分类的分割方法中使用的**空间规整步骤，缺乏空间一致性**。






