# 矩阵基础知识
## 标量、向量、矩阵、张量
 ### 理论知识
1. **标量 (Scalar):** 表示一个单独的数，通常用斜体小写字母表示，如 $s \in \mathbb{R}, n \in \mathbb{N}$。
2. **向量 (Vector):** 表示一列数，这些数有序排列的，可以通过下标获取对应值，通常用粗体小写字母表示: $\boldsymbol{x} \in \mathbb{R}_n$ ，它表示元素取实数，且有 n个元素，第一个元素表示为:x1 。将向量写成列向量的形式:
$$
\boldsymbol{x}=\left[\begin{array}{l}
x_{1} \\
x_{2} \\
\cdots \\
x_{n}
\end{array}\right]
$$
 
有时需要向量的子集，例如第1,3,6个元素，那么我们可以令集合$S = \{1,3,6\}$ , 然后 $x_s$ 来表示这个集合，另外我们用符号 - 来表示集合的补集：$x_{-1}$ 表示除 x1 外 x 中的所有元素，$x_{-S}$表示除 x1, x3, x6 外 x 中的所有元素。
3. **矩阵 (Matrix):** 表示一个二维数组, 每个元素的下标由两个数字确定，通常用大写粗体字母表示:$\boldsymbol{A} \in \mathbb{R}^{m \times n}$ ，它表示元素取实数的 m 行 n 列矩阵，其元素可以表示为:$\boldsymbol{A}_{1,1}$, $\boldsymbol{A}_{m,n}$。我们用 : 表示矩阵的一行或者一列 $\boldsymbol{A}_{i,:}$ 为第 i 行，$\boldsymbol{A}_{:,j}$ 为第 j 列。
4.  **张量 (Tensor):**  超过二维的数组，我们用 $\boldsymbol{A}$ 表示张量，$\boldsymbol{A}_{i,j,k}$ 表示其元素(三维张量情况下)。

:::tip
**张量和矩阵的区别？**

从代数的角度出发，向量可以看做一维的表格，矩阵是二维的表格，那么n阶的张量就是一个n维的表格。
:::

### 代码示例
```python
import numpy as np
 
# 标量  
s = 5  
# 向量  
v = np.array([1,2]) # 矩阵, 2维的

m = np.array([[1,2], [3,4]]) # 张量，大于二维的 
t = np.array([

 [[1,2,3],[4,5,6],[7,8,9]],
  [[11,12,13],[14,15,16],[17,18,19]],
  [[21,22,23],[24,25,26],[27,28,29]],
  ]) 

print("标量: " + str(s)) 
print("向量: " + str(v)) 
print("矩阵: " + str(m)) 
print("张量: " + str(t))
```
Out:
![](https://gitee.com/coronapolvo/images/raw/master/20220221140420.png)

## 矩阵转置
 矩阵转置 (Transpose) 相当于沿着对角线翻转，定义如下:
 $$
A_{i, j}^{\top}=A_{i, j}
$$
 矩阵转置的转置等于矩阵本身:
 $$
\left(\boldsymbol{A}^{\top}\right)^{\top}=\boldsymbol{A}
$$
 

转置将矩阵的形状从 m×n 变成了 n×m 。 

向量可以看成是只有一列的矩阵，为了方便，我们可以使用行向量加转置的操作，如: $x = [x1, x2, x3]^T$。
 
标量也可以看成是一行一列的矩阵，其转置等于它自身: $a^T = a$ 。
```python
In [3]: import numpy as np                                                      
In [4]: A = np.array([[1.0,2.0],[1.0,0.0],[2.0,3.0]])                          
In [5]: A_t = A.transpose()                                                     
In [6]: print("A:", A)                                                          
A: [[1. 2.]
 [1. 0.]
 [2. 3.]]
In [7]: print("A 的转置:", A_t)                                                 
A 的转置: [[1. 1. 2.]
 [2. 0. 3.]]
```

## 矩阵加法
 
加法即对应元素相加，要求两个矩阵的形状一样:
$$
\boldsymbol{C}=\boldsymbol{A}+\boldsymbol{B}, C_{i, j}=A_{i, j}+B_{i, j}
$$
 数乘即一个标量与矩阵每个元素相乘:
 $$
\boldsymbol{D}=a \cdot \boldsymbol{B}+c, D_{i, j}=a \cdot B_{i, j}+c
$$ 

有时我们允许矩阵和向量相加的，得到一个矩阵，把 b 加到了 A 的每一行上，本质上是构造了一个将 b 按行复制的一个新矩阵，这种机制叫做广播 (Broadcasting):

$$
\boldsymbol{C}=\boldsymbol{A}+\boldsymbol{b}, C_{i, j}=A_{i, j}+b_{j}
$$
## 矩阵乘法
 
两个矩阵相乘得到第三个矩阵，我们需要 A 的形状为 m × n，B 的形状为 n × p，得到的矩阵为 C 的形状为 m × p:
$$
C=A B
$$
 具体定义为：
 $$
C_{i, j}=\sum_{k} A_{i, k} B_{k, j}
$$
 注意矩阵乘法不是元素对应相乘，元素对应相乘又叫 Hadamard 乘积，记作 A ⊙ B。  
 
向量可以看作是列为 1 的矩阵，两个相同维数的向量 x 和 y 的点乘(Dot Product)或者内积，可以表示为$x^Ty$。 

我们也可以把矩阵乘法理解为:$C_{i,j}$ 表示 $\boldsymbol{A}$ 的第 i 行与 B 的第 j 列的点积。

## 单位矩阵
 
为了引入矩阵的逆，我们需要先定义单位矩阵 (Identity Matrix):单位矩阵乘以任意一个向量等于这个向量本身。记 In 为保持 n 维向量不变的 单位矩阵，即:
$$
\boldsymbol{I}_{n} \in \mathbb{R}^{n \times n}, \forall \boldsymbol{x} \in \mathbb{R}^{n}, \boldsymbol{I}_{n} \boldsymbol{x}=\boldsymbol{x}
$$ 

单位矩阵的结构十分简单，所有的对角元素都为 1 ，其他元素都为 0，如:
$$
\boldsymbol{I}_{3}=\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]
$$
```python
In [8]: np.identity(3)                                                            
Out[8]: 
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]])
```

:::tip
如何笔算矩阵乘法？
:::
![](https://gitee.com/coronapolvo/images/raw/master/20220221142322.png)

## 矩阵的逆
 
矩阵 A 的逆 (Inversion) 记作 $A^{-1}$，定义为一个矩阵使得:
$$
\boldsymbol{A}^{-1} \boldsymbol{A}=\boldsymbol{I}_{n}
$$
 如果 $A^{-1}$ 存在，那么线性方程组 Ax = b 的解为:
$$
\boldsymbol{A}^{-1} \boldsymbol{A} \boldsymbol{x}=\boldsymbol{I}_{n} \boldsymbol{x}=\boldsymbol{x}=\boldsymbol{A}^{-1} \boldsymbol{b}
$$
```python
In [10]: A = [[1.0,2.0],[3.0,4.0]]                                                
In [11]: A_inv = np.linalg.inv(A)                                                 
In [12]: print("A 的逆矩阵", A_inv)                                               
A 的逆矩阵 [[-2.   1. ]
 [ 1.5 -0.5]]
```

:::tip
如何计算矩阵的逆元？
:::
![](https://gitee.com/coronapolvo/images/raw/master/20220221142741.png)

## 范数
 :::tip
 你可以简单地将范数理解为矩阵的“模长”
 :::
 通常我们用范数 (norm) 来衡量向量，向量的 Lp 范数定义为:
$$
\|\boldsymbol{x}\|_{p}=\left(\sum_{i}\left|x_{i}\right|^{p}\right)^{\frac{1}{p}}, p \in \mathbb{R}, p \geq 1
$$
$L^{2}$ 范数, 也称**欧几里得范数 (Euclidean norm)**, 是向量 $\boldsymbol{x}$ 到原点的欧几里得距离。有时也用 $L^{2}$ 范数的平方来衡量向量: $\boldsymbol{x}^{\top} \boldsymbol{x}$ 。事实上, 平方 $L^{2}$ 范数在计算上更为便利, 例如它的对 $\boldsymbol{x}$ 梯度的各个分量只依赖于 $\boldsymbol{x}$ 的对应的各个分量, 而 $L^{2}$ 范数对 $\boldsymbol{x}$ 梯度的各个分量要依赖于整个 $\boldsymbol{x}$ 向量。

$L^{1}$ 范数： $L^{2}$ 范数并不一定适用于所有的情况, 它在**原点附近的增长就十分缓慢**, 因此**不适用于需要区别 0 和非常小但是非 0 值的情况**。 $L^{1}$ 范数 就是一个比较好的选择, 它在**所有方向上的增长速率都是一样的**, 定义为:
$$
\|\boldsymbol{x}\|_{1}=\sum_{i}\left|x_{i}\right|
$$
它**经常使用在需要区分 0 和非 0 元素的情形中**。

$L^{0}$ 范数：如果需要衡量向量中非 0 元素的个数, 但它**并不是一个范数** (不满足三角不等式和数乘), 此时 $L^{1}$ 范数可以作为它的一个替代。

$L^{\infty}$ 范数：它在数学上是向量元素绝对值的最大值, 因此也被叫做 (Max norm):

$$
\|\boldsymbol{x}\|_{\infty}=\max _{i}\left|x_{i}\right|
$$
有时我们想衡量一个矩阵,  机器学习中通常使用的是 $\mathrm{F}$ 范数 (Frobenius norm), 其定义为:
$$
\|\boldsymbol{A}\|_{F}=\sqrt{\sum_{i, j} A_{i, j}^{2}}
$$
:::tip
F范数实际上就是衡量这个矩阵和对应的零矩阵的距离，就像二维平面上的一个点，和原点的距离就是它的F范数
:::

```python
a = np.array([1,2,3,4])
# 二范数
np.linalg.norm(a,ord=2)
# 一范数
np.linalg.norm(a,ord=1)
# 无穷范数
np.linalg.norm(a,ord=np.inf)
# F范数
np.linalg.norm(a,ord="fro")
```

:::tip
说出常用的矩阵范数，并说出它们的区别

常用的矩阵范数有L0范数、L1范数和F范数。

L0范数可以计算矩阵中的非零元素的个数，L0范数越小0元素越多，也就是越稀疏。

L1范数表示矩阵中每个元素绝对值之和，它是L0范数的最优凸近似，它也可以表示稀疏

F范数是各个元素之和再开平方根，它通常也叫做矩阵的L2范数，也是机器学习中最长使用的范数，因为它是一个凸函数，可以求导求解，易于计算
:::

## 特征值分解

如果一个 $n \times n$ **方阵**（**注意只能是方阵**） $\boldsymbol{A}$ 有 $n$ 组**线性无关**的**单位特征向量** $\left\{\boldsymbol{v}^{(1)}, \ldots, \boldsymbol{v}^{(n)}\right\}$, 以及对应的特征值 $\lambda_{1}, \ldots, \lambda_{n}$ 。将这些特征向量按列拼接成一个矩阵: $\boldsymbol{V}=\left[\boldsymbol{v}^{(1)}, \ldots, \boldsymbol{v}^{(n)}\right]$, 并将对应的特征值拼接成一个向量: $\boldsymbol{\lambda}=\left[\lambda_{1}, \ldots, \lambda_{n}\right]$ 。

$\boldsymbol{A}$ 的特征值分解 (Eigendecomposition) 为:
$$
\boldsymbol{A}=\boldsymbol{V} \operatorname{diag}(\boldsymbol{\lambda}) \boldsymbol{V}^{-1}
$$
使用代码对矩阵进行特征值分解非常的简单：
```python
A = np.array([[1.0,2.0,3.0],
			  [4.0,5.0,6.0], 
			  [7.0,8.0,9.0]])
# 计算特征值
np.linalg.eigvals(A)
# 计算特征值的特征向量
eigvals, eigvectors = np.linalg.eig(A)
print("特征值:", eigvals) 
print("特征向量:", eigvectors)
```
![](https://gitee.com/coronapolvo/images/raw/master/20220221152246.png)

## 奇异值分解
:::tip
特征分解只能对方阵进行处理，奇异值分解则可以对任意矩阵进行分解
:::

奇异值分解 (Singular Value Decomposition, SVD) 提供了另一种分解矩阵的方式, 将其分解为奇异向量和奇异值。

与特征值分解相比, 奇异值分解更加通用, **所有的实矩阵都可以进行奇异值分解, 而特征值分解只对某些方阵可以**。 奇异值分解的形式为:
$$
\boldsymbol{A}=\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\top}
$$
若 $\boldsymbol{A}$ 是 $m \times n$ 的, 那么 $\boldsymbol{U}$ 是 $m \times m$ 的, 其列向量称为左奇异向量, 而 $\boldsymbol{V}$ 是 $n \times n$ 的, 其列向量称为右奇异向量, 而 $\boldsymbol{\Sigma}$ 是 $m \times n$ 的一个**对角矩阵**, 其对角元素称为矩阵 $\boldsymbol{A}$ 的奇异值。

事实上, 左奇异向量是 $\boldsymbol{A} \boldsymbol{A}^{\top}$ 的特征向量, 而右奇异向量是 $\boldsymbol{A}^{\top} \boldsymbol{A}$ 的特征向量, 非 0 奇异值的平方是 $\boldsymbol{A}^{\top} \boldsymbol{A}$ 的非 0 特征值。



## 参考
[1]  [https://github.com/MingchaoZhu/DeepLearning](https://github.com/MingchaoZhu/DeepLearning)

