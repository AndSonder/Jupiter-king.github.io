# 共享内存缓存块

在全局内存之外，GPU还有一块位于芯片上的较小区域，被称为共享内存（SMEM）。每个SM（流多处理器）都配备了一块共享内存。以下是A100 GPU内存层次结构的图示：

![picture 0](images/919ee7461c42bbc16b30e33ef496cf63884be18b2fd6342319a5279f5678a524.png)  

从逻辑上看，共享内存在各个块之间进行了分区。这意味着一个线程可以通过共享内存块与同一块内的其他线程进行通信。在我的A6000 GPU上，每个块最多可以访问48KB的共享内存。共享内存的大小是可配置的，可以通过权衡以获得更大的共享内存而减小L1缓存的大小。具体细节请参阅计算能力文档。此外，通过使用动态共享内存，可以超过每个线程48KB的SMEM。

由于共享内存位于芯片上，其延迟较低，带宽较高。虽然我找不到Ampere架构的准确基准结果，但对于2017年发布的Volta架构，根据论文的基准测试报告，全局内存带宽为750GiB/s，共享内存带宽为12,080GiB/s。 从Volta以来，这些数字似乎并没有发生太大变化。

因此，对于这个新的内核，我们将A和B的全局内存一块加载到共享内存中。接着，我们将在这两块上尽可能多地执行计算，每个线程仍然分配给C的一个条目。我们将沿着A的列和B的行移动这些块，在C上执行部分求和，直到最终结果得出。

![picture 1](images/0f38c9170eab69162740def01d0cd30b432e54c6c3a022a38227ee3630691961.png)  

以下是代码的重要部分，其中变量名称对应上面的图表：

```cpp
// 推进指针到起始位置
A += cRow * BLOCKSIZE * K;                    // 行=cRow，列=0
B += cCol * BLOCKSIZE;                        // 行=0，列=cCol
C += cRow * BLOCKSIZE * N + cCol * BLOCKSIZE; // 行=cRow，列=cCol

float tmp = 0.0;
// 外部循环推进 A 沿列和 B 沿行，直到我们完全计算出 C 中的结果。
for (int bkIdx = 0; bkIdx < K; bkIdx += BLOCKSIZE) {
  // 每个线程从全局内存加载 A 和 B 中的一个元素到共享内存中。
  // 将 threadCol（=threadIdx.x）设为连续的索引，以允许全局内存访问协同。
  As[threadRow * BLOCKSIZE + threadCol] = A[threadRow * K + threadCol];
  Bs[threadRow * BLOCKSIZE + threadCol] = B[threadRow * N + threadCol];

  // 阻塞本块内的线程，直到缓存完全填充
  __syncthreads();

  // 推进指针到下一个块
  A += BLOCKSIZE;
  B += BLOCKSIZE * N;

  // 在当前缓存块上执行点积
  for (int dotIdx = 0; dotIdx < BLOCKSIZE; ++dotIdx) {
    tmp += As[threadRow * BLOCKSIZE + dotIdx] *
            Bs[dotIdx * BLOCKSIZE + threadCol];
  }
  // 在最后需要再次同步，以避免更快的线程在较慢的线程完成之前将下一个块提取到缓存中
  __syncthreads();
}
C[threadRow * N + threadCol] =
    alpha * tmp + beta * C[threadRow * N + threadCol];

```

这段代码通过将 A 和 B 的一块加载到共享内存中，然后在共享内存中执行点积，以充分利用共享内存的低延迟和高带宽。此方法通过优化数据访问模式和减少全局内存访问次数来提高性能。通过测试，将其与 cuBLAS 的结果进行比较，可以验证内核的正确性。

这个内核实现了约2200 GFLOPS，相较于之前的版本提高了50%。部分原因是因为我们之前的内核已经具有相当良好的L1缓存命中率。然而，我们仍然离GPU可以提供的约30 TFLOPs相去甚远。这一点在下面的图中表现得非常明显：请注意，我们实现了比cuBLAS更高的内存带宽。但由于每个从内存加载的字节所需的工作量较小（算术强度较低），因此整体性能较差。

![picture 2](images/ebcd04f729186cbd1a3a9d50bdcefb23b311f0c857d088847f3e2f7d8c4c2cf5.png)  

在将 CHUNKSIZE 设置为 32 的情况下，这个内核充分利用了 2\*32\*32*4B=8KB 的共享内存空间。你也可以通过在编译时添加参数 --ptxas-options=-v 来获取相应的信息，输出结果为：Used 37 registers, 8192 bytes smem, 400 bytes cmem[0]。我的 A6000 GPU 每个块可用的最大共享内存空间是48KB，因此我们离达到这个限制还有一些距离。但这并不一定是个问题，因为增加每个块的共享内存使用率也会带来一些不利因素。每个多处理器（SM）最多可用 100KB 的 SMEM。这就意味着，如果我们修改内核以使用完整的 48KB 可用的 SMEM，那么每个 SM 同一时间只能保持两个块加载。在 CUDA 的术语中，增加每块 SMEM 利用率可能会降低占用率。占用率定义为每个 SM 的活动 warp 数与每个 SM 的最大可能活动 warp 数之间的比率。

高占用率非常有用，因为它允许我们通过拥有更大的可发出指令池来掩盖操作的高延迟。在 GPU 上，像 FMA 这样的数学运算的延迟为 4 个周期，相当于在 1.5GHz 时钟下的 2.6ns。将这与最近的 x86 CPU 相比，那里 FMA 的延迟为 6 个周期，相当于在 3.5GHz 时钟下的 1.8ns。保持更多活动块加载到一个 SM 上有三个主要限制：寄存器数、warp 数和 SMEM 容量。

## 参考文献

1. https://siboehm.com/articles/22/CUDA-MMM